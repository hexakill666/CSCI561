
[Executed at: Sun Nov 20 22:55:10 PST 2022]

HW3 Report
Programming language: JAVA
==> Begin grading

Sun 20 Nov 2022 10:54:17 PM PST
(gaussian public test) accuracy: 97.80
(hidden test placeholder, not shown before deadline)
(xor public test) accuracy: 95.60
(hidden test placeholder, not shown before deadline)
(circle public test) accuracy: 92.20
(hidden test placeholder, not shown before deadline)
(spiral public test) accuracy: 98.20
(hidden test placeholder, not shown before deadline)
==> Stop grading
Sun 20 Nov 2022 10:54:59 PM PST
==> Score report
Raw avg accuracy: 95.94999999999999
Total score (according to hidden data grading scale): 100.0
==> End score report

==========Start ReadInput==========
readInputTime: 393
==========Finish ReadInput==========
==========Start BuildNetwork==========
buildNetworkTime: 10
==========Finish BuildNetwork==========
==========train 0==========
train: 0, loss: -1.0
layerType: input, nodeSize: 5
double[][] w: null
double[] b: null
double[][] partialDerivative: null
double[] z: null
double[] h: 
[0.0, 0.0, 0.0, 0.0, 0.0]
layerType: hidden, nodeSize: 8
double[][] w: 
[0.9322696917977171, 0.6337560552077339, 0.12507987976428736, 0.4871313392387868, 0.2770057293150183]
[-0.9635992693402795, 0.40527728644636674, -0.6880975101260907, -0.5497633565674669, 0.627691332547416]
[0.30091252085710174, -0.9065736532391717, -0.699719438557135, 0.1758151917987938, 0.006794115204436624]
[-0.09108713336820085, -0.6611847423796464, -0.7795017474340518, 0.25549923376170836, -0.4054306032886008]
[0.38031792345682724, 0.5546525402277775, -0.7181734118173335, 0.38732138301029284, 0.10180991261000161]
[-0.6804802598464874, 0.7561689445319828, -0.2623230149358933, 0.7026555349392734, 0.743158309424333]
[0.22690542346903064, -0.33950324267850696, 0.08049106724374666, 0.3665722662764104, -0.8705078353595852]
[-0.6638687677711026, -0.761810704876271, -0.021264642772333087, -0.9999928542289629, 0.5825934676581681]
double[] b: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[][] partialDerivative: 
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
double[] z: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[] h: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
layerType: output, nodeSize: 1
double[][] w: 
[0.6343090605665818, 0.7070309441048217, 0.9986392512503273, 0.9472348739439749, 0.45051567617039767, 0.17933786537781926, 0.5310107394068089, -0.3527832114359819]
double[] b: 
[0.0]
double[][] partialDerivative: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[] z: 
[0.0]
double[] h: 
[0.0]
==============================
==========Start Train==========
train: 1, loss: 0.011504066529799767
train: 2, loss: 0.004391497901376179
train: 3, loss: 0.0030150747380912747
train: 4, loss: 0.0014666255427524947
train: 5, loss: 0.0014470459938177686
train: 6, loss: 0.001717580230086453
train: 7, loss: 8.893061328314193E-4
train: 8, loss: 6.153588439682049E-4
train: 9, loss: 6.001315805383222E-4
train: 10, loss: 5.280781333060212E-4
train: 11, loss: 9.167713268788117E-4
train: 12, loss: 5.62338927559221E-4
train: 13, loss: 5.777564102404477E-4
train: 14, loss: 4.680599734243577E-4
train: 15, loss: 5.501125532379537E-4
train: 16, loss: 3.009355881714305E-4
train: 17, loss: 0.0015401766742659205
train: 18, loss: 3.67163102271791E-4
train: 19, loss: 0.0015526543256633007
train: 20, loss: 3.304093932666861E-4
train: 21, loss: 3.3738837846399364E-4
train: 22, loss: 2.452416820136915E-4
train: 23, loss: 8.938873149647927E-4
train: 24, loss: 7.913368768157265E-4
train: 25, loss: 2.8853024879163443E-4
train: 26, loss: 2.5673088801012425E-4
train: 27, loss: 3.0017944128782803E-4
train: 28, loss: 0.002162481560771613
train: 29, loss: 1.8599552194450346E-4
train: 30, loss: 2.0041361186638347E-4
train: 31, loss: 2.739117454599057E-4
train: 32, loss: 2.3928896599086322E-4
train: 33, loss: 1.4881882341675524E-4
train: 34, loss: 0.012353918078153472
train: 35, loss: 1.2493037249085095E-4
train: 36, loss: 2.6463689134723366E-4
train: 37, loss: 1.4215849228689344E-4
train: 38, loss: 2.0754312805858942E-4
train: 39, loss: 1.8912169078325706E-4
train: 40, loss: 2.0384351386318697E-4
train: 41, loss: 0.45939971243226657
train: 42, loss: 2.5115667490435066E-4
train: 43, loss: 1.2142069943941528E-4
train: 44, loss: 1.810565791262676E-4
train: 45, loss: 1.276839576010351E-4
train: 46, loss: 1.3692159840361572E-4
train: 47, loss: 4.0809100402913203E-4
train: 48, loss: 1.6135895667960986E-4
train: 49, loss: 1.493446976457036E-4
train: 50, loss: 5.375164986316225E-4
train: 51, loss: 1.7736814205422706E-4
train: 52, loss: 1.9062309014571233E-4
train: 53, loss: 1.1451278541130067E-4
train: 54, loss: 9.508978312538874E-5
train: 55, loss: 1.4769373755473922E-4
train: 56, loss: 1.4796653481103663E-4
train: 57, loss: 0.009625200962762207
train: 58, loss: 1.0660204262271134E-4
train: 59, loss: 1.0212412135420955E-4
train: 60, loss: 8.469865945688097E-5
train: 61, loss: 1.3613588395995237E-4
train: 62, loss: 8.608503787084368E-5
train: 63, loss: 8.2652609800903E-5
train: 64, loss: 7.97350724750723E-5
train: 65, loss: 1.61072795460926E-4
train: 66, loss: 6.395115896251228E-4
train: 67, loss: 1.5799018811203327E-4
train: 68, loss: 0.004285928856305395
train: 69, loss: 0.005736202772721682
train: 70, loss: 7.798521009373376E-5
train: 71, loss: 0.003856145607083025
train: 72, loss: 1.2953721654601177E-4
train: 73, loss: 1.4824502989676402E-4
train: 74, loss: 1.5193917558484045E-4
train: 75, loss: 1.208370896179463E-4
train: 76, loss: 7.11234048147816E-5
train: 77, loss: 1.2901415851904142E-4
train: 78, loss: 1.1519061983068474E-4
train: 79, loss: 1.1388099538736834E-4
train: 80, loss: 1.2348292708792356E-4
train: 81, loss: 8.123953775188467E-5
train: 82, loss: 1.4377025558399149E-4
train: 83, loss: 5.7082786302987944E-5
train: 84, loss: 1.1967301828437337E-4
train: 85, loss: 9.233460788814756E-5
train: 86, loss: 1.2326501428082344E-4
train: 87, loss: 4.546284865784953E-4
train: 88, loss: 7.440389212508561E-5
train: 89, loss: 4.978764527435823E-5
train: 90, loss: 7.22053136959643E-4
train: 91, loss: 3.015050732562858E-4
train: 92, loss: 1.1120484603706215E-4
train: 93, loss: 1.082632449226952E-4
train: 94, loss: 6.725181325055168E-5
train: 95, loss: 0.23353512385194217
train: 96, loss: 3.1135747686103217E-4
train: 97, loss: 9.587129993325438E-5
train: 98, loss: 1.17283910560025E-4
train: 99, loss: 1.0263527260994334E-4
train: 100, loss: 6.881202977529891E-4
train: 101, loss: 5.9423474608284656E-5
train: 102, loss: 9.17032476323348E-5
train: 103, loss: 9.007618539446087E-5
train: 104, loss: 9.965210186823627E-5
train: 105, loss: 2.0916461892279707E-4
train: 106, loss: 1.8157479474893338E-4
train: 107, loss: 1.306274299367112E-4
train: 108, loss: 6.631908857593512E-5
train: 109, loss: 5.356980421847853E-5
train: 110, loss: 4.6425960612878586E-5
train: 111, loss: 4.340745700166543E-5
train: 112, loss: 4.756413559579867E-5
train: 113, loss: 8.89939476727364E-5
train: 114, loss: 0.0013171599938382793
train: 115, loss: 9.268798170790617E-5
train: 116, loss: 4.346628189913534E-5
train: 117, loss: 1.1521123726569484E-4
train: 118, loss: 4.328179502258226E-5
train: 119, loss: 8.350128095427803E-5
train: 120, loss: 1.7015122185689934E-4
train: 121, loss: 3.5412582918276324E-5
train: 122, loss: 0.06367741899069126
train: 123, loss: 0.013626483516785089
train: 124, loss: 8.883324351029555E-5
train: 125, loss: 3.0800305544274653E-4
train: 126, loss: 0.268706076452292
train: 127, loss: 0.41434994819136695
train: 128, loss: 0.0031771299302408807
train: 129, loss: 3.7483386924482634E-5
train: 130, loss: 8.72900586532683E-5
train: 131, loss: 5.4551018261192566E-5
train: 132, loss: 8.936927591846455E-5
train: 133, loss: 4.309340607353225E-5
train: 134, loss: 8.087878930544885E-5
train: 135, loss: 9.634356548894927E-5
train: 136, loss: 1.2031043900519815E-4
train: 137, loss: 6.299535696321789E-4
train: 138, loss: 9.737556175771585E-5
train: 139, loss: 0.01571524650453
train: 140, loss: 3.1719228905573606E-5
train: 141, loss: 1.0679666835726181E-4
train: 142, loss: 0.04413118789643758
train: 143, loss: 1.5679622074217373E-4
train: 144, loss: 8.284249867378834E-5
train: 145, loss: 6.439739271478662E-5
train: 146, loss: 4.6626132445632516E-5
train: 147, loss: 7.631914729088007E-5
train: 148, loss: 6.659731310932178E-5
train: 149, loss: 3.3569564990212286E-5
train: 150, loss: 1.0035903572700755E-4
train: 151, loss: 0.020668897723084084
train: 152, loss: 8.506829358680628E-4
train: 153, loss: 2.640117653020734E-5
train: 154, loss: 9.683653612936592E-5
train: 155, loss: 7.251221836244613E-5
train: 156, loss: 6.450906214297999E-5
train: 157, loss: 6.103556407042978E-5
train: 158, loss: 1.729100011024594E-4
train: 159, loss: 3.992623119336202E-4
train: 160, loss: 0.029963135348623875
train: 161, loss: 6.336737039932943E-5
train: 162, loss: 2.308104346433479E-5
train: 163, loss: 2.2879826456850054E-5
train: 164, loss: 6.600169411199896E-5
train: 165, loss: 2.3211009717247245E-5
train: 166, loss: 0.0027578748804734797
train: 167, loss: 2.4997708990404764E-5
train: 168, loss: 4.5342757038860555E-5
train: 169, loss: 6.335344250519052E-5
train: 170, loss: 7.102666688463967E-5
train: 171, loss: 3.583715184562457E-5
train: 172, loss: 2.1238717152832093E-5
train: 173, loss: 0.004379466422853714
train: 174, loss: 1.1032655142005317E-4
train: 175, loss: 3.0554322839347126E-5
train: 176, loss: 6.168971094219743E-5
train: 177, loss: 4.524546611103025E-5
train: 178, loss: 1.7260202085109247E-5
train: 179, loss: 7.233739630601906E-5
train: 180, loss: 0.003755397583000884
train: 181, loss: 1.5714226902636467E-4
train: 182, loss: 1.9028530805233262E-5
train: 183, loss: 4.8629487720737425E-5
train: 184, loss: 5.934070276338407E-5
train: 185, loss: 4.998288492523563E-5
train: 186, loss: 5.305664980527543E-5
train: 187, loss: 2.880349171150681E-5
train: 188, loss: 6.001308738196329E-5
train: 189, loss: 8.411181248082816E-5
train: 190, loss: 7.112218223247408E-5
train: 191, loss: 6.93935750952157E-5
train: 192, loss: 4.417019045401486E-5
train: 193, loss: 1.9929818222661363E-5
train: 194, loss: 6.008801703837927E-5
train: 195, loss: 5.2380619057735903E-5
train: 196, loss: 3.657885023861464E-5
train: 197, loss: 5.891392430258288E-5
train: 198, loss: 1.9880585481819035E-5
train: 199, loss: 7.864131692583194E-5
train: 200, loss: 1.1680037841706214E-4
train: 201, loss: 1.4729485516094404E-5
train: 202, loss: 1.7517414414133582E-5
train: 203, loss: 5.2821442840034744E-5
train: 204, loss: 1.1962060441980423E-4
train: 205, loss: 5.1789155767616824E-5
train: 206, loss: 1.2187188610467679E-4
train: 207, loss: 1.060266041020297E-4
train: 208, loss: 2.0031965028772244E-5
train: 209, loss: 5.718338459718319E-4
train: 210, loss: 2.177946476962437E-5
train: 211, loss: 0.0024293912591115173
train: 212, loss: 7.41896211965144E-5
train: 213, loss: 4.0885359607747455E-5
train: 214, loss: 2.828665330126794E-5
train: 215, loss: 1.619730464355745E-5
train: 216, loss: 2.9981890839515527E-5
train: 217, loss: 1.569800940893932E-5
train: 218, loss: 9.1142715595411E-5
train: 219, loss: 1.3909457970613182E-4
train: 220, loss: 0.0012918828233680504
train: 221, loss: 8.188337711625191E-4
train: 222, loss: 4.223280458890581E-4
train: 223, loss: 3.002819475087289E-5
train: 224, loss: 6.035394000956044E-5
train: 225, loss: 7.355569967458514E-5
train: 226, loss: 4.239463993896516E-5
train: 227, loss: 3.0511450998396553E-4
train: 228, loss: 3.984947872511663E-5
train: 229, loss: 9.257367959599449E-5
train: 230, loss: 4.647898203724798E-4
train: 231, loss: 0.18687586478175797
train: 232, loss: 4.2149664661687944E-5
train: 233, loss: 3.5790588058872865E-5
train: 234, loss: 0.23566067862445114
train: 235, loss: 1.3179343178615574E-5
train: 236, loss: 1.2298060404322077E-5
train: 237, loss: 1.6515401580198162E-5
train: 238, loss: 3.177540535798836E-5
train: 239, loss: 3.687969566217312E-5
train: 240, loss: 5.888961157172467E-5
train: 241, loss: 1.839889213823261E-5
train: 242, loss: 1.2824247073236503E-5
train: 243, loss: 1.705869485468483E-5
train: 244, loss: 4.339200741730619E-4
train: 245, loss: 0.35132190191767215
train: 246, loss: 7.958822685031884E-5
train: 247, loss: 3.3568257510823145E-5
train: 248, loss: 0.0029797426124131767
train: 249, loss: 3.3377221030605334E-5
train: 250, loss: 7.098777708197745E-4
train: 251, loss: 1.4689204690413073E-4
train: 252, loss: 8.901683659674751E-4
train: 253, loss: 0.3960105080411893
train: 254, loss: 3.334812096397283E-5
train: 255, loss: 1.2261078457868518E-5
train: 256, loss: 3.636087589830391E-5
train: 257, loss: 9.898496499914787E-6
train: 258, loss: 1.0163096456829199E-5
train: 259, loss: 3.896864296268371E-5
train: 260, loss: 5.4691339211933934E-5
train: 261, loss: 3.105713209544918E-5
train: 262, loss: 1.0897765464513668E-5
train: 263, loss: 1.5021627903996166E-4
train: 264, loss: 0.02941070796511913
train: 265, loss: 9.357790004057882E-6
train: 266, loss: 1.3724664909507259E-5
train: 267, loss: 3.6308472554059114E-5
train: 268, loss: 4.8677466714334106E-5
train: 269, loss: 0.008832889234004052
train: 270, loss: 3.2907397196153865E-5
train: 271, loss: 3.793121167207194E-4
train: 272, loss: 3.051023584494496E-5
train: 273, loss: 3.587837484223821E-5
train: 274, loss: 5.8186010988971385E-5
train: 275, loss: 2.7689113420238813E-5
train: 276, loss: 0.03240795795931313
train: 277, loss: 2.4304405650115664E-5
train: 278, loss: 2.8325298962972814E-5
train: 279, loss: 0.009225292362584554
train: 280, loss: 0.1201639299392169
train: 281, loss: 1.1864027259935188E-4
train: 282, loss: 8.913056061351153E-6
train: 283, loss: 1.1223392863780333E-5
train: 284, loss: 0.004967115816142122
train: 285, loss: 5.368615857117488E-5
train: 286, loss: 9.564527170262212E-4
train: 287, loss: 4.7211800900704565E-4
train: 288, loss: 3.333229615882508E-5
train: 289, loss: 1.3393960257625808E-5
train: 290, loss: 1.0656348913017103E-5
train: 291, loss: 2.6336746287161998E-5
train: 292, loss: 9.426523720026657E-6
train: 293, loss: 7.05584451397115E-4
train: 294, loss: 2.6083081285156805E-4
train: 295, loss: 1.1801728852547707E-5
train: 296, loss: 7.496042900145552E-6
train: 297, loss: 8.69947973650974E-6
train: 298, loss: 1.129290647418938E-4
train: 299, loss: 0.01964756874941179
train: 300, loss: 7.848008392626758E-6
train: 301, loss: 9.524080252001946E-6
train: 302, loss: 0.0013138175167160754
train: 303, loss: 2.6350123590041863E-5
train: 304, loss: 8.66585685929193E-6
train: 305, loss: 3.336317602636846E-5
train: 306, loss: 8.22716165624642E-6
train: 307, loss: 5.273573856655019E-4
train: 308, loss: 8.46070684299121E-6
train: 309, loss: 0.0027547658597681613
train: 310, loss: 1.1274868536260295E-5
train: 311, loss: 2.2495917192998326E-5
train: 312, loss: 1.1899710897415267E-4
train: 313, loss: 1.4455526644777055E-4
train: 314, loss: 8.125277588178278E-6
train: 315, loss: 2.832387223673494E-4
train: 316, loss: 4.8494177826000805E-5
train: 317, loss: 8.229198201107833E-5
train: 318, loss: 1.2306214809781788E-5
train: 319, loss: 7.144858268226264E-5
train: 320, loss: 3.9968568499769826E-4
train: 321, loss: 0.0019247300450734393
train: 322, loss: 7.942236477874988E-6
train: 323, loss: 9.364640731389696E-5
train: 324, loss: 4.1456115667091476E-5
train: 325, loss: 9.437000536261881E-6
train: 326, loss: 0.14431051506336923
train: 327, loss: 8.28329282191764E-6
train: 328, loss: 2.9317268960261442E-5
train: 329, loss: 3.413095024238691E-4
train: 330, loss: 0.0013858181729172858
train: 331, loss: 2.3829097128704983E-5
train: 332, loss: 1.4583413729492703E-5
train: 333, loss: 6.328972859871239E-6
train: 334, loss: 1.0554542689570564E-5
train: 335, loss: 6.605015504011018E-6
train: 336, loss: 9.450774664539792E-6
train: 337, loss: 7.884332905344743E-6
train: 338, loss: 0.0010259884351396344
train: 339, loss: 3.0049091599518475E-4
train: 340, loss: 7.477391769156657E-5
train: 341, loss: 2.2582079202862146E-5
train: 342, loss: 0.3921823445580179
train: 343, loss: 2.0999552149354223E-5
train: 344, loss: 7.924399818527723E-6
train: 345, loss: 2.04343625462679E-5
train: 346, loss: 0.12962444957334335
train: 347, loss: 2.259051159437584E-5
train: 348, loss: 7.304683829150208E-6
train: 349, loss: 7.236110142608435E-6
train: 350, loss: 3.068066323223054E-5
train: 351, loss: 2.1832898496739416E-5
train: 352, loss: 1.8620118080812853E-5
train: 353, loss: 1.4254305760884057E-4
train: 354, loss: 1.3723300926434466E-5
train: 355, loss: 1.8771151172259207E-5
train: 356, loss: 7.125694989666125E-6
train: 357, loss: 2.2775599782970534E-5
train: 358, loss: 9.308806537068688E-6
train: 359, loss: 5.1915663880170415E-5
train: 360, loss: 6.1922136569050905E-6
train: 361, loss: 3.0185203148955252E-5
train: 362, loss: 9.156093361089322E-5
train: 363, loss: 6.839635262353129E-6
train: 364, loss: 6.622369648034151E-4
train: 365, loss: 5.918464618574652E-6
train: 366, loss: 2.5168096541661808E-5
train: 367, loss: 7.352288025552574E-6
train: 368, loss: 1.1376266548469244E-5
train: 369, loss: 2.4032486101687013E-5
train: 370, loss: 6.787214557349325E-6
train: 371, loss: 5.928184198753691E-6
train: 372, loss: 1.8584287086434344E-5
train: 373, loss: 1.7179903133761847E-5
train: 374, loss: 5.7949353212003384E-6
train: 375, loss: 5.561826920764326E-6
train: 376, loss: 6.159196162266093E-4
train: 377, loss: 5.705666182448099E-6
train: 378, loss: 2.081352836356535E-5
train: 379, loss: 2.2728158998677407E-5
train: 380, loss: 1.0528911387975696E-4
train: 381, loss: 2.968220697028105E-5
train: 382, loss: 0.004110330336749005
train: 383, loss: 7.174044743653904E-6
train: 384, loss: 1.0895088658008957E-5
train: 385, loss: 1.824332142077177E-5
train: 386, loss: 5.75263755237083E-6
train: 387, loss: 1.6691663616050337E-4
train: 388, loss: 0.003062509425627797
train: 389, loss: 1.953569205113847E-5
train: 390, loss: 3.595850684797544E-5
train: 391, loss: 1.8864245462028468E-5
train: 392, loss: 1.9627349253398165E-5
train: 393, loss: 4.28965046271191E-5
train: 394, loss: 2.482255570103113E-5
train: 395, loss: 9.133491112792618E-5
train: 396, loss: 9.166097861300879E-6
train: 397, loss: 4.780733584432206E-6
train: 398, loss: 0.026068169728974515
train: 399, loss: 9.877572192826707E-6
train: 400, loss: 1.5767740137778607E-5
train: 401, loss: 5.055900433620574E-6
train: 402, loss: 2.960563102888013E-5
train: 403, loss: 5.010064433383056E-4
train: 404, loss: 1.07664717605792E-5
train: 405, loss: 5.493144644201598E-6
train: 406, loss: 4.4857440215038554E-6
train: 407, loss: 1.7171525266202996E-4
train: 408, loss: 2.1833030726136495E-5
train: 409, loss: 1.508875183379954E-5
train: 410, loss: 1.3958396007199806E-5
train: 411, loss: 2.549219937192786E-5
train: 412, loss: 0.029059438144075515
train: 413, loss: 9.274655271692853E-6
train: 414, loss: 4.8093533874215297E-4
train: 415, loss: 2.5349543266124585E-5
train: 416, loss: 5.6265873232872395E-5
train: 417, loss: 2.7453568786705035E-5
train: 418, loss: 1.947263114097199E-5
train: 419, loss: 5.10801081249159E-6
train: 420, loss: 4.757508331495146E-6
train: 421, loss: 8.990303540602554E-6
train: 422, loss: 1.821742547149238E-5
train: 423, loss: 3.3258569386274734E-5
train: 424, loss: 7.707044373758119E-6
train: 425, loss: 9.556995597402824E-6
train: 426, loss: 1.552277538078747E-5
train: 427, loss: 5.9640832816907855E-5
train: 428, loss: 5.6662798205948496E-6
train: 429, loss: 3.070999939629143E-5
train: 430, loss: 1.7870631750595728E-5
train: 431, loss: 7.008117045750657E-6
train: 432, loss: 1.4567338971566707E-5
train: 433, loss: 1.0185700139905424E-5
train: 434, loss: 3.7330512133731983E-6
train: 435, loss: 5.042367952383565E-6
train: 436, loss: 1.3871460682745497E-4
train: 437, loss: 3.96953485425998E-6
train: 438, loss: 8.43542105944008E-6
train: 439, loss: 1.1128877705450442E-4
train: 440, loss: 0.1384842823020631
train: 441, loss: 5.478400583319032E-6
train: 442, loss: 4.477855701986385E-6
train: 443, loss: 5.653282149917757E-6
train: 444, loss: 1.2701567330773781E-5
train: 445, loss: 3.195006244927874E-4
train: 446, loss: 4.812411926096984E-6
train: 447, loss: 4.884243440169753E-5
train: 448, loss: 4.834442078717111E-6
train: 449, loss: 1.1382687274422605E-5
train: 450, loss: 5.065161472865491E-6
train: 451, loss: 2.912243981373291E-5
train: 452, loss: 1.3794736272744637E-5
train: 453, loss: 6.910013562042141E-6
train: 454, loss: 1.6123032737569337E-5
train: 455, loss: 8.67350526443628E-6
train: 456, loss: 1.0657899703547676E-5
train: 457, loss: 0.02079577583429434
train: 458, loss: 1.3421266022140952E-5
train: 459, loss: 4.6768101447540015E-5
train: 460, loss: 5.073680410216232E-6
train: 461, loss: 1.0761553779525875E-5
train: 462, loss: 5.6898884499469106E-5
train: 463, loss: 1.607562197513651E-5
train: 464, loss: 2.192863293121222E-5
train: 465, loss: 4.410880450305385E-6
train: 466, loss: 3.686985534821694E-6
train: 467, loss: 8.38063986979384E-6
train: 468, loss: 4.964208496154897E-6
train: 469, loss: 7.709051649287939E-5
train: 470, loss: 0.20629554392183247
train: 471, loss: 1.1082721318756337E-5
train: 472, loss: 6.0122578286693276E-5
train: 473, loss: 1.076076866580009E-5
train: 474, loss: 3.558358870358503E-6
train: 475, loss: 4.3897705777420925E-5
train: 476, loss: 7.515852527288861E-6
train: 477, loss: 0.038708590471822346
train: 478, loss: 3.564980031529936E-6
train: 479, loss: 1.3308235024198942E-5
train: 480, loss: 8.004689843818797E-6
train: 481, loss: 8.631266672809865E-4
train: 482, loss: 3.2312924707071906E-5
train: 483, loss: 5.088937806646667E-6
train: 484, loss: 1.9574053054772353E-5
train: 485, loss: 5.857916472893259E-5
train: 486, loss: 0.22202318558426726
train: 487, loss: 9.04435105522299E-6
train: 488, loss: 1.0241284089108538E-5
train: 489, loss: 4.408576462594881E-6
train: 490, loss: 1.2284781066309127E-5
train: 491, loss: 7.325561100589583E-6
train: 492, loss: 9.628277523512723E-4
train: 493, loss: 1.20893713810744E-5
train: 494, loss: 0.21320033049038545
train: 495, loss: 3.968113226593897E-4
train: 496, loss: 1.1247062003151339E-5
train: 497, loss: 1.6026337193970474E-5
train: 498, loss: 1.0376401236169363E-5
train: 499, loss: 4.7783399267443055E-6
train: 500, loss: 7.143517025316695E-6
train: 501, loss: 3.060933828195918E-4
train: 502, loss: 1.4249885602371834E-4
train: 503, loss: 3.171000096088692E-4
train: 504, loss: 0.009727989575303238
train: 505, loss: 4.993380596045733E-6
train: 506, loss: 8.153685358759526E-6
train: 507, loss: 0.1411462007064533
train: 508, loss: 9.665569841221445E-6
train: 509, loss: 4.7157563079197505E-6
train: 510, loss: 8.441479931038219E-6
train: 511, loss: 4.3804703565061355E-6
train: 512, loss: 2.8769112580633067E-6
train: 513, loss: 0.0035319771435452915
train: 514, loss: 8.825579649501593E-6
train: 515, loss: 0.0027629262131647586
train: 516, loss: 9.89237569481158E-4
train: 517, loss: 1.462582233533878E-4
train: 518, loss: 0.011274394002319757
train: 519, loss: 1.464982593537485E-5
train: 520, loss: 3.359812308875472E-4
train: 521, loss: 9.03846514744238E-6
train: 522, loss: 3.952710127599755E-6
train: 523, loss: 5.8695915615609596E-5
train: 524, loss: 0.003896240962824673
train: 525, loss: 0.07616689310960033
train: 526, loss: 1.4776814133047169E-5
train: 527, loss: 3.301439872276159E-5
train: 528, loss: 9.188575812935375E-6
train: 529, loss: 1.1822635015471267E-5
train: 530, loss: 5.944040404083804E-6
train: 531, loss: 1.3328508881366824E-5
train: 532, loss: 9.974372869641108E-6
train: 533, loss: 5.140050288460075E-6
train: 534, loss: 9.631587360414563E-6
train: 535, loss: 0.0010891719408821561
train: 536, loss: 4.179544174549899E-6
train: 537, loss: 4.4437254753070525E-6
train: 538, loss: 3.060144728371242E-6
train: 539, loss: 4.2868929433244E-4
train: 540, loss: 3.291223345541807E-6
train: 541, loss: 6.806008766396206E-5
train: 542, loss: 1.0657046185706711E-5
train: 543, loss: 3.66306686204891E-6
train: 544, loss: 1.4423746840803928E-5
train: 545, loss: 3.6104268492236888E-6
train: 546, loss: 4.661730840882915E-6
train: 547, loss: 3.576961210674997E-6
train: 548, loss: 4.27773343397905E-6
train: 549, loss: 3.7189496589006446E-6
train: 550, loss: 5.621488499117151E-4
train: 551, loss: 1.11867177568056E-5
train: 552, loss: 9.660550720080216E-5
train: 553, loss: 6.173437888677503E-6
train: 554, loss: 2.597126270521006E-5
train: 555, loss: 4.564713646092269E-6
train: 556, loss: 5.124977723965426E-6
train: 557, loss: 1.3869378534476802E-5
train: 558, loss: 3.2679762299889092E-6
train: 559, loss: 1.2994986798936084E-5
train: 560, loss: 7.703274735539407E-6
train: 561, loss: 9.487457601900451E-6
train: 562, loss: 6.745276123745191E-6
train: 563, loss: 4.643466246584144E-6
train: 564, loss: 5.033241289373317E-6
train: 565, loss: 6.649154406210826E-6
train: 566, loss: 9.070187012860802E-6
train: 567, loss: 9.384835612337002E-6
train: 568, loss: 4.0778225912835956E-6
train: 569, loss: 1.4661586107162972E-5
train: 570, loss: 2.6989388725830898E-5
train: 571, loss: 9.19954595345729E-6
train: 572, loss: 6.309925735510703E-6
train: 573, loss: 4.303092424657477E-6
train: 574, loss: 7.257675283059547E-6
train: 575, loss: 8.951048621719287E-6
train: 576, loss: 3.855500843562169E-5
train: 577, loss: 2.171924718804519E-5
train: 578, loss: 8.410407011197412E-6
train: 579, loss: 0.0018490896998013112
train: 580, loss: 1.14033456269313E-5
train: 581, loss: 1.6948672902417727E-5
train: 582, loss: 1.0405249225937722E-4
train: 583, loss: 3.78601722710577E-6
train: 584, loss: 4.7125443080408435E-6
train: 585, loss: 2.3597103177958857E-5
train: 586, loss: 0.003481277819685557
train: 587, loss: 0.029657071241439573
train: 588, loss: 5.001448845870779E-6
train: 589, loss: 5.72016486244244E-6
train: 590, loss: 6.827580732447693E-6
train: 591, loss: 5.000253178500869E-6
train: 592, loss: 4.949417405459141E-6
train: 593, loss: 0.015945242341906668
train: 594, loss: 7.0553872722359264E-6
train: 595, loss: 5.162282825346401E-6
train: 596, loss: 0.009126487293339593
train: 597, loss: 8.595156790466959E-6
train: 598, loss: 4.443588184748284E-6
train: 599, loss: 1.324364769922608E-5
train: 600, loss: 4.2623193897432596E-5
train: 601, loss: 8.958292167540254E-6
train: 602, loss: 3.607464453619232E-6
train: 603, loss: 1.1384077614454935E-4
train: 604, loss: 6.369575505190599E-6
train: 605, loss: 7.359903433550452E-6
train: 606, loss: 0.12631266158980267
train: 607, loss: 7.453446222822009E-6
train: 608, loss: 4.509524294718219E-6
train: 609, loss: 1.7438971763466016E-5
train: 610, loss: 3.3731377784828714E-6
train: 611, loss: 3.626377067942916E-6
train: 612, loss: 6.588148299107579E-5
train: 613, loss: 7.806929772295104E-6
train: 614, loss: 7.403641062876923E-6
train: 615, loss: 3.611983310726965E-4
train: 616, loss: 4.202631692273002E-6
train: 617, loss: 1.0177433524162972E-5
train: 618, loss: 1.7266878845607505E-5
train: 619, loss: 1.6338972208389564E-5
train: 620, loss: 7.121863536346074E-6
train: 621, loss: 4.634212827100155E-6
train: 622, loss: 0.001127314810573161
train: 623, loss: 1.249410779149334E-5
train: 624, loss: 1.3350964122977584E-5
train: 625, loss: 6.603968213345373E-6
train: 626, loss: 4.264849784503266E-5
train: 627, loss: 0.010236987282023317
train: 628, loss: 4.4629477748784155E-6
train: 629, loss: 2.4084081134561636E-5
train: 630, loss: 2.658388020040687E-4
train: 631, loss: 6.342754869368458E-6
train: 632, loss: 2.465656742354518E-6
train: 633, loss: 0.006205969336494724
train: 634, loss: 0.025597715995660242
train: 635, loss: 3.255153014855931E-5
train: 636, loss: 5.9342074811837945E-6
train: 637, loss: 4.2235267195401245E-6
train: 638, loss: 4.044658355344908E-6
train: 639, loss: 4.796671705352381E-6
train: 640, loss: 0.005572738546632968
train: 641, loss: 9.29429958210665E-6
train: 642, loss: 3.7883732749980473E-6
train: 643, loss: 3.6102322133391025E-6
train: 644, loss: 6.350029021672279E-6
train: 645, loss: 0.04402378180629133
train: 646, loss: 8.550938423720811E-4
train: 647, loss: 0.0016325683643905677
train: 648, loss: 4.260300141836316E-6
train: 649, loss: 3.350248988580012E-6
train: 650, loss: 4.2855534426858625E-6
train: 651, loss: 0.011717619056718077
train: 652, loss: 3.9566125566383195E-6
train: 653, loss: 7.890125489454713E-6
train: 654, loss: 7.796810934806948E-6
train: 655, loss: 3.538310283678264E-5
train: 656, loss: 3.6068568485780427E-6
train: 657, loss: 3.483080235329993E-6
train: 658, loss: 8.273314310757273E-6
train: 659, loss: 6.675806522276504E-5
train: 660, loss: 1.8482872928194311E-6
train: 661, loss: 0.00489217907393674
train: 662, loss: 4.0634591254529085E-6
train: 663, loss: 1.2560497021777532E-5
train: 664, loss: 0.16264183351696168
train: 665, loss: 0.0020210858142975384
train: 666, loss: 6.303090566650617E-6
train: 667, loss: 1.1202048734079464E-5
train: 668, loss: 4.107486242907109E-6
train: 669, loss: 2.3025577988442977E-6
train: 670, loss: 7.217040293313497E-6
train: 671, loss: 5.9267773028398406E-6
train: 672, loss: 3.494965916802655E-6
train: 673, loss: 3.1258632811954716E-4
train: 674, loss: 3.2121415588699146E-6
train: 675, loss: 3.2413232633273422E-6
train: 676, loss: 5.52762632807207E-6
train: 677, loss: 4.2683689570312226E-5
train: 678, loss: 5.542280237038751E-6
train: 679, loss: 2.2521775930563572E-5
train: 680, loss: 5.446036303646538E-6
train: 681, loss: 7.569085575951073E-4
train: 682, loss: 5.219366873058244E-6
train: 683, loss: 0.08243618663680612
train: 684, loss: 1.3374936462705239E-5
train: 685, loss: 5.166232384600755E-6
train: 686, loss: 5.923436918187113E-6
train: 687, loss: 4.327304875927132E-6
train: 688, loss: 2.5323948172280086E-4
train: 689, loss: 4.6842650080317005E-6
train: 690, loss: 4.437729120655967E-6
train: 691, loss: 2.4435401906610083E-5
train: 692, loss: 0.23797219887751409
train: 693, loss: 9.194402933429736E-5
train: 694, loss: 7.512209375849378E-6
train: 695, loss: 5.365506939285968E-6
train: 696, loss: 5.931195318620331E-6
train: 697, loss: 6.479369770702458E-5
train: 698, loss: 2.972449915992938E-6
train: 699, loss: 6.667588040457854E-6
train: 700, loss: 9.538854886860953E-6
train: 701, loss: 9.872302457810631E-5
train: 702, loss: 4.54015449876735E-6
train: 703, loss: 3.1228233547694725E-6
train: 704, loss: 5.177170455612284E-6
train: 705, loss: 5.287062950953737E-6
train: 706, loss: 0.0026596575762338126
train: 707, loss: 5.335144078090269E-6
train: 708, loss: 7.098311291428271E-6
train: 709, loss: 1.1182172629061441E-4
train: 710, loss: 4.176590038229838E-6
train: 711, loss: 2.0737206004485848E-5
train: 712, loss: 4.866155317186319E-6
train: 713, loss: 6.384017211303637E-6
train: 714, loss: 4.743007219611014E-6
train: 715, loss: 4.60201690959409E-6
train: 716, loss: 3.2006991878146424E-4
train: 717, loss: 6.667947983819096E-6
train: 718, loss: 4.812603092309571E-6
train: 719, loss: 0.011113172818278526
train: 720, loss: 2.7880018816162154E-5
train: 721, loss: 9.382154371911107E-6
train: 722, loss: 3.822179578348232E-6
train: 723, loss: 3.770104486490892E-5
train: 724, loss: 4.963559945415537E-6
train: 725, loss: 4.189443436930067E-6
train: 726, loss: 0.08343009568678157
train: 727, loss: 5.005713711251751E-6
train: 728, loss: 4.180221044099998E-6
train: 729, loss: 5.570800073247738E-6
train: 730, loss: 1.6931717415790722E-5
train: 731, loss: 4.849299860543969E-6
train: 732, loss: 6.569187227706264E-5
train: 733, loss: 9.023422396643831E-5
train: 734, loss: 4.522207084827263E-6
train: 735, loss: 8.405125787609766E-6
train: 736, loss: 3.446184885312249E-6
train: 737, loss: 1.5130377110920478E-6
train: 738, loss: 3.671583467618823E-6
train: 739, loss: 5.56044082864665E-4
train: 740, loss: 3.958942898796105E-6
train: 741, loss: 1.5954953942753397E-5
train: 742, loss: 5.415882845944327E-6
train: 743, loss: 1.7151385468586468E-4
train: 744, loss: 3.3183674425343432E-6
train: 745, loss: 0.0021249271528887056
train: 746, loss: 4.768281150779982E-6
train: 747, loss: 1.3666384584268258E-5
train: 748, loss: 6.869466751086734E-5
train: 749, loss: 2.294841572938371E-5
train: 750, loss: 3.8082239171126567E-6
train: 751, loss: 0.015046840261190018
train: 752, loss: 1.8022746446247512E-6
train: 753, loss: 1.677838413460188E-6
train: 754, loss: 1.0306577019640457E-5
train: 755, loss: 1.0845283749769614E-5
train: 756, loss: 3.829201759578054E-6
train: 757, loss: 4.407309063762616E-6
train: 758, loss: 8.327588877810945E-6
train: 759, loss: 5.2777001507016366E-6
train: 760, loss: 5.468529761910976E-6
train: 761, loss: 2.7135116813508295E-6
train: 762, loss: 3.775883420585584E-6
train: 763, loss: 3.781475573723822E-6
train: 764, loss: 4.055892430473266E-6
train: 765, loss: 4.131151622981165E-6
train: 766, loss: 4.165041848468432E-6
train: 767, loss: 1.7254733569887258E-6
train: 768, loss: 2.0393627339895705E-6
train: 769, loss: 1.1367372323585277E-5
train: 770, loss: 1.4323565424946467E-5
train: 771, loss: 4.1186517453976E-6
train: 772, loss: 6.4762299678365966E-6
train: 773, loss: 4.455446037996398E-6
train: 774, loss: 0.006641425817897031
train: 775, loss: 1.1572015289350247E-4
train: 776, loss: 4.003284922933271E-6
train: 777, loss: 8.980563743266551E-5
train: 778, loss: 0.0036530100031289016
train: 779, loss: 3.928799415456591E-6
train: 780, loss: 1.4944815108555538E-5
train: 781, loss: 3.976541034699928E-6
train: 782, loss: 0.1730956347945947
train: 783, loss: 4.784229569794468E-6
train: 784, loss: 4.713786997092707E-6
train: 785, loss: 0.2084032783145596
train: 786, loss: 3.0594504072446365E-6
train: 787, loss: 1.0509771792355054E-5
train: 788, loss: 3.7251824056830105E-6
train: 789, loss: 3.795222284065077E-6
train: 790, loss: 6.8298992335901496E-6
train: 791, loss: 4.4135032545307423E-4
train: 792, loss: 4.074687283684703E-6
train: 793, loss: 3.658630275386109E-6
train: 794, loss: 2.1701111082598134E-4
train: 795, loss: 3.6063294727869646E-6
train: 796, loss: 0.002151486797258526
train: 797, loss: 4.1704442260628135E-6
train: 798, loss: 5.6368745858861165E-6
train: 799, loss: 1.7786338646317672E-5
train: 800, loss: 2.8855607547441672E-6
train: 801, loss: 3.879397897567934E-6
train: 802, loss: 6.4003792076730645E-6
train: 803, loss: 2.3880436162661186E-5
train: 804, loss: 5.314672953816753E-6
train: 805, loss: 3.6740764155185817E-6
train: 806, loss: 3.4273853459984656E-6
train: 807, loss: 4.135167877862484E-6
train: 808, loss: 1.0029180701900698E-5
train: 809, loss: 8.768941271934943E-6
train: 810, loss: 8.873795959222008E-5
train: 811, loss: 1.0313767905312112E-5
train: 812, loss: 3.919213404486764E-6
train: 813, loss: 0.002232014117289612
train: 814, loss: 1.1269530174714342E-5
train: 815, loss: 1.4893678683750798E-5
train: 816, loss: 1.1528471451632947E-4
train: 817, loss: 4.178050644702251E-6
train: 818, loss: 2.18917510267999E-4
train: 819, loss: 2.667471652350065E-6
train: 820, loss: 4.911106356753432E-6
train: 821, loss: 3.4420721914501473E-6
train: 822, loss: 1.99185327076685E-5
train: 823, loss: 7.482389697677948E-6
train: 824, loss: 0.014937428733525021
train: 825, loss: 1.86971063120859E-5
train: 826, loss: 9.46945611989721E-6
train: 827, loss: 4.198974736826035E-6
train: 828, loss: 2.523721088379934E-6
train: 829, loss: 4.181040780597058E-6
train: 830, loss: 2.54944701589599E-6
train: 831, loss: 2.4679283237072646E-5
train: 832, loss: 3.5061424296231827E-6
train: 833, loss: 2.528378578624252E-5
train: 834, loss: 9.613885328146333E-5
train: 835, loss: 1.2280213975449753E-5
train: 836, loss: 2.183993316424995E-4
train: 837, loss: 2.0186341235727847E-6
train: 838, loss: 2.6954610223796664E-5
train: 839, loss: 4.309153332956471E-6
train: 840, loss: 1.7996884293985407E-5
train: 841, loss: 4.2801014288374053E-4
train: 842, loss: 2.654437528581027E-6
train: 843, loss: 3.7115051385010888E-6
train: 844, loss: 1.0986272775674125E-5
train: 845, loss: 3.9698675385346034E-4
train: 846, loss: 3.808188180328523E-6
train: 847, loss: 3.137770045057909E-6
train: 848, loss: 1.2586495302714637E-5
train: 849, loss: 1.0225201744368012E-4
train: 850, loss: 1.490203427492083E-4
train: 851, loss: 1.1375643393422704E-6
train: 852, loss: 3.4556598424503048E-6
train: 853, loss: 3.0524473495638964E-6
train: 854, loss: 2.1415998394602073E-6
train: 855, loss: 5.162403337342943E-6
train: 856, loss: 4.414614372769673E-6
train: 857, loss: 2.9305975503209486E-6
train: 858, loss: 3.081476409904595E-5
train: 859, loss: 4.0715911381917165E-5
train: 860, loss: 3.2254032005461946E-6
train: 861, loss: 5.8713015572278625E-6
train: 862, loss: 3.7388223387293564E-6
train: 863, loss: 1.7838911232557917E-5
train: 864, loss: 3.288316273718921E-7
train: 865, loss: 2.495940057023568E-6
train: 866, loss: 0.014945817612575827
train: 867, loss: 2.9928552311594866E-4
train: 868, loss: 4.0284628208198103E-7
train: 869, loss: 2.8791838011996563E-4
train: 870, loss: 8.639786218667376E-4
train: 871, loss: 7.47648210005787E-6
train: 872, loss: 6.514216463038476E-6
train: 873, loss: 4.2674763989915126E-5
train: 874, loss: 4.468288751370625E-5
train: 875, loss: 2.1712755968253784E-6
train: 876, loss: 3.224323391799726E-6
train: 877, loss: 5.8867173129726336E-6
train: 878, loss: 2.6427105496215563E-6
train: 879, loss: 1.5025920728337545E-5
train: 880, loss: 1.2982827398240227E-5
train: 881, loss: 3.0760168845426365E-6
train: 882, loss: 1.8138767556666515E-5
train: 883, loss: 3.271598923236349E-6
train: 884, loss: 3.060275555135567E-6
train: 885, loss: 9.879752554089643E-7
train: 886, loss: 1.3437681944337985E-5
train: 887, loss: 2.9372931171094025E-6
train: 888, loss: 2.463475486283395E-6
train: 889, loss: 2.2939243524682174E-6
train: 890, loss: 7.166005423632865E-5
train: 891, loss: 7.582540418446355E-6
train: 892, loss: 1.389484601894252E-6
train: 893, loss: 0.11509506178196452
train: 894, loss: 3.0417647871578324E-6
train: 895, loss: 3.2192665406379424E-6
train: 896, loss: 2.5898371648489895E-6
train: 897, loss: 9.522597470988795E-6
train: 898, loss: 2.7905376925799043E-6
train: 899, loss: 2.9844566154734866E-6
train: 900, loss: 3.2279558215239013E-6
train: 901, loss: 4.596804033489669E-6
train: 902, loss: 0.291476985019355
train: 903, loss: 2.412137959181226E-6
train: 904, loss: 2.7295735295953215E-6
train: 905, loss: 1.3518123978079637E-5
train: 906, loss: 3.117944164974571E-6
train: 907, loss: 7.993164322726469E-7
train: 908, loss: 8.378962556023767E-7
train: 909, loss: 2.2658504047480473E-6
train: 910, loss: 4.865060614027902E-6
train: 911, loss: 1.7564475103897838E-6
train: 912, loss: 3.808848181212908E-5
train: 913, loss: 2.0357951075569854E-5
train: 914, loss: 0.16902905644025984
train: 915, loss: 2.8094071455114263E-6
train: 916, loss: 0.22551545715331292
train: 917, loss: 5.007731054318463E-6
train: 918, loss: 1.914838336418739E-6
train: 919, loss: 4.938041650096583E-6
train: 920, loss: 2.633702784086663E-4
train: 921, loss: 1.726584617021535E-5
train: 922, loss: 1.1311655776956823E-5
train: 923, loss: 2.582014783798409E-6
train: 924, loss: 1.7777966321516873E-5
train: 925, loss: 1.3729526743616847E-5
train: 926, loss: 2.8427303120025595E-6
train: 927, loss: 6.675281762647312E-5
train: 928, loss: 2.4964469377355645E-6
train: 929, loss: 3.0640390674557297E-6
train: 930, loss: 3.893125669530621E-6
train: 931, loss: 1.4585847312342305E-6
train: 932, loss: 3.6097723238447703E-6
train: 933, loss: 4.123579642120558E-6
train: 934, loss: 4.348178493533013E-4
train: 935, loss: 2.745542137747794E-6
train: 936, loss: 2.680167743323643E-6
train: 937, loss: 4.066694881869742E-5
train: 938, loss: 1.2430147880118194E-6
train: 939, loss: 0.007993661251756328
train: 940, loss: 2.911743326218832E-6
train: 941, loss: 1.086230202957721E-5
train: 942, loss: 3.8818896737380894E-6
train: 943, loss: 1.9722418166621198E-5
train: 944, loss: 3.953046174648671E-6
train: 945, loss: 7.260535029007836E-6
train: 946, loss: 3.164824403615498E-6
train: 947, loss: 9.710770360695277E-7
train: 948, loss: 5.982130626214295E-7
train: 949, loss: 2.275628476477806E-6
train: 950, loss: 0.11576968433988573
train: 951, loss: 3.7778297897878148E-6
train: 952, loss: 3.534382511150514E-6
train: 953, loss: 2.9859587963862783E-6
train: 954, loss: 0.0014539002931657577
train: 955, loss: 1.4834677408236339E-5
train: 956, loss: 8.866920294710262E-6
train: 957, loss: 4.3814149135102694E-4
train: 958, loss: 3.596454884644525E-6
train: 959, loss: 1.1428933335991082E-5
train: 960, loss: 2.493158982018673E-4
train: 961, loss: 3.282713282552089E-6
train: 962, loss: 5.088108169275389E-6
train: 963, loss: 2.0237526107741765E-6
train: 964, loss: 2.394995179220392E-5
train: 965, loss: 4.909474155664772E-6
train: 966, loss: 6.66209913649704E-6
train: 967, loss: 4.931679419446221E-6
train: 968, loss: 0.04294751680414635
train: 969, loss: 4.108598972742561E-6
train: 970, loss: 3.572440445296666E-6
train: 971, loss: 0.033107136421760046
train: 972, loss: 3.007941042832656E-7
train: 973, loss: 4.01313937612952E-6
train: 974, loss: 3.658862705732415E-6
train: 975, loss: 5.390130629857504E-6
train: 976, loss: 2.811027878772074E-6
train: 977, loss: 5.1144619979183264E-5
train: 978, loss: 0.00395799584933583
train: 979, loss: 4.82176785959842E-6
train: 980, loss: 5.694330120561245E-6
train: 981, loss: 1.229157941666443E-4
train: 982, loss: 2.359989867311581E-6
train: 983, loss: 7.20033287951488E-5
train: 984, loss: 4.732404533057952E-6
train: 985, loss: 6.4200545231094376E-6
train: 986, loss: 0.02208481176398317
train: 987, loss: 5.926182897109723E-7
train: 988, loss: 9.185417213503736E-6
train: 989, loss: 6.235063933468033E-4
train: 990, loss: 1.7800597865658254E-6
train: 991, loss: 0.00242333428489208
train: 992, loss: 1.3601977298969054E-6
train: 993, loss: 2.010256578735489E-6
train: 994, loss: 7.257288035382663E-4
train: 995, loss: 2.426396639036239E-7
train: 996, loss: 0.0016335665674855487
train: 997, loss: 3.5581824084122495E-6
train: 998, loss: 1.1535341373570793E-6
train: 999, loss: 1.7800385385726654E-6
train: 1000, loss: 0.2398294803096697
trainTime: 4374
==========Finish Train==========
==========Start Predict==========
label: -1.0, output: 0.9968041754270308, predict: 1.0
label: -1.0, output: 0.11346670496860076, predict: 0.0
label: -1.0, output: 0.0019429307022966272, predict: 0.0
label: -1.0, output: 0.9984285757140583, predict: 1.0
label: -1.0, output: 0.002025694401596812, predict: 0.0
label: -1.0, output: 0.9981571477148612, predict: 1.0
label: -1.0, output: 0.9968769562447839, predict: 1.0
label: -1.0, output: 0.002333049532730313, predict: 0.0
label: -1.0, output: 0.0031989286104343513, predict: 0.0
label: -1.0, output: 0.9970916848849998, predict: 1.0
label: -1.0, output: 0.0012824160339422466, predict: 0.0
label: -1.0, output: 0.01175679264091537, predict: 0.0
label: -1.0, output: 0.965366085089197, predict: 1.0
label: -1.0, output: 0.0023250689536829057, predict: 0.0
label: -1.0, output: 0.5614183494961666, predict: 1.0
label: -1.0, output: 0.0014905930308223194, predict: 0.0
label: -1.0, output: 0.001892840830116621, predict: 0.0
label: -1.0, output: 0.002013606868687359, predict: 0.0
label: -1.0, output: 0.9971732576856077, predict: 1.0
label: -1.0, output: 0.9964531138908188, predict: 1.0
label: -1.0, output: 0.008761351185597888, predict: 0.0
label: -1.0, output: 0.0018947748294014515, predict: 0.0
label: -1.0, output: 0.9972824928330629, predict: 1.0
label: -1.0, output: 0.9975992073757283, predict: 1.0
label: -1.0, output: 0.0015210660341610487, predict: 0.0
label: -1.0, output: 0.9973703455538993, predict: 1.0
label: -1.0, output: 0.9973795873780475, predict: 1.0
label: -1.0, output: 0.003739362913614433, predict: 0.0
label: -1.0, output: 0.001724166319158386, predict: 0.0
label: -1.0, output: 0.9973410086384595, predict: 1.0
label: -1.0, output: 0.0024130836677731045, predict: 0.0
label: -1.0, output: 0.004569527905259609, predict: 0.0
label: -1.0, output: 0.9971433618039134, predict: 1.0
label: -1.0, output: 0.9971613219511848, predict: 1.0
label: -1.0, output: 0.9982189467136882, predict: 1.0
label: -1.0, output: 0.9939291235827442, predict: 1.0
label: -1.0, output: 0.0030300770224964617, predict: 0.0
label: -1.0, output: 0.9962553464208272, predict: 1.0
label: -1.0, output: 0.0016708970515004072, predict: 0.0
label: -1.0, output: 0.9971730021940731, predict: 1.0
label: -1.0, output: 0.0261194837140004, predict: 0.0
label: -1.0, output: 0.0010430298442447475, predict: 0.0
label: -1.0, output: 0.0050578869283802695, predict: 0.0
label: -1.0, output: 0.002378281282814989, predict: 0.0
label: -1.0, output: 0.9977221666462709, predict: 1.0
label: -1.0, output: 0.0012253277140300734, predict: 0.0
label: -1.0, output: 0.006547166020192532, predict: 0.0
label: -1.0, output: 0.975822809169549, predict: 1.0
label: -1.0, output: 0.9976273803705463, predict: 1.0
label: -1.0, output: 0.002108439415476557, predict: 0.0
label: -1.0, output: 0.0038719937750135262, predict: 0.0
label: -1.0, output: 0.0025167748680302512, predict: 0.0
label: -1.0, output: 0.0028879028696453027, predict: 0.0
label: -1.0, output: 0.999078573607708, predict: 1.0
label: -1.0, output: 0.997109733476844, predict: 1.0
label: -1.0, output: 0.9974687330925638, predict: 1.0
label: -1.0, output: 0.9969810022241679, predict: 1.0
label: -1.0, output: 0.9711265342213757, predict: 1.0
label: -1.0, output: 0.006364230520171196, predict: 0.0
label: -1.0, output: 0.01610265620200623, predict: 0.0
label: -1.0, output: 0.9967395172022944, predict: 1.0
label: -1.0, output: 0.0033793440873286, predict: 0.0
label: -1.0, output: 0.9957707004277319, predict: 1.0
label: -1.0, output: 0.9968326921541016, predict: 1.0
label: -1.0, output: 0.997506733519739, predict: 1.0
label: -1.0, output: 0.0016410380088180655, predict: 0.0
label: -1.0, output: 0.004642643325560778, predict: 0.0
label: -1.0, output: 0.07747483635900226, predict: 0.0
label: -1.0, output: 0.006056539715086687, predict: 0.0
label: -1.0, output: 0.002603132903997868, predict: 0.0
label: -1.0, output: 0.005820766379003844, predict: 0.0
label: -1.0, output: 0.983078672758005, predict: 1.0
label: -1.0, output: 0.0024466695680247035, predict: 0.0
label: -1.0, output: 0.001160001809501943, predict: 0.0
label: -1.0, output: 0.997543760526566, predict: 1.0
label: -1.0, output: 0.9990158974378499, predict: 1.0
label: -1.0, output: 0.0019641243142978085, predict: 0.0
label: -1.0, output: 0.9983650544789564, predict: 1.0
label: -1.0, output: 0.9777797719001028, predict: 1.0
label: -1.0, output: 0.009451164246995422, predict: 0.0
label: -1.0, output: 0.0018132320863873523, predict: 0.0
label: -1.0, output: 0.0020702391827432482, predict: 0.0
label: -1.0, output: 0.0057085390416089, predict: 0.0
label: -1.0, output: 0.9975833430013878, predict: 1.0
label: -1.0, output: 0.006638905953797086, predict: 0.0
label: -1.0, output: 0.9992488198052959, predict: 1.0
label: -1.0, output: 0.9945490117593418, predict: 1.0
label: -1.0, output: 0.9973388675588303, predict: 1.0
label: -1.0, output: 0.9192765773472247, predict: 1.0
label: -1.0, output: 0.9610596629544269, predict: 1.0
label: -1.0, output: 0.0019921154977670863, predict: 0.0
label: -1.0, output: 0.0016615282627275324, predict: 0.0
label: -1.0, output: 0.9968399748888308, predict: 1.0
label: -1.0, output: 9.263628114599636E-4, predict: 0.0
label: -1.0, output: 0.004760392227598581, predict: 0.0
label: -1.0, output: 0.9975928249031079, predict: 1.0
label: -1.0, output: 0.06762049268684887, predict: 0.0
label: -1.0, output: 0.999114891841988, predict: 1.0
label: -1.0, output: 0.01496008249000338, predict: 0.0
label: -1.0, output: 0.5439913233450945, predict: 1.0
label: -1.0, output: 0.001910209704930978, predict: 0.0
label: -1.0, output: 0.9970455886509592, predict: 1.0
label: -1.0, output: 0.012890661665190631, predict: 0.0
label: -1.0, output: 0.9990302211157948, predict: 1.0
label: -1.0, output: 0.011647252845950496, predict: 0.0
label: -1.0, output: 0.861833655842149, predict: 1.0
label: -1.0, output: 0.9979954844705383, predict: 1.0
label: -1.0, output: 0.9968804052008413, predict: 1.0
label: -1.0, output: 6.684548829341721E-4, predict: 0.0
label: -1.0, output: 0.997139968864755, predict: 1.0
label: -1.0, output: 0.007900624210590962, predict: 0.0
label: -1.0, output: 0.9990012416815308, predict: 1.0
label: -1.0, output: 0.002022642743539435, predict: 0.0
label: -1.0, output: 0.9972659584951403, predict: 1.0
label: -1.0, output: 0.0010465541620788142, predict: 0.0
label: -1.0, output: 0.012346131286045888, predict: 0.0
label: -1.0, output: 0.0021186730090791616, predict: 0.0
label: -1.0, output: 0.9974649101908444, predict: 1.0
label: -1.0, output: 0.006227233525290908, predict: 0.0
label: -1.0, output: 0.9986389976388518, predict: 1.0
label: -1.0, output: 0.0012928928080464027, predict: 0.0
label: -1.0, output: 0.9922725915255702, predict: 1.0
label: -1.0, output: 0.9955532100550244, predict: 1.0
label: -1.0, output: 0.9978062714843089, predict: 1.0
label: -1.0, output: 0.007244594546286538, predict: 0.0
label: -1.0, output: 0.9967204748966908, predict: 1.0
label: -1.0, output: 0.06414035696808716, predict: 0.0
label: -1.0, output: 0.9973657917903532, predict: 1.0
label: -1.0, output: 0.9973100646775921, predict: 1.0
label: -1.0, output: 0.005498234368850223, predict: 0.0
label: -1.0, output: 0.0059908277183589615, predict: 0.0
label: -1.0, output: 0.9969360411938163, predict: 1.0
label: -1.0, output: 0.001774864809712069, predict: 0.0
label: -1.0, output: 0.002234785977652466, predict: 0.0
label: -1.0, output: 0.9981921107936726, predict: 1.0
label: -1.0, output: 0.007881025376723676, predict: 0.0
label: -1.0, output: 0.9971244651350601, predict: 1.0
label: -1.0, output: 0.9990945399394762, predict: 1.0
label: -1.0, output: 0.09262963845983789, predict: 0.0
label: -1.0, output: 0.1806064920969714, predict: 0.0
label: -1.0, output: 0.997546104762109, predict: 1.0
label: -1.0, output: 0.9972261193377538, predict: 1.0
label: -1.0, output: 0.002633099177158684, predict: 0.0
label: -1.0, output: 0.9966781106604756, predict: 1.0
label: -1.0, output: 0.9614916464424594, predict: 1.0
label: -1.0, output: 0.04851924674696125, predict: 0.0
label: -1.0, output: 0.9963901409278125, predict: 1.0
label: -1.0, output: 0.016375290551955957, predict: 0.0
label: -1.0, output: 0.998155845896682, predict: 1.0
label: -1.0, output: 0.9969483404576773, predict: 1.0
label: -1.0, output: 0.9976867109481288, predict: 1.0
label: -1.0, output: 0.9973297360657822, predict: 1.0
label: -1.0, output: 0.0018022740691559726, predict: 0.0
label: -1.0, output: 0.94958630575138, predict: 1.0
label: -1.0, output: 0.0016092266445441794, predict: 0.0
label: -1.0, output: 0.0020559157241501493, predict: 0.0
label: -1.0, output: 0.0011192287295190262, predict: 0.0
label: -1.0, output: 0.002716548120952919, predict: 0.0
label: -1.0, output: 0.002108576797935577, predict: 0.0
label: -1.0, output: 0.9970217736391153, predict: 1.0
label: -1.0, output: 8.032916243502559E-4, predict: 0.0
label: -1.0, output: 0.9974209367569682, predict: 1.0
label: -1.0, output: 0.9970039307554992, predict: 1.0
label: -1.0, output: 0.9960735091173657, predict: 1.0
label: -1.0, output: 0.0017492414302886397, predict: 0.0
label: -1.0, output: 0.999031544497133, predict: 1.0
label: -1.0, output: 0.002694379776447804, predict: 0.0
label: -1.0, output: 0.5137409095318208, predict: 1.0
label: -1.0, output: 0.014286992554560746, predict: 0.0
label: -1.0, output: 0.9975505254488697, predict: 1.0
label: -1.0, output: 0.9977457325405226, predict: 1.0
label: -1.0, output: 0.9972571973507508, predict: 1.0
label: -1.0, output: 0.9983874332312536, predict: 1.0
label: -1.0, output: 0.9948910372728421, predict: 1.0
label: -1.0, output: 0.9972521297111164, predict: 1.0
label: -1.0, output: 0.007922142654340322, predict: 0.0
label: -1.0, output: 0.9967318327050821, predict: 1.0
label: -1.0, output: 0.9986179435619901, predict: 1.0
label: -1.0, output: 0.9122817752088701, predict: 1.0
label: -1.0, output: 0.00270401577589331, predict: 0.0
label: -1.0, output: 0.9973135013545319, predict: 1.0
label: -1.0, output: 0.9970266162537955, predict: 1.0
label: -1.0, output: 0.002370529872077158, predict: 0.0
label: -1.0, output: 0.0322809095097606, predict: 0.0
label: -1.0, output: 0.0021282581517950666, predict: 0.0
label: -1.0, output: 0.010931124359596091, predict: 0.0
label: -1.0, output: 0.9989327181259283, predict: 1.0
label: -1.0, output: 0.0026524225601948573, predict: 0.0
label: -1.0, output: 0.9966388750055187, predict: 1.0
label: -1.0, output: 0.0015072919002822027, predict: 0.0
label: -1.0, output: 0.9973642220521524, predict: 1.0
label: -1.0, output: 0.003154550242304426, predict: 0.0
label: -1.0, output: 0.9971389951812211, predict: 1.0
label: -1.0, output: 0.996432270154704, predict: 1.0
label: -1.0, output: 0.0017267260810680043, predict: 0.0
label: -1.0, output: 0.9952257064096267, predict: 1.0
label: -1.0, output: 0.9851394908164054, predict: 1.0
label: -1.0, output: 0.0013623332392550301, predict: 0.0
label: -1.0, output: 0.9915269414387119, predict: 1.0
label: -1.0, output: 0.6084206328632137, predict: 1.0
label: -1.0, output: 0.9969095358051154, predict: 1.0
label: -1.0, output: 0.9687312925997512, predict: 1.0
label: -1.0, output: 0.9971576221380708, predict: 1.0
label: -1.0, output: 0.9945227191529189, predict: 1.0
label: -1.0, output: 0.9966858552599189, predict: 1.0
label: -1.0, output: 0.9979419124721852, predict: 1.0
label: -1.0, output: 0.022169624199710386, predict: 0.0
label: -1.0, output: 0.9963482695495544, predict: 1.0
label: -1.0, output: 0.007429327058798588, predict: 0.0
label: -1.0, output: 0.9929977859412203, predict: 1.0
label: -1.0, output: 0.996790542840597, predict: 1.0
label: -1.0, output: 0.9976889393237431, predict: 1.0
label: -1.0, output: 0.002104478911407736, predict: 0.0
label: -1.0, output: 0.008586045926690928, predict: 0.0
label: -1.0, output: 0.0020387198611186118, predict: 0.0
label: -1.0, output: 0.0024541390212226965, predict: 0.0
label: -1.0, output: 9.918636344743218E-4, predict: 0.0
label: -1.0, output: 0.997240188037334, predict: 1.0
label: -1.0, output: 0.9971511373142352, predict: 1.0
label: -1.0, output: 0.003222694312109479, predict: 0.0
label: -1.0, output: 0.001601942963720024, predict: 0.0
label: -1.0, output: 0.003679241056395714, predict: 0.0
label: -1.0, output: 0.6615877534839214, predict: 1.0
label: -1.0, output: 0.9969735774818822, predict: 1.0
label: -1.0, output: 0.0027115517530286767, predict: 0.0
label: -1.0, output: 9.868445933509603E-4, predict: 0.0
label: -1.0, output: 0.9989539839194337, predict: 1.0
label: -1.0, output: 0.9993183884296853, predict: 1.0
label: -1.0, output: 0.9986859570232599, predict: 1.0
label: -1.0, output: 0.9973503233145132, predict: 1.0
label: -1.0, output: 0.0021199091740508797, predict: 0.0
label: -1.0, output: 0.0023542658824809203, predict: 0.0
label: -1.0, output: 0.0016145408635357779, predict: 0.0
label: -1.0, output: 0.9975254988660767, predict: 1.0
label: -1.0, output: 0.2999742723977237, predict: 0.0
label: -1.0, output: 0.9974109735817599, predict: 1.0
label: -1.0, output: 0.9970555776262913, predict: 1.0
label: -1.0, output: 0.9943364460805489, predict: 1.0
label: -1.0, output: 0.9969971095740647, predict: 1.0
label: -1.0, output: 0.0035949906945728907, predict: 0.0
label: -1.0, output: 0.004998058344276402, predict: 0.0
label: -1.0, output: 0.002129216396101073, predict: 0.0
label: -1.0, output: 0.9975921742537711, predict: 1.0
label: -1.0, output: 0.9986701495888469, predict: 1.0
label: -1.0, output: 0.002507740875678392, predict: 0.0
label: -1.0, output: 0.002444233421895678, predict: 0.0
label: -1.0, output: 0.9975849601522104, predict: 1.0
label: -1.0, output: 0.9990241123906257, predict: 1.0
label: -1.0, output: 0.0016886732588040371, predict: 0.0
label: -1.0, output: 0.9970870840980581, predict: 1.0
label: -1.0, output: 0.9959367243152794, predict: 1.0
label: -1.0, output: 0.002216544358761416, predict: 0.0
label: -1.0, output: 0.002280301867803118, predict: 0.0
label: -1.0, output: 0.05659927546918976, predict: 0.0
label: -1.0, output: 0.994877756408984, predict: 1.0
label: -1.0, output: 0.0035022019301145057, predict: 0.0
label: -1.0, output: 0.9973226715754566, predict: 1.0
label: -1.0, output: 0.9983103180733969, predict: 1.0
label: -1.0, output: 0.004167191838998972, predict: 0.0
label: -1.0, output: 0.0028997975260200482, predict: 0.0
label: -1.0, output: 0.9984731358436815, predict: 1.0
label: -1.0, output: 0.18060567865892968, predict: 0.0
label: -1.0, output: 0.0028268440451634583, predict: 0.0
label: -1.0, output: 0.002805340294293182, predict: 0.0
label: -1.0, output: 0.0020493021205646895, predict: 0.0
label: -1.0, output: 0.32724771600430674, predict: 0.0
label: -1.0, output: 0.9979497952204367, predict: 1.0
label: -1.0, output: 0.9981937698445846, predict: 1.0
label: -1.0, output: 0.015123961033031605, predict: 0.0
label: -1.0, output: 0.0019600405795542316, predict: 0.0
label: -1.0, output: 0.03338298405446517, predict: 0.0
label: -1.0, output: 0.997432949715311, predict: 1.0
label: -1.0, output: 0.0057385207219626646, predict: 0.0
label: -1.0, output: 0.9973024269165691, predict: 1.0
label: -1.0, output: 0.02241118743273735, predict: 0.0
label: -1.0, output: 0.9973974577984925, predict: 1.0
label: -1.0, output: 0.9505280885002367, predict: 1.0
label: -1.0, output: 0.99810277364754, predict: 1.0
label: -1.0, output: 0.0014932341533462293, predict: 0.0
label: -1.0, output: 0.9972619278687131, predict: 1.0
label: -1.0, output: 0.003550176825616291, predict: 0.0
label: -1.0, output: 0.9978271673077365, predict: 1.0
label: -1.0, output: 0.049203237338616486, predict: 0.0
label: -1.0, output: 0.6251630725958425, predict: 1.0
label: -1.0, output: 0.007161036132680715, predict: 0.0
label: -1.0, output: 0.9946257266345664, predict: 1.0
label: -1.0, output: 0.06453650016841062, predict: 0.0
label: -1.0, output: 0.0030801074987669903, predict: 0.0
label: -1.0, output: 0.0022483229414824095, predict: 0.0
label: -1.0, output: 0.0018782904047827422, predict: 0.0
label: -1.0, output: 0.997311080978029, predict: 1.0
label: -1.0, output: 0.999076078864616, predict: 1.0
label: -1.0, output: 0.9971443672176074, predict: 1.0
label: -1.0, output: 0.9942588935628748, predict: 1.0
label: -1.0, output: 0.9971124242753066, predict: 1.0
label: -1.0, output: 0.9985095217156529, predict: 1.0
label: -1.0, output: 0.02893075256153705, predict: 0.0
label: -1.0, output: 0.0017118315522443122, predict: 0.0
label: -1.0, output: 0.002522274504812205, predict: 0.0
label: -1.0, output: 0.0019968987545534366, predict: 0.0
label: -1.0, output: 0.9973576686871811, predict: 1.0
label: -1.0, output: 0.8655263253925212, predict: 1.0
label: -1.0, output: 0.0017996051097721212, predict: 0.0
label: -1.0, output: 0.006382186564628645, predict: 0.0
label: -1.0, output: 0.9915043227021397, predict: 1.0
label: -1.0, output: 0.9630662151025157, predict: 1.0
label: -1.0, output: 0.9970739883908435, predict: 1.0
label: -1.0, output: 0.9969611802121382, predict: 1.0
label: -1.0, output: 0.9848688012981541, predict: 1.0
label: -1.0, output: 0.003035925849935555, predict: 0.0
label: -1.0, output: 0.0025874916641147362, predict: 0.0
label: -1.0, output: 0.9978697191531959, predict: 1.0
label: -1.0, output: 8.335817298727354E-4, predict: 0.0
label: -1.0, output: 0.014352109083820922, predict: 0.0
label: -1.0, output: 0.997189998503997, predict: 1.0
label: -1.0, output: 0.002029472257022316, predict: 0.0
label: -1.0, output: 0.0018400158863281653, predict: 0.0
label: -1.0, output: 0.0020586115708513828, predict: 0.0
label: -1.0, output: 0.8344982569214612, predict: 1.0
label: -1.0, output: 0.0013181338317179655, predict: 0.0
label: -1.0, output: 0.9985490965787784, predict: 1.0
label: -1.0, output: 0.0021262803423318013, predict: 0.0
label: -1.0, output: 0.9970055614613574, predict: 1.0
label: -1.0, output: 0.9971494985481975, predict: 1.0
label: -1.0, output: 0.004196970760772644, predict: 0.0
label: -1.0, output: 0.9980086232232688, predict: 1.0
label: -1.0, output: 0.004628971987814803, predict: 0.0
label: -1.0, output: 0.0015242891900271354, predict: 0.0
label: -1.0, output: 0.9974868797957739, predict: 1.0
label: -1.0, output: 0.9975155756966535, predict: 1.0
label: -1.0, output: 0.9935132017389318, predict: 1.0
label: -1.0, output: 0.0013639429144716934, predict: 0.0
label: -1.0, output: 0.9980146895059381, predict: 1.0
label: -1.0, output: 0.9855781978189828, predict: 1.0
label: -1.0, output: 0.0021289589892408605, predict: 0.0
label: -1.0, output: 0.006735947401152553, predict: 0.0
label: -1.0, output: 0.0025977781462811008, predict: 0.0
label: -1.0, output: 0.9961555277284591, predict: 1.0
label: -1.0, output: 0.00342306054370873, predict: 0.0
label: -1.0, output: 0.0033780620790908595, predict: 0.0
label: -1.0, output: 0.0014184986645960568, predict: 0.0
label: -1.0, output: 0.002093400955759147, predict: 0.0
label: -1.0, output: 0.9972437230176937, predict: 1.0
label: -1.0, output: 0.9972344035128033, predict: 1.0
label: -1.0, output: 0.0012755436850913934, predict: 0.0
label: -1.0, output: 0.0021297337328601493, predict: 0.0
label: -1.0, output: 0.9967004634573748, predict: 1.0
label: -1.0, output: 0.9975099692910212, predict: 1.0
label: -1.0, output: 0.0017537268551218225, predict: 0.0
label: -1.0, output: 0.9966709778414, predict: 1.0
label: -1.0, output: 0.9970798842501465, predict: 1.0
label: -1.0, output: 0.0022153395492613796, predict: 0.0
label: -1.0, output: 0.9986701647985371, predict: 1.0
label: -1.0, output: 0.0020313329869971765, predict: 0.0
label: -1.0, output: 0.0025361676223149823, predict: 0.0
label: -1.0, output: 0.9981217412431092, predict: 1.0
label: -1.0, output: 0.9959613540146782, predict: 1.0
label: -1.0, output: 0.054035328822216226, predict: 0.0
label: -1.0, output: 0.0024176057243615393, predict: 0.0
label: -1.0, output: 0.001305951843892748, predict: 0.0
label: -1.0, output: 0.9977211941258852, predict: 1.0
label: -1.0, output: 0.004943908429443446, predict: 0.0
label: -1.0, output: 0.998833032989652, predict: 1.0
label: -1.0, output: 0.00218436142395113, predict: 0.0
label: -1.0, output: 0.9974311823196196, predict: 1.0
label: -1.0, output: 0.996104665865356, predict: 1.0
label: -1.0, output: 0.997462097460382, predict: 1.0
label: -1.0, output: 0.9975633131262883, predict: 1.0
label: -1.0, output: 0.0011427619871865868, predict: 0.0
label: -1.0, output: 0.9990444443469195, predict: 1.0
label: -1.0, output: 0.9973124057963538, predict: 1.0
label: -1.0, output: 0.06716378203013425, predict: 0.0
label: -1.0, output: 0.002653364403982799, predict: 0.0
label: -1.0, output: 0.997125576740527, predict: 1.0
label: -1.0, output: 0.9971220205351452, predict: 1.0
label: -1.0, output: 0.823099828461345, predict: 1.0
label: -1.0, output: 0.01259100214170466, predict: 0.0
label: -1.0, output: 0.0013238725449551622, predict: 0.0
label: -1.0, output: 0.0020497681636719536, predict: 0.0
label: -1.0, output: 0.9966253097565932, predict: 1.0
label: -1.0, output: 0.9972783745126246, predict: 1.0
label: -1.0, output: 0.9973475900897546, predict: 1.0
label: -1.0, output: 0.9964007696581865, predict: 1.0
label: -1.0, output: 0.9096845887658618, predict: 1.0
label: -1.0, output: 0.9972412922451233, predict: 1.0
label: -1.0, output: 0.8514218565881873, predict: 1.0
label: -1.0, output: 0.5295963268861822, predict: 1.0
label: -1.0, output: 0.0030585759484262538, predict: 0.0
label: -1.0, output: 0.9979804310590644, predict: 1.0
label: -1.0, output: 0.0021822395789885675, predict: 0.0
label: -1.0, output: 0.0016341643028013008, predict: 0.0
label: -1.0, output: 0.0018093799885043522, predict: 0.0
label: -1.0, output: 0.001716809279207585, predict: 0.0
label: -1.0, output: 0.015747215940586905, predict: 0.0
label: -1.0, output: 0.9972920683946448, predict: 1.0
label: -1.0, output: 0.9972130571450117, predict: 1.0
label: -1.0, output: 0.9973951470497484, predict: 1.0
label: -1.0, output: 0.6763877566476785, predict: 1.0
label: -1.0, output: 0.006016086847114098, predict: 0.0
label: -1.0, output: 0.001839604924413959, predict: 0.0
label: -1.0, output: 0.002228999907103361, predict: 0.0
label: -1.0, output: 0.0022047789311106453, predict: 0.0
label: -1.0, output: 0.004987802679973187, predict: 0.0
label: -1.0, output: 0.0021670132176971023, predict: 0.0
label: -1.0, output: 0.0016014554541319977, predict: 0.0
label: -1.0, output: 0.9971422406556905, predict: 1.0
label: -1.0, output: 0.9977827668221796, predict: 1.0
label: -1.0, output: 0.9973500809583145, predict: 1.0
label: -1.0, output: 0.0019552468884196854, predict: 0.0
label: -1.0, output: 0.0029205905992899565, predict: 0.0
label: -1.0, output: 0.9985700428378269, predict: 1.0
label: -1.0, output: 0.9955012046492921, predict: 1.0
label: -1.0, output: 0.004178850049880631, predict: 0.0
label: -1.0, output: 0.9972313785355386, predict: 1.0
label: -1.0, output: 0.99736877500268, predict: 1.0
label: -1.0, output: 0.998052827877139, predict: 1.0
label: -1.0, output: 0.9973802471123302, predict: 1.0
label: -1.0, output: 0.9978108640146737, predict: 1.0
label: -1.0, output: 0.05522805003223344, predict: 0.0
label: -1.0, output: 0.019794661342301494, predict: 0.0
label: -1.0, output: 0.003486093114861676, predict: 0.0
label: -1.0, output: 0.9981510534290393, predict: 1.0
label: -1.0, output: 0.03724245305776212, predict: 0.0
label: -1.0, output: 0.0013198187210448954, predict: 0.0
label: -1.0, output: 0.997784088745955, predict: 1.0
label: -1.0, output: 0.00261108857165993, predict: 0.0
label: -1.0, output: 0.9967981382110706, predict: 1.0
label: -1.0, output: 0.9974814043397126, predict: 1.0
label: -1.0, output: 0.8958002043959246, predict: 1.0
label: -1.0, output: 0.0015221055954232577, predict: 0.0
label: -1.0, output: 0.9958482534003179, predict: 1.0
label: -1.0, output: 0.0027569915033898986, predict: 0.0
label: -1.0, output: 0.0018584372301469734, predict: 0.0
label: -1.0, output: 0.9974464587045534, predict: 1.0
label: -1.0, output: 0.0021623732187612205, predict: 0.0
label: -1.0, output: 0.9949562272351319, predict: 1.0
label: -1.0, output: 0.9971012567466196, predict: 1.0
label: -1.0, output: 0.0014757003672127116, predict: 0.0
label: -1.0, output: 0.9971769580261912, predict: 1.0
label: -1.0, output: 0.002452951604796228, predict: 0.0
label: -1.0, output: 0.006182663410774382, predict: 0.0
label: -1.0, output: 0.006244300848188371, predict: 0.0
label: -1.0, output: 0.9971926961077637, predict: 1.0
label: -1.0, output: 0.003249451532926769, predict: 0.0
label: -1.0, output: 0.007480016710976202, predict: 0.0
label: -1.0, output: 0.9694142601886637, predict: 1.0
label: -1.0, output: 0.0020725273953679994, predict: 0.0
label: -1.0, output: 0.028389775001797887, predict: 0.0
label: -1.0, output: 0.00222111660259345, predict: 0.0
label: -1.0, output: 0.9906779697731591, predict: 1.0
label: -1.0, output: 0.9966422228604309, predict: 1.0
label: -1.0, output: 9.346003057387937E-4, predict: 0.0
label: -1.0, output: 0.005315564932415994, predict: 0.0
label: -1.0, output: 0.0026505979640404267, predict: 0.0
label: -1.0, output: 0.9970561188014756, predict: 1.0
label: -1.0, output: 0.0019307113890359956, predict: 0.0
label: -1.0, output: 0.003732031130705738, predict: 0.0
label: -1.0, output: 0.13071957145983087, predict: 0.0
label: -1.0, output: 0.001445012833362524, predict: 0.0
label: -1.0, output: 0.998660258309828, predict: 1.0
label: -1.0, output: 0.9977256490774521, predict: 1.0
label: -1.0, output: 0.9987861319899316, predict: 1.0
label: -1.0, output: 0.003013484359935642, predict: 0.0
label: -1.0, output: 0.6824292118050501, predict: 1.0
label: -1.0, output: 0.9971453644098912, predict: 1.0
label: -1.0, output: 0.0023101827613786737, predict: 0.0
label: -1.0, output: 0.002258398399507951, predict: 0.0
label: -1.0, output: 0.002115896558116765, predict: 0.0
label: -1.0, output: 0.00942271849469427, predict: 0.0
label: -1.0, output: 0.9991582533859955, predict: 1.0
label: -1.0, output: 0.9982898320414852, predict: 1.0
label: -1.0, output: 0.9974022476752721, predict: 1.0
label: -1.0, output: 0.0022251494044703652, predict: 0.0
label: -1.0, output: 0.9964671000708473, predict: 1.0
label: -1.0, output: 0.003806567010805425, predict: 0.0
label: -1.0, output: 0.0022370299754002642, predict: 0.0
label: -1.0, output: 0.0056700467048733435, predict: 0.0
label: -1.0, output: 0.015548946699541561, predict: 0.0
label: -1.0, output: 0.9928687862814496, predict: 1.0
label: -1.0, output: 0.0016622339807766011, predict: 0.0
label: -1.0, output: 0.9941690371899632, predict: 1.0
label: -1.0, output: 0.9614342349007888, predict: 1.0
label: -1.0, output: 0.9830437448883608, predict: 1.0
label: -1.0, output: 0.997128804467065, predict: 1.0
label: -1.0, output: 0.012250915597219894, predict: 0.0
label: -1.0, output: 0.0029272292957414525, predict: 0.0
label: -1.0, output: 0.0024980910313163276, predict: 0.0
label: -1.0, output: 0.001671097118963689, predict: 0.0
label: -1.0, output: 9.62309387468863E-4, predict: 0.0
label: -1.0, output: 0.11084421418249146, predict: 0.0
label: -1.0, output: 0.9971787562624015, predict: 1.0
label: -1.0, output: 0.997228902284264, predict: 1.0
label: -1.0, output: 0.0025948172204844373, predict: 0.0
label: -1.0, output: 0.0028585967546897787, predict: 0.0
label: -1.0, output: 0.06359787126860494, predict: 0.0
label: -1.0, output: 0.9974782424985346, predict: 1.0
label: -1.0, output: 0.002021535022174214, predict: 0.0
label: -1.0, output: 0.0021767013616558364, predict: 0.0
label: -1.0, output: 0.002139503861709323, predict: 0.0
label: -1.0, output: 0.9974624783941779, predict: 1.0
total: 0.0, correct: 0.0, wrong: 0.0, accurate: 0.0
predictTime: 99
==========Finish Predict==========
==========Start WritePredict==========
writePredictTime: 4976
==========Finish WritePredict==========
==========train 0==========
train: 0, loss: -1.0
layerType: input, nodeSize: 5
double[][] w: null
double[] b: null
double[][] partialDerivative: null
double[] z: null
double[] h: 
[3.242074293613774, 2.016352321144595, 6.537164027251356, -0.10031263887879129, 0.9023711937473464]
layerType: hidden, nodeSize: 8
double[][] w: 
[2.6805238936620994, 2.0849739647236514, -0.43083644249190894, -1.8282807682386764, 1.6032072571870406]
[-0.8180500055446264, 0.2744690697632882, -0.5509072619957115, -0.35753868461344107, 0.5296658294312365]
[0.6889340130832977, -0.6748175066158204, -0.8833597092493798, 0.29672801058539383, 0.011190142929965606]
[-0.013029360571224736, -0.16297718101941144, -1.211996354143744, 0.5240841674107077, -0.17667806211535053]
[1.402444777448089, 1.2677008403561052, -0.36867028831424375, -1.0039826003633896, 0.03998906339554926]
[-0.5549795394205768, 0.7924923272761852, -0.09003943368833747, 0.8144446658903643, 0.7203796711941886]
[0.703031099392721, 0.47323063559216094, -0.5277029410729747, 0.21131823908448252, -0.8761135250130165]
[-1.9910635020322047, -1.0203111269035678, -0.4154244618106183, 1.2380790697668136, 1.469650446893267]
double[] b: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[][] partialDerivative: 
[0.07424377611153535, 0.07424377611153535, 0.07424377611153535, 0.07424377611153535, 0.07424377611153535]
[-0.010084915963253194, -0.010084915963253194, -0.010084915963253194, -0.010084915963253194, -0.010084915963253194]
[0.020844890001660035, 0.020844890001660035, 0.020844890001660035, 0.020844890001660035, 0.020844890001660035]
[-0.01895104046569782, -0.01895104046569782, -0.01895104046569782, -0.01895104046569782, -0.01895104046569782]
[0.07354185701167913, 0.07354185701167913, 0.07354185701167913, 0.07354185701167913, 0.07354185701167913]
[-0.01826105149536336, -0.01826105149536336, -0.01826105149536336, -0.01826105149536336, -0.01826105149536336]
[0.033462435574898065, 0.033462435574898065, 0.033462435574898065, 0.033462435574898065, 0.033462435574898065]
[-0.22015939826253594, -0.22015939826253594, -0.22015939826253594, -0.22015939826253594, -0.22015939826253594]
double[] z: 
[11.708138923834564, -5.186302847691268, -4.9214298135657035, -8.505881999328121, 4.829700669365989, -0.22159293139532665, -1.0279794356218108, -10.026205104176034]
double[] h: 
[0.9999917734773217, 0.005561541769894215, 0.007235961144184107, 2.0223417416662893E-4, 0.9920744046513876, 0.44482734646880245, 0.26347602019519356, 4.4223717009235486E-5]
layerType: output, nodeSize: 1
double[][] w: 
[3.8111084360618794, -0.30624377822576687, 0.5899147362971521, -0.5189597563386958, 2.18371827615222, -0.5613136824032651, 0.9262278598824077, -6.173943845489456]
double[] b: 
[0.0]
double[][] partialDerivative: 
[0.1474595611259944, 0.1474595611259944, 0.1474595611259944, 0.1474595611259944, 0.1474595611259944, 0.1474595611259944, 0.1474595611259944, 0.1474595611259944]
double[] z: 
[5.974026673470273]
double[] h: 
[0.9974624783941779]
==============================
programTotalTime: 4977
==========Start ReadInput==========
readInputTime: 439
==========Finish ReadInput==========
==========Start BuildNetwork==========
buildNetworkTime: 5
==========Finish BuildNetwork==========
==========train 0==========
train: 0, loss: -1.0
layerType: input, nodeSize: 5
double[][] w: null
double[] b: null
double[][] partialDerivative: null
double[] z: null
double[] h: 
[0.0, 0.0, 0.0, 0.0, 0.0]
layerType: hidden, nodeSize: 8
double[][] w: 
[-0.2244935725566266, 0.1913729527519632, 0.20111217484565103, 0.4332979687308882, 0.4297837588653577]
[0.8150068102067602, 0.5860226530077564, -0.729635512743086, 0.3616569664452671, 0.32814731802471386]
[0.5780437385455985, -0.3410100446209716, 0.9213462331004643, -0.6962708008884799, 0.5373388455969477]
[-0.01869793202701442, -0.42426483452947417, -0.687373867881153, -0.23490022462406657, 0.9877247049187656]
[0.6721355191560718, 0.231521597213302, 0.8539252530076149, -0.5697605231950609, 0.9551893982144701]
[0.6624526895109555, 0.6476905263054695, -0.009909741838155739, -0.22528563916261346, -0.2851789267124998]
[0.17547130096647567, -0.9397051899731113, 0.35393707163023436, 0.7229298934892334, 0.7619758569723698]
[0.9218425579010334, 0.23170166326436714, -0.39312924638299673, 0.6673291363821114, 0.6434737260837196]
double[] b: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[][] partialDerivative: 
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
double[] z: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[] h: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
layerType: output, nodeSize: 1
double[][] w: 
[0.6995728812414614, 0.3275270759624771, 0.8437797519105228, -0.21240488277063507, 0.09283443020742688, 0.9818282990317824, 0.6185694594965832, 0.10035899787588187]
double[] b: 
[0.0]
double[][] partialDerivative: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[] z: 
[0.0]
double[] h: 
[0.0]
==============================
==========Start Train==========
train: 1, loss: 0.0034650271805110656
train: 2, loss: 0.005911407104672736
train: 3, loss: 9.65838555976392E-4
train: 4, loss: 9.155382543935391E-4
train: 5, loss: 0.0032077050655120423
train: 6, loss: 3.026487644094489E-4
train: 7, loss: 9.224169122297128E-4
train: 8, loss: 2.6986251964456815E-4
train: 9, loss: 1.637603767222103E-4
train: 10, loss: 1.6271539289657843E-4
train: 11, loss: 3.2972161371512575E-4
train: 12, loss: 1.523415361915014E-4
train: 13, loss: 0.005276170319502168
train: 14, loss: 0.008530314634964242
train: 15, loss: 2.551512722593066E-4
train: 16, loss: 0.0149976722377688
train: 17, loss: 0.03370266961018787
train: 18, loss: 0.02567084474327798
train: 19, loss: 0.0011859017531504557
train: 20, loss: 2.0887172454265747E-4
train: 21, loss: 2.0781490701552861E-4
train: 22, loss: 8.783589523366276E-5
train: 23, loss: 9.023218439760142E-5
train: 24, loss: 0.007662737081287972
train: 25, loss: 0.0012522382612228824
train: 26, loss: 0.019769141830361887
train: 27, loss: 7.308990953091263E-5
train: 28, loss: 2.4160934433781255E-4
train: 29, loss: 0.00950366431013228
train: 30, loss: 1.41316806244852E-4
train: 31, loss: 0.008102450288026749
train: 32, loss: 1.3477035489929098E-4
train: 33, loss: 0.013326259390458195
train: 34, loss: 8.404591543752637E-4
train: 35, loss: 1.2011321140361306E-4
train: 36, loss: 6.182609023223652E-5
train: 37, loss: 1.2165801081133504E-4
train: 38, loss: 5.16828238417458E-4
train: 39, loss: 5.4164556333166586E-5
train: 40, loss: 1.1065546699431834E-4
train: 41, loss: 7.516400459511268E-5
train: 42, loss: 0.04056099970259087
train: 43, loss: 9.464243197392819E-5
train: 44, loss: 1.9990080088277991E-4
train: 45, loss: 0.22012231631889032
train: 46, loss: 0.2089996850206972
train: 47, loss: 0.004522958434525381
train: 48, loss: 2.343121894371356E-4
train: 49, loss: 9.217274433183733E-5
train: 50, loss: 0.0010052178018452567
train: 51, loss: 6.049365872466759E-5
train: 52, loss: 6.895369119870955E-4
train: 53, loss: 7.439117798657371E-5
train: 54, loss: 0.05613557601027143
train: 55, loss: 8.813655746759795E-5
train: 56, loss: 1.5539354224882164E-4
train: 57, loss: 0.016265142380805366
train: 58, loss: 0.0026413066382282194
train: 59, loss: 8.432488300725584E-5
train: 60, loss: 7.967497168520171E-5
train: 61, loss: 0.001323544112105998
train: 62, loss: 1.2315225142052756E-4
train: 63, loss: 1.1685418720890212E-4
train: 64, loss: 8.274014215178542E-5
train: 65, loss: 7.794810097499118E-4
train: 66, loss: 9.07666801885582E-4
train: 67, loss: 0.0018808286785008549
train: 68, loss: 6.821962364601045E-5
train: 69, loss: 2.4065411533602865E-4
train: 70, loss: 9.826052280217985E-5
train: 71, loss: 4.8149913905279015E-4
train: 72, loss: 6.77072224279703E-5
train: 73, loss: 0.0025820667941940357
train: 74, loss: 1.7999505814971742E-4
train: 75, loss: 0.03128910608074643
train: 76, loss: 6.0995095027799324E-5
train: 77, loss: 0.0014372578809613598
train: 78, loss: 2.8153764691032338E-5
train: 79, loss: 0.039593781108539186
train: 80, loss: 0.15687849193654071
train: 81, loss: 0.009463035836294183
train: 82, loss: 5.870532715005377E-5
train: 83, loss: 3.9236479149749806E-5
train: 84, loss: 8.174287763880654E-4
train: 85, loss: 5.451116135488328E-5
train: 86, loss: 0.001545106355322419
train: 87, loss: 7.744836247292049E-5
train: 88, loss: 7.717873451350012E-5
train: 89, loss: 5.2346583337723804E-5
train: 90, loss: 4.964553382633682E-4
train: 91, loss: 2.4636336326042285E-5
train: 92, loss: 8.006285163970671E-4
train: 93, loss: 2.0947443816969545E-5
train: 94, loss: 2.316368164927451E-5
train: 95, loss: 0.016751102762089572
train: 96, loss: 5.658409132540305E-5
train: 97, loss: 1.1944062073999603E-4
train: 98, loss: 4.6126448817048057E-4
train: 99, loss: 0.014832860949472467
train: 100, loss: 4.36284079214322E-5
train: 101, loss: 2.177728291451619E-5
train: 102, loss: 1.1354999063271864E-4
train: 103, loss: 2.1031150320730702E-5
train: 104, loss: 0.001405425204060328
train: 105, loss: 1.0159145158112004E-4
train: 106, loss: 4.1854887500516874E-5
train: 107, loss: 3.851994427337945E-5
train: 108, loss: 0.018008514102763797
train: 109, loss: 6.075764980799666E-5
train: 110, loss: 4.3224544556421486E-5
train: 111, loss: 4.325562428098199E-5
train: 112, loss: 0.038551614760197646
train: 113, loss: 0.002439426922885618
train: 114, loss: 2.983762953429202E-5
train: 115, loss: 0.025284185877919942
train: 116, loss: 0.0014676537919248362
train: 117, loss: 4.188047655302691E-5
train: 118, loss: 0.0012861508249130192
train: 119, loss: 2.1686067239207275E-4
train: 120, loss: 0.019803539302429462
train: 121, loss: 1.8026588596005568E-4
train: 122, loss: 1.7784782446422647E-5
train: 123, loss: 1.8437150112916692E-5
train: 124, loss: 1.6581070575597107E-5
train: 125, loss: 1.7730716950284478E-5
train: 126, loss: 8.348450338944348E-5
train: 127, loss: 3.6614153594815164E-4
train: 128, loss: 4.998955137384173E-5
train: 129, loss: 0.011891201022826797
train: 130, loss: 4.2511722984627366E-5
train: 131, loss: 7.398528750302801E-5
train: 132, loss: 3.886229015335127E-5
train: 133, loss: 0.32105037441585044
train: 134, loss: 2.3103443862809147E-5
train: 135, loss: 1.766049931443547E-5
train: 136, loss: 0.2614816915086774
train: 137, loss: 0.0035386645877720654
train: 138, loss: 0.17436388077921053
train: 139, loss: 1.9987929955829763E-4
train: 140, loss: 0.024231120656425385
train: 141, loss: 1.6769678244927508E-5
train: 142, loss: 3.641940509888101E-5
train: 143, loss: 1.718032672442437E-5
train: 144, loss: 6.677489999637415E-5
train: 145, loss: 3.529142964149844E-5
train: 146, loss: 1.780134005736232E-5
train: 147, loss: 0.003128961495714049
train: 148, loss: 7.718216599650083E-5
train: 149, loss: 1.5320639039173787E-4
train: 150, loss: 1.2847090885525543E-4
train: 151, loss: 1.6160409058399937E-4
train: 152, loss: 1.7184470428414797E-5
train: 153, loss: 1.4762212267754366E-4
train: 154, loss: 6.696273184372086E-4
train: 155, loss: 3.6482561779805245E-5
train: 156, loss: 2.3710068424210356E-4
train: 157, loss: 0.045502963313569324
train: 158, loss: 4.2988334541882304E-4
train: 159, loss: 0.35184043172244983
train: 160, loss: 6.782606049512953E-4
train: 161, loss: 4.107341842671307E-4
train: 162, loss: 3.44402458008447E-5
train: 163, loss: 0.0019704015172057766
train: 164, loss: 3.548961390731179E-5
train: 165, loss: 1.4427056641878455E-5
train: 166, loss: 1.7225179420121402E-4
train: 167, loss: 1.4572883570887033E-5
train: 168, loss: 2.0068885746371443E-5
train: 169, loss: 3.6011428766310906E-5
train: 170, loss: 3.461314230903009E-5
train: 171, loss: 3.8571207341621805E-5
train: 172, loss: 0.0016435700859584095
train: 173, loss: 0.0012058724095861991
train: 174, loss: 3.486597720781295E-5
train: 175, loss: 4.472061853797165E-5
train: 176, loss: 0.002511511376114501
train: 177, loss: 3.4086047521307593E-5
train: 178, loss: 1.4492456886031034E-5
train: 179, loss: 3.347298819126685E-5
train: 180, loss: 3.3283071852374636E-5
train: 181, loss: 3.3438328900502456E-5
train: 182, loss: 3.659801863825625E-5
train: 183, loss: 3.2878397003672664E-4
train: 184, loss: 1.4695669144376694E-5
train: 185, loss: 0.04042565476486915
train: 186, loss: 3.4835774471809066E-5
train: 187, loss: 0.27673213139306524
train: 188, loss: 0.0330376709111888
train: 189, loss: 8.859814716377785E-5
train: 190, loss: 0.002466166022313533
train: 191, loss: 2.1937196230252537E-4
train: 192, loss: 3.3990498197173516E-5
train: 193, loss: 1.321934038745823E-5
train: 194, loss: 0.046027978779251646
train: 195, loss: 3.258538953119722E-5
train: 196, loss: 0.0037086762195685167
train: 197, loss: 3.3691516434663215E-5
train: 198, loss: 0.05298425423509919
train: 199, loss: 0.08245892970365915
train: 200, loss: 0.01611434506116327
train: 201, loss: 3.359796518169017E-5
train: 202, loss: 9.466553705522444E-4
train: 203, loss: 1.4103096701541914E-5
train: 204, loss: 3.238816321455411E-5
train: 205, loss: 0.32643175490056164
train: 206, loss: 0.002097966514822421
train: 207, loss: 2.1716506923744673E-5
train: 208, loss: 3.682062697076468E-5
train: 209, loss: 2.1358700333174073E-5
train: 210, loss: 0.019719885160204204
train: 211, loss: 1.350713306569097E-5
train: 212, loss: 1.535342230671953E-5
train: 213, loss: 3.2410569808945026E-5
train: 214, loss: 3.074685134263581E-5
train: 215, loss: 3.0756114639315397E-5
train: 216, loss: 3.4290988820206236E-5
train: 217, loss: 1.4116689080435633E-5
train: 218, loss: 3.224816766876626E-5
train: 219, loss: 2.0656705737430616E-5
train: 220, loss: 3.141018820483887E-5
train: 221, loss: 0.43077310238183547
train: 222, loss: 3.174104053864886E-5
train: 223, loss: 0.0041565918874547645
train: 224, loss: 0.40770780998240197
train: 225, loss: 1.243872863798611E-5
train: 226, loss: 0.005647650532394562
train: 227, loss: 1.125936517972969E-4
train: 228, loss: 0.00909022438238717
train: 229, loss: 2.6277708700247034E-4
train: 230, loss: 3.075417638114467E-5
train: 231, loss: 2.596675631688428E-5
train: 232, loss: 4.1826311895443096E-5
train: 233, loss: 7.405429188691408E-4
train: 234, loss: 2.174575199137371E-4
train: 235, loss: 1.235738438110975E-4
train: 236, loss: 0.15166411450356074
train: 237, loss: 3.0107342965951163E-5
train: 238, loss: 0.019495685577719656
train: 239, loss: 0.0036836746820367258
train: 240, loss: 0.005134468293262466
train: 241, loss: 2.9071425622302207E-5
train: 242, loss: 2.49637643908407E-5
train: 243, loss: 0.07590180088464743
train: 244, loss: 0.03738088625171699
train: 245, loss: 0.011173063520563055
train: 246, loss: 0.4309533758451722
train: 247, loss: 9.728894586859794E-5
train: 248, loss: 1.1151241898866896E-5
train: 249, loss: 2.7593698074583574E-4
train: 250, loss: 1.158017465546225E-5
train: 251, loss: 0.005319655895445391
train: 252, loss: 0.012310938168567289
train: 253, loss: 2.6064296258665965E-5
train: 254, loss: 1.3467215002618914E-5
train: 255, loss: 1.1429712219954256E-5
train: 256, loss: 2.7508880551629176E-5
train: 257, loss: 0.2482205056188771
train: 258, loss: 5.862797428503171E-4
train: 259, loss: 1.238758882027467E-4
train: 260, loss: 0.05374099249461509
train: 261, loss: 1.4193162112628827E-4
train: 262, loss: 1.1253721011602874E-5
train: 263, loss: 1.1166527984715642E-5
train: 264, loss: 5.755635997582224E-5
train: 265, loss: 0.006919776631658928
train: 266, loss: 1.581507962992399E-5
train: 267, loss: 0.002222370678778652
train: 268, loss: 3.9163236412879596E-4
train: 269, loss: 0.0014476304188002446
train: 270, loss: 0.07823712685782133
train: 271, loss: 0.31530621423736116
train: 272, loss: 7.508239601863094E-4
train: 273, loss: 1.2347642885957803E-5
train: 274, loss: 1.4371023164091969E-5
train: 275, loss: 2.683730344208494E-5
train: 276, loss: 0.0014077448503618097
train: 277, loss: 1.0430420329947389E-4
train: 278, loss: 6.941990331563705E-4
train: 279, loss: 4.480124869752701E-5
train: 280, loss: 0.30684907142040646
train: 281, loss: 0.005098770839471614
train: 282, loss: 2.6023065189418518E-5
train: 283, loss: 2.4778717826937145E-5
train: 284, loss: 3.1004803240554233E-4
train: 285, loss: 4.20610729481367E-5
train: 286, loss: 6.905013133302797E-4
train: 287, loss: 0.02103247811193627
train: 288, loss: 2.5130817030782732E-5
train: 289, loss: 1.126413734621663E-5
train: 290, loss: 0.001256720207253854
train: 291, loss: 2.5439053666404845E-5
train: 292, loss: 2.59027763522312E-5
train: 293, loss: 2.593741275022198E-5
train: 294, loss: 9.229827209101672E-6
train: 295, loss: 0.0032077385328016074
train: 296, loss: 0.003088054742104227
train: 297, loss: 9.531891522455371E-6
train: 298, loss: 1.091013182409813E-5
train: 299, loss: 6.273403809167683E-5
train: 300, loss: 4.245538300654419E-5
train: 301, loss: 3.891046850985074E-5
train: 302, loss: 0.005602262585748149
train: 303, loss: 2.35226675877947E-5
train: 304, loss: 5.266638774770279E-4
train: 305, loss: 0.3656174996787525
train: 306, loss: 0.0015915035609519125
train: 307, loss: 0.01486444910255888
train: 308, loss: 9.618025009896787E-6
train: 309, loss: 5.839227909764456E-4
train: 310, loss: 9.401299134819903E-6
train: 311, loss: 2.307183885987151E-5
train: 312, loss: 8.898087907897492E-6
train: 313, loss: 3.141616468420162E-5
train: 314, loss: 0.029704007097634322
train: 315, loss: 2.3461263026144228E-5
train: 316, loss: 1.8001047878824192E-5
train: 317, loss: 0.31828253513203253
train: 318, loss: 3.111855381896922E-4
train: 319, loss: 0.070925612979698
train: 320, loss: 2.5734537604690706E-5
train: 321, loss: 1.964614239698135E-5
train: 322, loss: 2.2423737547488888E-5
train: 323, loss: 1.4940349401207736E-5
train: 324, loss: 0.005412556380974056
train: 325, loss: 7.988653685271919E-4
train: 326, loss: 2.2895151853063906E-5
train: 327, loss: 0.23544829090518646
train: 328, loss: 3.0513222668817508E-5
train: 329, loss: 1.6868560086551274E-5
train: 330, loss: 0.039012461196673065
train: 331, loss: 4.183793360342833E-4
train: 332, loss: 4.770380608696739E-5
train: 333, loss: 2.4432353255161477E-5
train: 334, loss: 3.965841418193325E-4
train: 335, loss: 2.2542299915452857E-5
train: 336, loss: 0.0512654451421135
train: 337, loss: 2.3562872241255E-5
train: 338, loss: 0.03720512558026581
train: 339, loss: 0.010573179131363002
train: 340, loss: 0.00365114101861874
train: 341, loss: 1.1286011475016412E-5
train: 342, loss: 0.014309519685691136
train: 343, loss: 8.346900116195874E-6
train: 344, loss: 7.589609461700566E-5
train: 345, loss: 5.13608152020809E-4
train: 346, loss: 2.7908278494674863E-5
train: 347, loss: 2.0912525397025188E-5
train: 348, loss: 0.008817884167972611
train: 349, loss: 2.857295397842299E-5
train: 350, loss: 9.869323473531899E-6
train: 351, loss: 3.482790545539175E-5
train: 352, loss: 8.114120614702373E-6
train: 353, loss: 1.9950836779963334E-5
train: 354, loss: 1.67088164011586E-4
train: 355, loss: 7.666194401437772E-6
train: 356, loss: 0.32801010597426655
train: 357, loss: 3.3510429466614654E-4
train: 358, loss: 2.1238074677381028E-5
train: 359, loss: 0.3233082572894879
train: 360, loss: 2.0954430811785436E-5
train: 361, loss: 2.1525457210434753E-5
train: 362, loss: 2.3896160952690184E-5
train: 363, loss: 2.832358750006266E-5
train: 364, loss: 5.689157303288724E-4
train: 365, loss: 2.032991310144338E-5
train: 366, loss: 2.2535503099594772E-4
train: 367, loss: 2.0514655102670384E-5
train: 368, loss: 2.9036063230946583E-5
train: 369, loss: 5.672964529499805E-4
train: 370, loss: 2.0037348327234818E-5
train: 371, loss: 2.704040012331105E-5
train: 372, loss: 0.002555774667144763
train: 373, loss: 3.815672752230726E-5
train: 374, loss: 8.913693780412457E-5
train: 375, loss: 0.2910722312363807
train: 376, loss: 7.883289241309936E-5
train: 377, loss: 0.0013345080042840092
train: 378, loss: 0.05551696734907876
train: 379, loss: 2.0081994704764943E-5
train: 380, loss: 0.30972705811422335
train: 381, loss: 1.8819379414650475E-5
train: 382, loss: 0.011009084972344834
train: 383, loss: 0.00971547221161716
train: 384, loss: 2.8167767453827244E-5
train: 385, loss: 0.0010410249528762937
train: 386, loss: 2.1387703485908575E-5
train: 387, loss: 2.7032810860620174E-4
train: 388, loss: 0.041068201812395996
train: 389, loss: 2.3068900818363517E-5
train: 390, loss: 0.36993728912022733
train: 391, loss: 1.941811075988588E-5
train: 392, loss: 1.1653941615437613E-4
train: 393, loss: 5.185369197121489E-4
train: 394, loss: 0.01555136066802449
train: 395, loss: 1.802835106832345E-5
train: 396, loss: 0.011713119728987533
train: 397, loss: 0.32916847464917165
train: 398, loss: 1.126626857304147E-5
train: 399, loss: 3.59796499752534E-5
train: 400, loss: 0.0013518225845770846
train: 401, loss: 3.1333979835823196E-5
train: 402, loss: 2.057103290036564E-5
train: 403, loss: 1.8698666283825082E-5
train: 404, loss: 0.0015684589241296936
train: 405, loss: 4.6087399783256955E-4
train: 406, loss: 1.8081824711710736E-5
train: 407, loss: 1.8200122939818057E-5
train: 408, loss: 8.623942037813784E-6
train: 409, loss: 2.0825848265254557E-4
train: 410, loss: 0.2507335937370421
train: 411, loss: 4.7505109444725074E-4
train: 412, loss: 0.05630993440533474
train: 413, loss: 4.059039423974938E-4
train: 414, loss: 0.03306158574976476
train: 415, loss: 0.0019117366951981367
train: 416, loss: 2.567238150677885E-4
train: 417, loss: 0.0036917185653384337
train: 418, loss: 0.005307697979856641
train: 419, loss: 4.685759776893105E-5
train: 420, loss: 1.3560705529410726E-4
train: 421, loss: 0.02733412638916141
train: 422, loss: 0.043660980754453786
train: 423, loss: 2.82443441396849E-5
train: 424, loss: 3.715463933767014E-5
train: 425, loss: 4.577686590700458E-4
train: 426, loss: 0.21470491973734654
train: 427, loss: 0.04785634223666361
train: 428, loss: 6.446948314082002E-6
train: 429, loss: 1.4765391512370795E-5
train: 430, loss: 0.024512174128299355
train: 431, loss: 7.174562036303623E-6
train: 432, loss: 8.108542309739996E-6
train: 433, loss: 5.396828733655411E-4
train: 434, loss: 0.011483248457578856
train: 435, loss: 2.1371268593616417E-4
train: 436, loss: 1.5903745021432432E-5
train: 437, loss: 5.07696463875225E-4
train: 438, loss: 0.029851805706121864
train: 439, loss: 1.5775744143649648E-5
train: 440, loss: 1.2762983448941782E-5
train: 441, loss: 0.13845822328676202
train: 442, loss: 0.014542816416096314
train: 443, loss: 8.56565470899489E-6
train: 444, loss: 0.005885312851689416
train: 445, loss: 2.0987434733610693E-4
train: 446, loss: 1.5309683293538577E-5
train: 447, loss: 0.0013680633248058368
train: 448, loss: 0.037717366956710456
train: 449, loss: 2.594482242950385E-5
train: 450, loss: 1.2950834739153994E-5
train: 451, loss: 0.01918913393539541
train: 452, loss: 2.6359745277565877E-5
train: 453, loss: 5.920004530623632E-6
train: 454, loss: 1.5289566970820958E-5
train: 455, loss: 0.016854622929225987
train: 456, loss: 1.701918593289801E-5
train: 457, loss: 7.730403477643295E-6
train: 458, loss: 4.712538315130935E-4
train: 459, loss: 0.05703204560039197
train: 460, loss: 5.972273990339845E-6
train: 461, loss: 0.005526537379788957
train: 462, loss: 3.4725502361286414E-4
train: 463, loss: 2.1538619856125357E-5
train: 464, loss: 7.40077980680014E-6
train: 465, loss: 0.004552195396951931
train: 466, loss: 5.078996412939816E-6
train: 467, loss: 0.019776445910032272
train: 468, loss: 6.98514574480267E-6
train: 469, loss: 5.252040986623379E-6
train: 470, loss: 1.7140073690941175E-5
train: 471, loss: 5.624524686624526E-6
train: 472, loss: 1.5251725892699783E-5
train: 473, loss: 0.01904780797037499
train: 474, loss: 0.004691459582251291
train: 475, loss: 0.01677789264769155
train: 476, loss: 1.731492928520887E-4
train: 477, loss: 0.004252466580231116
train: 478, loss: 0.00914960306610417
train: 479, loss: 3.1278582044870647E-4
train: 480, loss: 1.6807159102515546E-5
train: 481, loss: 1.5730632421522274E-4
train: 482, loss: 1.907847314027818E-5
train: 483, loss: 0.03223391123018034
train: 484, loss: 2.1887507975159785E-5
train: 485, loss: 0.033403116656031566
train: 486, loss: 5.207788573794197E-6
train: 487, loss: 0.14160853139101237
train: 488, loss: 0.011747879536163516
train: 489, loss: 0.005133313325308967
train: 490, loss: 1.4415015703809979E-5
train: 491, loss: 1.1567255152405102E-5
train: 492, loss: 5.3250443959371454E-6
train: 493, loss: 0.023212176180234342
train: 494, loss: 0.0019703920107120893
train: 495, loss: 7.184842778868487E-6
train: 496, loss: 2.35153219722558E-5
train: 497, loss: 0.011593857178824095
train: 498, loss: 8.15253651009245E-4
train: 499, loss: 6.648048078443742E-5
train: 500, loss: 4.768523906847782E-6
train: 501, loss: 1.4297704794412783E-5
train: 502, loss: 0.004118561565222362
train: 503, loss: 4.9031109289028824E-6
train: 504, loss: 0.0030294799960219727
train: 505, loss: 1.745321378182176E-5
train: 506, loss: 7.99980114905432E-6
train: 507, loss: 4.661318937170521E-6
train: 508, loss: 2.0667602581015685E-5
train: 509, loss: 0.0017495679432184327
train: 510, loss: 4.710510525612699E-6
train: 511, loss: 4.056942266647437E-6
train: 512, loss: 1.2467168118412162E-5
train: 513, loss: 0.0037317375493896862
train: 514, loss: 0.05919134048433402
train: 515, loss: 1.2396808450341414E-5
train: 516, loss: 0.07076185789356851
train: 517, loss: 2.296109681995234E-4
train: 518, loss: 0.0012417749063120753
train: 519, loss: 0.028863608063559882
train: 520, loss: 0.002017575305594215
train: 521, loss: 0.019285212552024023
train: 522, loss: 0.004875051627445159
train: 523, loss: 7.75533363910016E-6
train: 524, loss: 5.46780575612731E-6
train: 525, loss: 1.2569733574223225E-5
train: 526, loss: 0.017819653706763635
train: 527, loss: 3.581199852817719E-5
train: 528, loss: 8.144899476085994E-6
train: 529, loss: 1.0561515790455873E-4
train: 530, loss: 3.0820746415886023E-4
train: 531, loss: 3.97200234163413E-6
train: 532, loss: 0.003968569440702479
train: 533, loss: 2.0845332843498967E-4
train: 534, loss: 1.608365907759352E-4
train: 535, loss: 2.39514531610161E-4
train: 536, loss: 5.48308915459571E-6
train: 537, loss: 0.013644235626501845
train: 538, loss: 0.005492232149625871
train: 539, loss: 1.0789575692520837E-5
train: 540, loss: 6.377768020997562E-6
train: 541, loss: 1.0997117648002598E-5
train: 542, loss: 8.524372314632803E-4
train: 543, loss: 1.611469374261238E-5
train: 544, loss: 4.553471025440283E-6
train: 545, loss: 4.444667961543708E-6
train: 546, loss: 3.684902939114118E-6
train: 547, loss: 1.94944747992431E-5
train: 548, loss: 2.9001750917798363E-5
train: 549, loss: 1.4326941993101177E-5
train: 550, loss: 0.004277355103566385
train: 551, loss: 1.5681724422009913E-4
train: 552, loss: 0.0614334478488962
train: 553, loss: 2.2373356284753908E-5
train: 554, loss: 3.595964460851431E-4
train: 555, loss: 2.365126472324756E-5
train: 556, loss: 2.1197030913013825E-4
train: 557, loss: 5.122846673599484E-5
train: 558, loss: 0.004164070940003978
train: 559, loss: 1.1213539949967756E-5
train: 560, loss: 8.038434872823475E-5
train: 561, loss: 1.3988757299164332E-5
train: 562, loss: 9.798737655496717E-6
train: 563, loss: 0.0011739585309034704
train: 564, loss: 1.648938759351014E-5
train: 565, loss: 6.56428691714067E-5
train: 566, loss: 0.001241489568546001
train: 567, loss: 5.621121624457801E-4
train: 568, loss: 0.1515417955490442
train: 569, loss: 0.2149263766850191
train: 570, loss: 0.09709240078913746
train: 571, loss: 7.192491216086927E-4
train: 572, loss: 0.010542597511332174
train: 573, loss: 9.996476295502031E-5
train: 574, loss: 0.002866232376547215
train: 575, loss: 4.9914151847580134E-5
train: 576, loss: 9.738645345390649E-6
train: 577, loss: 0.017489207583861806
train: 578, loss: 0.0013663500585027457
train: 579, loss: 0.017142216046128415
train: 580, loss: 1.013746323857631E-5
train: 581, loss: 0.002537953269202704
train: 582, loss: 0.04763366900921878
train: 583, loss: 3.4092316756783037E-6
train: 584, loss: 1.1638700403982392E-4
train: 585, loss: 5.328505560638358E-6
train: 586, loss: 0.2514957796191862
train: 587, loss: 3.83991199561947E-6
train: 588, loss: 9.836688742094233E-6
train: 589, loss: 1.5216986122619657E-4
train: 590, loss: 9.120193992077726E-6
train: 591, loss: 8.944900184544294E-6
train: 592, loss: 2.4518752206855066E-5
train: 593, loss: 5.340570564479813E-4
train: 594, loss: 0.17255629558313976
train: 595, loss: 3.9838586607895926E-5
train: 596, loss: 1.3841853202520738E-5
train: 597, loss: 3.5758288525060604E-6
train: 598, loss: 0.012870960453550188
train: 599, loss: 3.3267089707830925E-6
train: 600, loss: 5.81826312828638E-4
train: 601, loss: 8.916688073164391E-6
train: 602, loss: 2.534269073471562E-5
train: 603, loss: 8.854776022211074E-6
train: 604, loss: 8.563513890443393E-6
train: 605, loss: 4.30430808017638E-6
train: 606, loss: 1.555945326614816E-5
train: 607, loss: 0.021683302838059967
train: 608, loss: 8.651125805694503E-6
train: 609, loss: 9.580252298608107E-6
train: 610, loss: 0.04532346099180234
train: 611, loss: 7.068234551677196E-4
train: 612, loss: 0.010761627006814857
train: 613, loss: 8.683510572296554E-6
train: 614, loss: 2.1029615077627345E-4
train: 615, loss: 6.3377508074683E-4
train: 616, loss: 2.571420924175382E-5
train: 617, loss: 4.681758520682818E-6
train: 618, loss: 0.01300103533631631
train: 619, loss: 1.3826941908061582E-5
train: 620, loss: 1.2454765713478195E-5
train: 621, loss: 0.3147623354942938
train: 622, loss: 8.550721821586234E-6
train: 623, loss: 8.235323604991282E-6
train: 624, loss: 7.650658691916032E-6
train: 625, loss: 6.8137215431220155E-6
train: 626, loss: 0.0020081564594932695
train: 627, loss: 2.89544508382982E-4
train: 628, loss: 7.4843330600601365E-6
train: 629, loss: 0.037049311052978005
train: 630, loss: 7.568092123624691E-5
train: 631, loss: 1.1560950982335881E-5
train: 632, loss: 3.058096010520318E-6
train: 633, loss: 1.1313006557508995E-5
train: 634, loss: 0.0010180417933917153
train: 635, loss: 0.0011516593588529335
train: 636, loss: 0.22014720840612093
train: 637, loss: 1.4287540999381095E-4
train: 638, loss: 0.0216554206451051
train: 639, loss: 4.689456729344293E-6
train: 640, loss: 1.0683443027061906E-5
train: 641, loss: 4.5929131191951183E-4
train: 642, loss: 8.019920860314418E-6
train: 643, loss: 3.885618227725523E-5
train: 644, loss: 0.07230371221783657
train: 645, loss: 0.005322712611129509
train: 646, loss: 3.298712985305673E-4
train: 647, loss: 0.0240491898834614
train: 648, loss: 0.017812279322681895
train: 649, loss: 0.021101048696185114
train: 650, loss: 0.04830115310732226
train: 651, loss: 1.4195727627984804E-5
train: 652, loss: 0.024609999393644843
train: 653, loss: 5.0552514574015335E-6
train: 654, loss: 1.4517348142839715E-5
train: 655, loss: 1.1768483462807335E-5
train: 656, loss: 4.025119963648697E-4
train: 657, loss: 1.908973684577507E-5
train: 658, loss: 0.29899487157449356
train: 659, loss: 1.9446531745063312E-5
train: 660, loss: 3.4614627430246917E-6
train: 661, loss: 3.2902887286905714E-6
train: 662, loss: 5.4478999575522914E-5
train: 663, loss: 6.730435663814916E-6
train: 664, loss: 4.285226400918919E-4
train: 665, loss: 2.875740638340394E-5
train: 666, loss: 0.00190890777932634
train: 667, loss: 0.024045824245759902
train: 668, loss: 6.779250156553085E-6
train: 669, loss: 0.0035639687466861476
train: 670, loss: 0.07053938441369563
train: 671, loss: 2.639480127922266E-6
train: 672, loss: 1.2659984227571251E-5
train: 673, loss: 0.0015242380643687968
train: 674, loss: 0.024142187719268488
train: 675, loss: 4.2904970671231254E-5
train: 676, loss: 1.2390317036809255E-5
train: 677, loss: 0.0581724531490533
train: 678, loss: 0.0011598601220910141
train: 679, loss: 0.0015095363048764805
train: 680, loss: 0.003910020694271274
train: 681, loss: 0.0012860109021326257
train: 682, loss: 0.017406831365113975
train: 683, loss: 7.80513333676495E-6
train: 684, loss: 5.355077739511521E-6
train: 685, loss: 2.131477765686865E-6
train: 686, loss: 3.384249137809556E-6
train: 687, loss: 0.021244354736211987
train: 688, loss: 5.222973835044191E-6
train: 689, loss: 4.712421825870812E-6
train: 690, loss: 0.027177238012460185
train: 691, loss: 6.054432025490464E-4
train: 692, loss: 8.19717443313342E-5
train: 693, loss: 7.288891175215981E-6
train: 694, loss: 1.5388638139085452E-4
train: 695, loss: 0.016286357325848815
train: 696, loss: 1.4639333106985554E-5
train: 697, loss: 2.641367603677987E-5
train: 698, loss: 1.0276114127058541E-4
train: 699, loss: 4.0329321085433276E-6
train: 700, loss: 0.012736598565360118
train: 701, loss: 0.004011769594299885
train: 702, loss: 0.19992954793162476
train: 703, loss: 8.084697109625994E-5
train: 704, loss: 1.9597645517448493E-4
train: 705, loss: 1.3961923327418708E-4
train: 706, loss: 8.777252303704306E-5
train: 707, loss: 6.333235185358983E-6
train: 708, loss: 0.00629235822762198
train: 709, loss: 1.568436922422132E-5
train: 710, loss: 5.391560723663239E-6
train: 711, loss: 7.60805251431292E-6
train: 712, loss: 3.781781395040206E-5
train: 713, loss: 1.2245677008639264E-5
train: 714, loss: 0.001095092699300748
train: 715, loss: 0.04983900713337735
train: 716, loss: 0.0019993049383839417
train: 717, loss: 0.010963540660301014
train: 718, loss: 1.431491887914667E-5
train: 719, loss: 0.003555189922030348
train: 720, loss: 1.6943668306318694E-6
train: 721, loss: 4.8172374249171694E-6
train: 722, loss: 0.0035072051773825785
train: 723, loss: 5.789574084894257E-6
train: 724, loss: 0.03053040498322821
train: 725, loss: 8.44446259906661E-6
train: 726, loss: 0.004052032062599702
train: 727, loss: 1.532743272681183E-5
train: 728, loss: 2.1900858612717367E-5
train: 729, loss: 8.959530676738517E-4
train: 730, loss: 1.3213729787230259E-5
train: 731, loss: 0.21706813348990964
train: 732, loss: 0.008623205108168825
train: 733, loss: 6.342494149587926E-6
train: 734, loss: 0.3454198463546154
train: 735, loss: 8.595208636726046E-6
train: 736, loss: 0.004776188486634046
train: 737, loss: 4.049177539048488E-5
train: 738, loss: 4.747818104954323E-6
train: 739, loss: 1.56217409231667E-5
train: 740, loss: 0.0023362570888863688
train: 741, loss: 4.6254516295277055E-4
train: 742, loss: 0.0022173906389396374
train: 743, loss: 9.965185863235911E-6
train: 744, loss: 3.580806552094821E-4
train: 745, loss: 7.025440680588811E-4
train: 746, loss: 2.094415147501086E-6
train: 747, loss: 0.017333851220927576
train: 748, loss: 4.085201185753071E-4
train: 749, loss: 5.905010400931521E-5
train: 750, loss: 1.5596793382750354E-5
train: 751, loss: 0.0011930904689892636
train: 752, loss: 4.2802653470927875E-6
train: 753, loss: 3.060014357787954E-5
train: 754, loss: 7.915601083880763E-6
train: 755, loss: 4.030444318218128E-6
train: 756, loss: 1.6249657159757986E-6
train: 757, loss: 6.738485152563362E-6
train: 758, loss: 6.519469420172186E-4
train: 759, loss: 4.563198612446591E-6
train: 760, loss: 0.00477160407396847
train: 761, loss: 0.0019607028896390668
train: 762, loss: 7.112664021466808E-4
train: 763, loss: 1.4111622798769946E-6
train: 764, loss: 1.7029958096907667E-5
train: 765, loss: 1.5545694954685707E-6
train: 766, loss: 1.7649392893957928E-5
train: 767, loss: 1.677454278913003E-6
train: 768, loss: 0.0017803133156621268
train: 769, loss: 2.022792011990269E-5
train: 770, loss: 2.2272385980672147E-5
train: 771, loss: 2.5809276783744124E-5
train: 772, loss: 1.8003807870247342E-6
train: 773, loss: 3.1793858712774816E-5
train: 774, loss: 0.004480357128496546
train: 775, loss: 4.216332792587863E-6
train: 776, loss: 3.290590090611416E-5
train: 777, loss: 2.382581970132679E-4
train: 778, loss: 1.5482196014679278E-6
train: 779, loss: 8.99732948197436E-4
train: 780, loss: 1.1822154329287608E-5
train: 781, loss: 0.01417778703922525
train: 782, loss: 3.826266956054298E-5
train: 783, loss: 1.909926601446462E-5
train: 784, loss: 8.698100885704611E-4
train: 785, loss: 3.2293710010067345E-5
train: 786, loss: 3.335954653556053E-5
train: 787, loss: 3.319421994756807E-5
train: 788, loss: 1.5571289618085228E-5
train: 789, loss: 3.9577878852857775E-6
train: 790, loss: 4.231652298400837E-6
train: 791, loss: 2.0570060507263513E-4
train: 792, loss: 0.002488867710569998
train: 793, loss: 1.3904968999868418E-6
train: 794, loss: 0.2939743691866002
train: 795, loss: 6.764449844640526E-5
train: 796, loss: 2.2993895974331587E-5
train: 797, loss: 1.4947813831590888E-5
train: 798, loss: 3.817384448101696E-5
train: 799, loss: 1.1957466733880225E-4
train: 800, loss: 0.04393922833514549
train: 801, loss: 0.29865205429477154
train: 802, loss: 1.7219159684021148E-4
train: 803, loss: 0.02887126589664373
train: 804, loss: 1.4476029466511132E-6
train: 805, loss: 0.01581585774544062
train: 806, loss: 3.3490376609460916E-6
train: 807, loss: 2.483471458475871E-6
train: 808, loss: 4.3529605501200044E-4
train: 809, loss: 3.159536643075232E-4
train: 810, loss: 6.139177755691533E-5
train: 811, loss: 1.3886087088208509E-5
train: 812, loss: 0.41546273545533774
train: 813, loss: 1.977824681787411E-5
train: 814, loss: 4.1279739074055564E-6
train: 815, loss: 5.781863815203223E-5
train: 816, loss: 0.043606758111209884
train: 817, loss: 0.030707373057551904
train: 818, loss: 3.7241736037315077E-6
train: 819, loss: 0.04638621047937536
train: 820, loss: 2.229530827185687E-5
train: 821, loss: 0.017379108604194077
train: 822, loss: 5.441106343474744E-5
train: 823, loss: 1.1482540762752958E-6
train: 824, loss: 2.925969869913712E-4
train: 825, loss: 4.2739607321681305E-6
train: 826, loss: 3.2016599115372615E-5
train: 827, loss: 0.04889031883949419
train: 828, loss: 0.2160169730919309
train: 829, loss: 4.610099549665223E-6
train: 830, loss: 3.432231536237162E-5
train: 831, loss: 4.811389935292658E-6
train: 832, loss: 0.0027498677539917666
train: 833, loss: 0.30917511732708897
train: 834, loss: 0.011799138771538408
train: 835, loss: 0.001786788768194577
train: 836, loss: 0.0024512279165501004
train: 837, loss: 1.1986776971213186E-6
train: 838, loss: 0.009104542482011965
train: 839, loss: 2.9352850884293934E-6
train: 840, loss: 4.2629338696483507E-4
train: 841, loss: 1.1458021539463116E-5
train: 842, loss: 0.05219582876572595
train: 843, loss: 0.228717421366016
train: 844, loss: 3.87522632228914E-6
train: 845, loss: 8.302003834301106E-5
train: 846, loss: 4.306401182614902E-6
train: 847, loss: 2.7734761236935943E-5
train: 848, loss: 6.570798890631267E-5
train: 849, loss: 0.006361041163400661
train: 850, loss: 2.703331953398308E-6
train: 851, loss: 5.1020098644323995E-5
train: 852, loss: 0.006290659482346531
train: 853, loss: 1.2997597607405732E-5
train: 854, loss: 0.002787929690666284
train: 855, loss: 3.427565764505019E-6
train: 856, loss: 0.00258520392112879
train: 857, loss: 6.459736063524189E-4
train: 858, loss: 4.4149047857902965E-4
train: 859, loss: 2.418601568410103E-6
train: 860, loss: 6.102744982887093E-6
train: 861, loss: 0.012322234230146526
train: 862, loss: 0.0010989492986392209
train: 863, loss: 1.3996708201970611E-4
train: 864, loss: 1.0669897356523427E-6
train: 865, loss: 0.03246786020989361
train: 866, loss: 4.3114446493815046E-5
train: 867, loss: 0.021287004815162328
train: 868, loss: 2.352025277169594E-6
train: 869, loss: 5.814495264263959E-6
train: 870, loss: 2.3938700338175165E-5
train: 871, loss: 0.0017993731268311772
train: 872, loss: 6.514909404660199E-4
train: 873, loss: 0.034832992180728446
train: 874, loss: 0.03348180143080276
train: 875, loss: 0.18965772360457922
train: 876, loss: 0.033262395098042206
train: 877, loss: 1.1867150560695062E-6
train: 878, loss: 8.71822778445123E-6
train: 879, loss: 2.7202476711539084E-5
train: 880, loss: 0.07534411011414022
train: 881, loss: 0.0018851951591009083
train: 882, loss: 8.887676246180392E-7
train: 883, loss: 1.0890556762952967E-4
train: 884, loss: 1.2165725020347342E-6
train: 885, loss: 3.0892348233298606E-4
train: 886, loss: 0.01913408249582922
train: 887, loss: 2.9709521629699814E-6
train: 888, loss: 0.017586605086250887
train: 889, loss: 0.005653410788322335
train: 890, loss: 3.60782392347381E-5
train: 891, loss: 0.010852214524091226
train: 892, loss: 1.2442062712765222E-4
train: 893, loss: 5.827054560243551E-5
train: 894, loss: 4.8393396361821305E-6
train: 895, loss: 2.349875108646283E-6
train: 896, loss: 1.2652149397415241E-5
train: 897, loss: 5.171408870120409E-5
train: 898, loss: 2.2508270342588394E-6
train: 899, loss: 0.02731530053510567
train: 900, loss: 0.0334479238104062
train: 901, loss: 1.2154559843691012E-4
train: 902, loss: 9.379616903132073E-7
train: 903, loss: 8.501215361945634E-5
train: 904, loss: 1.4034039000512004E-5
train: 905, loss: 5.3861374248794435E-5
train: 906, loss: 1.6789160530435653E-5
train: 907, loss: 4.961207248568279E-5
train: 908, loss: 1.5298473611233922E-5
train: 909, loss: 0.004065648599934303
train: 910, loss: 4.82399854286719E-6
train: 911, loss: 2.8310401871512196E-4
train: 912, loss: 5.883244247004806E-6
train: 913, loss: 0.2862807217952627
train: 914, loss: 8.673580434635463E-7
train: 915, loss: 0.0063566506545408566
train: 916, loss: 1.0455176652203436E-6
train: 917, loss: 0.022273823022696274
train: 918, loss: 3.3797291810842416E-4
train: 919, loss: 6.098512563959592E-5
train: 920, loss: 0.039145335500282076
train: 921, loss: 3.893084655871877E-6
train: 922, loss: 0.004572661362022217
train: 923, loss: 2.9291827197763086E-6
train: 924, loss: 0.3151697924882156
train: 925, loss: 2.5827161843658125E-6
train: 926, loss: 0.22326468716129635
train: 927, loss: 0.2078342486074929
train: 928, loss: 2.035915041196245E-6
train: 929, loss: 3.6285557721056955E-5
train: 930, loss: 4.0981712383606694E-5
train: 931, loss: 2.850089660764168E-6
train: 932, loss: 0.42255298395010077
train: 933, loss: 9.30343407551402E-6
train: 934, loss: 0.012767412230230155
train: 935, loss: 0.2549142687481558
train: 936, loss: 0.41264614608354655
train: 937, loss: 2.3376156931230772E-4
train: 938, loss: 0.23043528415011275
train: 939, loss: 0.004446695632011458
train: 940, loss: 0.011073202470631266
train: 941, loss: 7.641218119276823E-6
train: 942, loss: 6.0130282384494E-5
train: 943, loss: 0.04482163411985773
train: 944, loss: 0.022052852429686697
train: 945, loss: 1.3850656127118453E-5
train: 946, loss: 3.20777875565059E-4
train: 947, loss: 0.0033332717034295477
train: 948, loss: 0.002173717220568061
train: 949, loss: 8.036200161086563E-7
train: 950, loss: 0.004603452244232919
train: 951, loss: 2.2217798905206537E-4
train: 952, loss: 1.9116434299657804E-4
train: 953, loss: 1.710606835704842E-4
train: 954, loss: 0.009112874164655598
train: 955, loss: 1.7657494926488898E-4
train: 956, loss: 0.0024182752380672817
train: 957, loss: 0.4181067566603796
train: 958, loss: 4.810632283800108E-5
train: 959, loss: 0.0017494265960268248
train: 960, loss: 0.04487379469003096
train: 961, loss: 0.001489200066520008
train: 962, loss: 3.903848145853745E-5
train: 963, loss: 3.1367517470588025E-6
train: 964, loss: 8.452079314084884E-7
train: 965, loss: 0.18882986796185225
train: 966, loss: 0.003842922591928921
train: 967, loss: 1.0793299919108876E-5
train: 968, loss: 5.614672022990735E-6
train: 969, loss: 2.061211425794553E-4
train: 970, loss: 4.748600037675821E-5
train: 971, loss: 4.535998306550775E-5
train: 972, loss: 9.697330586833287E-7
train: 973, loss: 1.920292528754464E-6
train: 974, loss: 1.9463013136302813E-6
train: 975, loss: 0.006653793355718341
train: 976, loss: 0.05539987724912959
train: 977, loss: 1.4591849087409634E-4
train: 978, loss: 3.349015576284679E-5
train: 979, loss: 0.013825211851133638
train: 980, loss: 3.531825418267756E-5
train: 981, loss: 3.1152892603351955E-4
train: 982, loss: 5.038881355964533E-5
train: 983, loss: 4.691282598539472E-5
train: 984, loss: 1.0881109239414131E-5
train: 985, loss: 0.002667583881369104
train: 986, loss: 0.09447646514488549
train: 987, loss: 7.734047453805123E-5
train: 988, loss: 0.003857773702248746
train: 989, loss: 1.561534674307917E-5
train: 990, loss: 0.0031472692289737955
train: 991, loss: 0.013824450968698778
train: 992, loss: 1.679155087994386E-6
train: 993, loss: 0.2897528996844149
train: 994, loss: 0.04368276296960146
train: 995, loss: 0.06327564965109304
train: 996, loss: 1.5349299453075037E-6
train: 997, loss: 5.340286667237021E-7
train: 998, loss: 4.424803048998893E-5
train: 999, loss: 8.89661920842498E-6
train: 1000, loss: 2.6859424794709728E-6
trainTime: 6238
==========Finish Train==========
==========Start Predict==========
label: -1.0, output: 0.9953817613881083, predict: 1.0
label: -1.0, output: 0.03334307849591209, predict: 0.0
label: -1.0, output: 0.010501652416871047, predict: 0.0
label: -1.0, output: 0.09415535326584977, predict: 0.0
label: -1.0, output: 0.9971803780679719, predict: 1.0
label: -1.0, output: 0.001965506851120934, predict: 0.0
label: -1.0, output: 0.9974538107332412, predict: 1.0
label: -1.0, output: 0.00857292570463325, predict: 0.0
label: -1.0, output: 0.9948854574915478, predict: 1.0
label: -1.0, output: 0.9941339294767709, predict: 1.0
label: -1.0, output: 0.9098369505615904, predict: 1.0
label: -1.0, output: 0.9371704963080724, predict: 1.0
label: -1.0, output: 0.6650333108297404, predict: 1.0
label: -1.0, output: 0.0017144693394965237, predict: 0.0
label: -1.0, output: 0.9977147972456674, predict: 1.0
label: -1.0, output: 0.008647229256250416, predict: 0.0
label: -1.0, output: 0.9986462351092911, predict: 1.0
label: -1.0, output: 0.002488872208124489, predict: 0.0
label: -1.0, output: 0.01172816586811155, predict: 0.0
label: -1.0, output: 0.001788498113153745, predict: 0.0
label: -1.0, output: 0.0020850634118236575, predict: 0.0
label: -1.0, output: 0.9976867389061531, predict: 1.0
label: -1.0, output: 0.8894689305569483, predict: 1.0
label: -1.0, output: 0.006548224436578646, predict: 0.0
label: -1.0, output: 0.0020865084565659187, predict: 0.0
label: -1.0, output: 0.9747839478852237, predict: 1.0
label: -1.0, output: 0.30384448490708577, predict: 0.0
label: -1.0, output: 0.5968869871848143, predict: 1.0
label: -1.0, output: 0.0754010553834518, predict: 0.0
label: -1.0, output: 0.12872601684432416, predict: 0.0
label: -1.0, output: 0.009291368613254015, predict: 0.0
label: -1.0, output: 0.9964327759154733, predict: 1.0
label: -1.0, output: 0.9969910567564455, predict: 1.0
label: -1.0, output: 0.00374392020665445, predict: 0.0
label: -1.0, output: 0.004454125444413852, predict: 0.0
label: -1.0, output: 0.0017782192491205741, predict: 0.0
label: -1.0, output: 0.958305521735025, predict: 1.0
label: -1.0, output: 0.01127179530495784, predict: 0.0
label: -1.0, output: 0.30983408524070494, predict: 0.0
label: -1.0, output: 0.9688245110999982, predict: 1.0
label: -1.0, output: 0.8197813549006945, predict: 1.0
label: -1.0, output: 0.06455083470282097, predict: 0.0
label: -1.0, output: 0.8107506421031907, predict: 1.0
label: -1.0, output: 0.9169436190067076, predict: 1.0
label: -1.0, output: 0.9933681736777735, predict: 1.0
label: -1.0, output: 0.007527812929506181, predict: 0.0
label: -1.0, output: 0.003732550044758136, predict: 0.0
label: -1.0, output: 0.9969236732570886, predict: 1.0
label: -1.0, output: 0.01739129039191615, predict: 0.0
label: -1.0, output: 0.009948340754744468, predict: 0.0
label: -1.0, output: 0.0017105110468437817, predict: 0.0
label: -1.0, output: 0.05360740714058095, predict: 0.0
label: -1.0, output: 0.9984260922310266, predict: 1.0
label: -1.0, output: 0.06611774332319821, predict: 0.0
label: -1.0, output: 0.19783782217796764, predict: 0.0
label: -1.0, output: 0.007592654268623002, predict: 0.0
label: -1.0, output: 0.36300518730989867, predict: 0.0
label: -1.0, output: 0.9986688128266035, predict: 1.0
label: -1.0, output: 0.9969921954597953, predict: 1.0
label: -1.0, output: 0.0742706487892343, predict: 0.0
label: -1.0, output: 0.9943428081263185, predict: 1.0
label: -1.0, output: 0.9366587978826253, predict: 1.0
label: -1.0, output: 0.002924115614467679, predict: 0.0
label: -1.0, output: 0.9974021001602087, predict: 1.0
label: -1.0, output: 0.0035150594090360594, predict: 0.0
label: -1.0, output: 0.05975244252984908, predict: 0.0
label: -1.0, output: 0.9985273919789661, predict: 1.0
label: -1.0, output: 0.005152990676575212, predict: 0.0
label: -1.0, output: 0.6978763001556558, predict: 1.0
label: -1.0, output: 0.0018889452970097082, predict: 0.0
label: -1.0, output: 0.002411217248964479, predict: 0.0
label: -1.0, output: 0.9987066860017153, predict: 1.0
label: -1.0, output: 0.30455903918539357, predict: 0.0
label: -1.0, output: 0.9800075949547984, predict: 1.0
label: -1.0, output: 0.002609404264371579, predict: 0.0
label: -1.0, output: 0.02466830355601436, predict: 0.0
label: -1.0, output: 0.989250829799673, predict: 1.0
label: -1.0, output: 0.22058948806070333, predict: 0.0
label: -1.0, output: 0.0018488901152346343, predict: 0.0
label: -1.0, output: 0.07377398129533023, predict: 0.0
label: -1.0, output: 0.8248656254214817, predict: 1.0
label: -1.0, output: 0.9986441506064287, predict: 1.0
label: -1.0, output: 0.9938017501611026, predict: 1.0
label: -1.0, output: 0.9986327897078712, predict: 1.0
label: -1.0, output: 0.9672343840864182, predict: 1.0
label: -1.0, output: 0.004018979118504705, predict: 0.0
label: -1.0, output: 0.04265725546815725, predict: 0.0
label: -1.0, output: 0.007050539616212427, predict: 0.0
label: -1.0, output: 0.9912212018739449, predict: 1.0
label: -1.0, output: 0.9379662780514408, predict: 1.0
label: -1.0, output: 0.07485175827776791, predict: 0.0
label: -1.0, output: 0.9516289518483532, predict: 1.0
label: -1.0, output: 0.9974303033570329, predict: 1.0
label: -1.0, output: 0.8513000523620273, predict: 1.0
label: -1.0, output: 0.019650142046557212, predict: 0.0
label: -1.0, output: 0.0040087606596677676, predict: 0.0
label: -1.0, output: 0.006744710607012369, predict: 0.0
label: -1.0, output: 0.992001328101419, predict: 1.0
label: -1.0, output: 0.05460753980452837, predict: 0.0
label: -1.0, output: 0.6124218429182989, predict: 1.0
label: -1.0, output: 0.00451153809599214, predict: 0.0
label: -1.0, output: 0.9924235208541726, predict: 1.0
label: -1.0, output: 0.8710507858697069, predict: 1.0
label: -1.0, output: 0.0018413115204048125, predict: 0.0
label: -1.0, output: 0.10829220840267476, predict: 0.0
label: -1.0, output: 0.008387181645282259, predict: 0.0
label: -1.0, output: 0.9988177620105712, predict: 1.0
label: -1.0, output: 0.9971882184131446, predict: 1.0
label: -1.0, output: 0.07429203033994419, predict: 0.0
label: -1.0, output: 0.04129084909176665, predict: 0.0
label: -1.0, output: 0.010179996089361439, predict: 0.0
label: -1.0, output: 0.17626131732141798, predict: 0.0
label: -1.0, output: 0.005203186685443461, predict: 0.0
label: -1.0, output: 0.9527654599044925, predict: 1.0
label: -1.0, output: 0.004539141892274034, predict: 0.0
label: -1.0, output: 0.9972591886574216, predict: 1.0
label: -1.0, output: 0.8090130074393607, predict: 1.0
label: -1.0, output: 0.05796442175414654, predict: 0.0
label: -1.0, output: 0.0021276053716150255, predict: 0.0
label: -1.0, output: 0.07173095965747228, predict: 0.0
label: -1.0, output: 0.9964147705298416, predict: 1.0
label: -1.0, output: 0.7694847134188444, predict: 1.0
label: -1.0, output: 0.9779806994041643, predict: 1.0
label: -1.0, output: 0.19756908493191086, predict: 0.0
label: -1.0, output: 0.0021408651035984827, predict: 0.0
label: -1.0, output: 0.992200726346367, predict: 1.0
label: -1.0, output: 0.004080138393418557, predict: 0.0
label: -1.0, output: 0.004186317490298057, predict: 0.0
label: -1.0, output: 0.952867567281055, predict: 1.0
label: -1.0, output: 0.005722283490627681, predict: 0.0
label: -1.0, output: 0.997214332226671, predict: 1.0
label: -1.0, output: 0.001748285822862463, predict: 0.0
label: -1.0, output: 0.8604030859064195, predict: 1.0
label: -1.0, output: 0.40638876962071535, predict: 0.0
label: -1.0, output: 0.0019404733668472308, predict: 0.0
label: -1.0, output: 0.002509930604981961, predict: 0.0
label: -1.0, output: 0.9710470324464979, predict: 1.0
label: -1.0, output: 0.3640391058173266, predict: 0.0
label: -1.0, output: 0.9877593020351083, predict: 1.0
label: -1.0, output: 0.9989418344216708, predict: 1.0
label: -1.0, output: 0.9911239191461992, predict: 1.0
label: -1.0, output: 0.9988336924856818, predict: 1.0
label: -1.0, output: 0.001965446354951171, predict: 0.0
label: -1.0, output: 0.9733433408504695, predict: 1.0
label: -1.0, output: 0.9989135893476787, predict: 1.0
label: -1.0, output: 0.9797690231904819, predict: 1.0
label: -1.0, output: 0.9894011541145419, predict: 1.0
label: -1.0, output: 0.9964827965074554, predict: 1.0
label: -1.0, output: 0.9970881449446837, predict: 1.0
label: -1.0, output: 0.032275245848620786, predict: 0.0
label: -1.0, output: 0.998908337173092, predict: 1.0
label: -1.0, output: 0.9989405613760242, predict: 1.0
label: -1.0, output: 0.9865449046780069, predict: 1.0
label: -1.0, output: 0.9987116040492232, predict: 1.0
label: -1.0, output: 0.08644651832283709, predict: 0.0
label: -1.0, output: 0.9937980409989353, predict: 1.0
label: -1.0, output: 0.816498870908119, predict: 1.0
label: -1.0, output: 0.00569013697320557, predict: 0.0
label: -1.0, output: 0.1262322857096785, predict: 0.0
label: -1.0, output: 0.9898916480997789, predict: 1.0
label: -1.0, output: 0.9989705434991888, predict: 1.0
label: -1.0, output: 0.9584537991851452, predict: 1.0
label: -1.0, output: 0.9989289370970189, predict: 1.0
label: -1.0, output: 0.9986492608550628, predict: 1.0
label: -1.0, output: 0.9826779165777537, predict: 1.0
label: -1.0, output: 0.007076155298943515, predict: 0.0
label: -1.0, output: 0.9921837778063591, predict: 1.0
label: -1.0, output: 0.988758835086037, predict: 1.0
label: -1.0, output: 0.9904152469874534, predict: 1.0
label: -1.0, output: 0.0049352694309339265, predict: 0.0
label: -1.0, output: 0.9910609718864953, predict: 1.0
label: -1.0, output: 0.724721954347296, predict: 1.0
label: -1.0, output: 0.004295326158260849, predict: 0.0
label: -1.0, output: 0.7037557177029601, predict: 1.0
label: -1.0, output: 0.9965102516025969, predict: 1.0
label: -1.0, output: 0.9989796129048633, predict: 1.0
label: -1.0, output: 0.9987133932216122, predict: 1.0
label: -1.0, output: 0.003321987865562776, predict: 0.0
label: -1.0, output: 0.0018145020945812922, predict: 0.0
label: -1.0, output: 0.9985833445670447, predict: 1.0
label: -1.0, output: 0.009445146983661156, predict: 0.0
label: -1.0, output: 0.0019523995362369145, predict: 0.0
label: -1.0, output: 0.006685937468372269, predict: 0.0
label: -1.0, output: 0.9922068641109565, predict: 1.0
label: -1.0, output: 0.002221021541292082, predict: 0.0
label: -1.0, output: 0.022252746142316787, predict: 0.0
label: -1.0, output: 0.011112417825274043, predict: 0.0
label: -1.0, output: 0.9948747208827353, predict: 1.0
label: -1.0, output: 0.9923302199684575, predict: 1.0
label: -1.0, output: 0.006357360781566846, predict: 0.0
label: -1.0, output: 0.007093518792590144, predict: 0.0
label: -1.0, output: 0.9919647801818618, predict: 1.0
label: -1.0, output: 0.9870551593797507, predict: 1.0
label: -1.0, output: 0.3000672428940482, predict: 0.0
label: -1.0, output: 0.042254684695931896, predict: 0.0
label: -1.0, output: 0.015089054203104092, predict: 0.0
label: -1.0, output: 0.02286023301433695, predict: 0.0
label: -1.0, output: 0.003286491274171061, predict: 0.0
label: -1.0, output: 0.006083207873168723, predict: 0.0
label: -1.0, output: 0.7019509512640818, predict: 1.0
label: -1.0, output: 0.0019501493781811966, predict: 0.0
label: -1.0, output: 0.8693287040370216, predict: 1.0
label: -1.0, output: 0.9766488719834409, predict: 1.0
label: -1.0, output: 0.023830395965580786, predict: 0.0
label: -1.0, output: 0.9987208292463283, predict: 1.0
label: -1.0, output: 0.9843106420392349, predict: 1.0
label: -1.0, output: 0.014983517487358294, predict: 0.0
label: -1.0, output: 0.9941422053358704, predict: 1.0
label: -1.0, output: 0.010523476082351936, predict: 0.0
label: -1.0, output: 0.9905609704391336, predict: 1.0
label: -1.0, output: 0.9976995317533901, predict: 1.0
label: -1.0, output: 0.11344972033581514, predict: 0.0
label: -1.0, output: 0.9838115399130101, predict: 1.0
label: -1.0, output: 0.003957819920811482, predict: 0.0
label: -1.0, output: 0.275899724550021, predict: 0.0
label: -1.0, output: 0.24811446734209344, predict: 0.0
label: -1.0, output: 0.9986968915719063, predict: 1.0
label: -1.0, output: 0.9907235746029954, predict: 1.0
label: -1.0, output: 0.00408753596914316, predict: 0.0
label: -1.0, output: 0.02938006238360276, predict: 0.0
label: -1.0, output: 0.08083193693942115, predict: 0.0
label: -1.0, output: 0.9988723643501356, predict: 1.0
label: -1.0, output: 0.007319310890475676, predict: 0.0
label: -1.0, output: 0.9899882967400094, predict: 1.0
label: -1.0, output: 0.9989664225281902, predict: 1.0
label: -1.0, output: 0.9189309561919572, predict: 1.0
label: -1.0, output: 0.07422137262873255, predict: 0.0
label: -1.0, output: 0.007946983774612492, predict: 0.0
label: -1.0, output: 0.20888013750555617, predict: 0.0
label: -1.0, output: 0.0049264723265263705, predict: 0.0
label: -1.0, output: 0.8177947865095004, predict: 1.0
label: -1.0, output: 0.001999163446510643, predict: 0.0
label: -1.0, output: 0.9698429032706395, predict: 1.0
label: -1.0, output: 0.9343812718991039, predict: 1.0
label: -1.0, output: 0.001940621703775635, predict: 0.0
label: -1.0, output: 0.008052691022162812, predict: 0.0
label: -1.0, output: 0.9986619605881757, predict: 1.0
label: -1.0, output: 0.9352251770260407, predict: 1.0
label: -1.0, output: 0.9725817908142699, predict: 1.0
label: -1.0, output: 0.9928596350227569, predict: 1.0
label: -1.0, output: 0.004473788478403358, predict: 0.0
label: -1.0, output: 0.997534188677499, predict: 1.0
label: -1.0, output: 0.8038827819951175, predict: 1.0
label: -1.0, output: 0.009163783189452265, predict: 0.0
label: -1.0, output: 0.06752612539879506, predict: 0.0
label: -1.0, output: 0.005120171144490149, predict: 0.0
label: -1.0, output: 0.001831907244806959, predict: 0.0
label: -1.0, output: 0.0019499118717293384, predict: 0.0
label: -1.0, output: 0.0018429579204641976, predict: 0.0
label: -1.0, output: 0.9956911119538845, predict: 1.0
label: -1.0, output: 0.003190143152039789, predict: 0.0
label: -1.0, output: 0.9984236120936668, predict: 1.0
label: -1.0, output: 0.01496544427483768, predict: 0.0
label: -1.0, output: 0.011030891461986093, predict: 0.0
label: -1.0, output: 0.9853285511037598, predict: 1.0
label: -1.0, output: 0.002147085733396, predict: 0.0
label: -1.0, output: 0.008521386805502238, predict: 0.0
label: -1.0, output: 0.062040171166751784, predict: 0.0
label: -1.0, output: 0.9989200632675713, predict: 1.0
label: -1.0, output: 0.9923130503553965, predict: 1.0
label: -1.0, output: 0.9926019267168843, predict: 1.0
label: -1.0, output: 0.0030676449285281316, predict: 0.0
label: -1.0, output: 0.007032881996701783, predict: 0.0
label: -1.0, output: 0.9910386965941407, predict: 1.0
label: -1.0, output: 0.008968144984892432, predict: 0.0
label: -1.0, output: 0.007740754498775288, predict: 0.0
label: -1.0, output: 0.6856949568562801, predict: 1.0
label: -1.0, output: 0.005336091040270552, predict: 0.0
label: -1.0, output: 0.06946505424655623, predict: 0.0
label: -1.0, output: 0.9963352481314863, predict: 1.0
label: -1.0, output: 0.3791251541722646, predict: 0.0
label: -1.0, output: 0.8926630142123171, predict: 1.0
label: -1.0, output: 0.9986885166944145, predict: 1.0
label: -1.0, output: 0.9859995280422097, predict: 1.0
label: -1.0, output: 0.7353342573848556, predict: 1.0
label: -1.0, output: 0.9689129117343399, predict: 1.0
label: -1.0, output: 0.009666495328231885, predict: 0.0
label: -1.0, output: 0.006511184629535135, predict: 0.0
label: -1.0, output: 0.006094877044002495, predict: 0.0
label: -1.0, output: 0.0021808768717731237, predict: 0.0
label: -1.0, output: 0.05041394024184547, predict: 0.0
label: -1.0, output: 0.008568925339266665, predict: 0.0
label: -1.0, output: 0.005564997103986084, predict: 0.0
label: -1.0, output: 0.008983388196373865, predict: 0.0
label: -1.0, output: 0.32889999738857295, predict: 0.0
label: -1.0, output: 0.9929048137462153, predict: 1.0
label: -1.0, output: 0.9972746699961437, predict: 1.0
label: -1.0, output: 0.9989205740740573, predict: 1.0
label: -1.0, output: 0.998929646982895, predict: 1.0
label: -1.0, output: 0.9985919572208932, predict: 1.0
label: -1.0, output: 0.053212463344780365, predict: 0.0
label: -1.0, output: 0.998945657445516, predict: 1.0
label: -1.0, output: 0.9947749635084021, predict: 1.0
label: -1.0, output: 0.9387627996238164, predict: 1.0
label: -1.0, output: 0.007708768102991733, predict: 0.0
label: -1.0, output: 0.19289381708091355, predict: 0.0
label: -1.0, output: 0.005383057152463499, predict: 0.0
label: -1.0, output: 0.9966630450327212, predict: 1.0
label: -1.0, output: 0.20090363162744126, predict: 0.0
label: -1.0, output: 0.9988307830933095, predict: 1.0
label: -1.0, output: 0.00762031543905557, predict: 0.0
label: -1.0, output: 0.71916257554702, predict: 1.0
label: -1.0, output: 0.99006275912395, predict: 1.0
label: -1.0, output: 0.007004776671691049, predict: 0.0
label: -1.0, output: 0.0019590285690197586, predict: 0.0
label: -1.0, output: 0.9019523944212281, predict: 1.0
label: -1.0, output: 0.20123295109845357, predict: 0.0
label: -1.0, output: 0.9864340812096999, predict: 1.0
label: -1.0, output: 0.0038569465439277618, predict: 0.0
label: -1.0, output: 0.9987412311899854, predict: 1.0
label: -1.0, output: 0.9938786784268744, predict: 1.0
label: -1.0, output: 0.9971206306127816, predict: 1.0
label: -1.0, output: 0.9961806577838341, predict: 1.0
label: -1.0, output: 0.015904128246633793, predict: 0.0
label: -1.0, output: 0.2802285935549888, predict: 0.0
label: -1.0, output: 0.19824940614278513, predict: 0.0
label: -1.0, output: 0.998853217812785, predict: 1.0
label: -1.0, output: 0.9922604212536719, predict: 1.0
label: -1.0, output: 0.9910128713348986, predict: 1.0
label: -1.0, output: 0.05104556619494116, predict: 0.0
label: -1.0, output: 0.02362470694798346, predict: 0.0
label: -1.0, output: 0.9808273298445996, predict: 1.0
label: -1.0, output: 0.9976652658481155, predict: 1.0
label: -1.0, output: 0.9915042231703194, predict: 1.0
label: -1.0, output: 0.9989292630237964, predict: 1.0
label: -1.0, output: 0.9988407843436289, predict: 1.0
label: -1.0, output: 0.9953739653622051, predict: 1.0
label: -1.0, output: 0.05297069870507359, predict: 0.0
label: -1.0, output: 0.992928287521813, predict: 1.0
label: -1.0, output: 0.23127494341213026, predict: 0.0
label: -1.0, output: 0.001885460642992476, predict: 0.0
label: -1.0, output: 0.9164210442903008, predict: 1.0
label: -1.0, output: 0.00816317407820013, predict: 0.0
label: -1.0, output: 0.9460920136282048, predict: 1.0
label: -1.0, output: 0.032492030466763355, predict: 0.0
label: -1.0, output: 0.9922285658341257, predict: 1.0
label: -1.0, output: 0.004487905067346465, predict: 0.0
label: -1.0, output: 0.9968694888518312, predict: 1.0
label: -1.0, output: 0.09218312086033022, predict: 0.0
label: -1.0, output: 0.9986024835417304, predict: 1.0
label: -1.0, output: 0.009587507134827944, predict: 0.0
label: -1.0, output: 0.8387349649110718, predict: 1.0
label: -1.0, output: 0.0022608174913852424, predict: 0.0
label: -1.0, output: 0.010051372534758164, predict: 0.0
label: -1.0, output: 0.9988757354644296, predict: 1.0
label: -1.0, output: 0.0056083225373260125, predict: 0.0
label: -1.0, output: 0.03435593175752319, predict: 0.0
label: -1.0, output: 0.8127034030598106, predict: 1.0
label: -1.0, output: 0.9903998952284595, predict: 1.0
label: -1.0, output: 0.2437922794792644, predict: 0.0
label: -1.0, output: 0.0074245528130479094, predict: 0.0
label: -1.0, output: 0.05349532888334534, predict: 0.0
label: -1.0, output: 0.006154558820870033, predict: 0.0
label: -1.0, output: 0.5930132261505225, predict: 1.0
label: -1.0, output: 0.7992262672928154, predict: 1.0
label: -1.0, output: 0.9134035021828038, predict: 1.0
label: -1.0, output: 0.0063145340821803165, predict: 0.0
label: -1.0, output: 0.005567937483970391, predict: 0.0
label: -1.0, output: 0.9929143474009143, predict: 1.0
label: -1.0, output: 0.012751342653381936, predict: 0.0
label: -1.0, output: 0.004957070450956297, predict: 0.0
label: -1.0, output: 0.9501108158037117, predict: 1.0
label: -1.0, output: 0.006946848386992206, predict: 0.0
label: -1.0, output: 0.9721815958738893, predict: 1.0
label: -1.0, output: 0.9988205152342855, predict: 1.0
label: -1.0, output: 0.97912213877617, predict: 1.0
label: -1.0, output: 0.9928337014022657, predict: 1.0
label: -1.0, output: 0.9961065694195367, predict: 1.0
label: -1.0, output: 0.9924550588109722, predict: 1.0
label: -1.0, output: 0.0017772807575298993, predict: 0.0
label: -1.0, output: 0.9712978359300458, predict: 1.0
label: -1.0, output: 0.7103660588536462, predict: 1.0
label: -1.0, output: 0.9964057639250995, predict: 1.0
label: -1.0, output: 0.041982610001808876, predict: 0.0
label: -1.0, output: 0.20054158405261202, predict: 0.0
label: -1.0, output: 0.005432393386714709, predict: 0.0
label: -1.0, output: 0.8606148263423621, predict: 1.0
label: -1.0, output: 0.995138725000843, predict: 1.0
label: -1.0, output: 0.9973234469628859, predict: 1.0
label: -1.0, output: 0.9986920248009759, predict: 1.0
label: -1.0, output: 0.9961826573218532, predict: 1.0
label: -1.0, output: 0.1396077060709129, predict: 0.0
label: -1.0, output: 0.7111689554601412, predict: 1.0
label: -1.0, output: 0.112494442326166, predict: 0.0
label: -1.0, output: 0.987168888751642, predict: 1.0
label: -1.0, output: 0.9866352221199309, predict: 1.0
label: -1.0, output: 0.9880944551474166, predict: 1.0
label: -1.0, output: 0.991675463034641, predict: 1.0
label: -1.0, output: 0.9986218388102173, predict: 1.0
label: -1.0, output: 0.9963118456300772, predict: 1.0
label: -1.0, output: 0.002503837312235435, predict: 0.0
label: -1.0, output: 0.0073368825026806535, predict: 0.0
label: -1.0, output: 0.9988266367832427, predict: 1.0
label: -1.0, output: 0.46730461993295497, predict: 0.0
label: -1.0, output: 0.9246164879705204, predict: 1.0
label: -1.0, output: 0.9988417589755245, predict: 1.0
label: -1.0, output: 0.9967533889328434, predict: 1.0
label: -1.0, output: 0.9960970393023203, predict: 1.0
label: -1.0, output: 0.08591909151839565, predict: 0.0
label: -1.0, output: 0.9986520737027441, predict: 1.0
label: -1.0, output: 0.21608150891366104, predict: 0.0
label: -1.0, output: 0.9524907331688865, predict: 1.0
label: -1.0, output: 0.018007338893082772, predict: 0.0
label: -1.0, output: 0.9978107448284985, predict: 1.0
label: -1.0, output: 0.9969025208347269, predict: 1.0
label: -1.0, output: 0.03594944081843859, predict: 0.0
label: -1.0, output: 0.9985799355818746, predict: 1.0
label: -1.0, output: 0.04498694252370709, predict: 0.0
label: -1.0, output: 0.9861546687583737, predict: 1.0
label: -1.0, output: 0.6541544630624107, predict: 1.0
label: -1.0, output: 0.9926022077510676, predict: 1.0
label: -1.0, output: 0.9530207716446796, predict: 1.0
label: -1.0, output: 0.006041749644396324, predict: 0.0
label: -1.0, output: 0.005842855740913881, predict: 0.0
label: -1.0, output: 0.9933668607195586, predict: 1.0
label: -1.0, output: 0.9950147810154274, predict: 1.0
label: -1.0, output: 0.07510707278387202, predict: 0.0
label: -1.0, output: 0.9406869219818405, predict: 1.0
label: -1.0, output: 0.9974551403182954, predict: 1.0
label: -1.0, output: 0.9898605956400963, predict: 1.0
label: -1.0, output: 0.7837802929328235, predict: 1.0
label: -1.0, output: 0.023991205783896644, predict: 0.0
label: -1.0, output: 0.9951662264879565, predict: 1.0
label: -1.0, output: 0.9986635615576355, predict: 1.0
label: -1.0, output: 0.9950750339186146, predict: 1.0
label: -1.0, output: 0.9988712703288799, predict: 1.0
label: -1.0, output: 0.16202862277572638, predict: 0.0
label: -1.0, output: 0.7752085390439158, predict: 1.0
label: -1.0, output: 0.07180789928409145, predict: 0.0
label: -1.0, output: 0.9971140325040008, predict: 1.0
label: -1.0, output: 0.010297868133329325, predict: 0.0
label: -1.0, output: 0.9986908378356334, predict: 1.0
label: -1.0, output: 0.9985804596530468, predict: 1.0
label: -1.0, output: 0.006453144526951541, predict: 0.0
label: -1.0, output: 0.998872577283016, predict: 1.0
label: -1.0, output: 0.9985775415450474, predict: 1.0
label: -1.0, output: 0.0074872454178899475, predict: 0.0
label: -1.0, output: 0.0057591598057139095, predict: 0.0
label: -1.0, output: 0.007141280234516194, predict: 0.0
label: -1.0, output: 0.9922017342149703, predict: 1.0
label: -1.0, output: 0.04086600582058286, predict: 0.0
label: -1.0, output: 0.0030449895553341213, predict: 0.0
label: -1.0, output: 0.007579387396799212, predict: 0.0
label: -1.0, output: 0.9947630481359763, predict: 1.0
label: -1.0, output: 0.00220825356134237, predict: 0.0
label: -1.0, output: 0.9926044053922022, predict: 1.0
label: -1.0, output: 0.07235186191283675, predict: 0.0
label: -1.0, output: 0.9820994058305882, predict: 1.0
label: -1.0, output: 0.9931468289930339, predict: 1.0
label: -1.0, output: 0.034564360760003375, predict: 0.0
label: -1.0, output: 0.803520865853595, predict: 1.0
label: -1.0, output: 0.0017732522380537825, predict: 0.0
label: -1.0, output: 0.8624102681666586, predict: 1.0
label: -1.0, output: 0.09491170871816108, predict: 0.0
label: -1.0, output: 0.9975264705775696, predict: 1.0
label: -1.0, output: 0.9987474511536955, predict: 1.0
label: -1.0, output: 0.9966134390066659, predict: 1.0
label: -1.0, output: 0.9530925990424364, predict: 1.0
label: -1.0, output: 0.007768959623420113, predict: 0.0
label: -1.0, output: 0.03578808485805011, predict: 0.0
label: -1.0, output: 0.9983494735993689, predict: 1.0
label: -1.0, output: 0.99042774442017, predict: 1.0
label: -1.0, output: 0.9894834092163504, predict: 1.0
label: -1.0, output: 0.0019356304978608127, predict: 0.0
label: -1.0, output: 0.004006158597490669, predict: 0.0
label: -1.0, output: 0.9969983923112162, predict: 1.0
label: -1.0, output: 0.1902501710141649, predict: 0.0
label: -1.0, output: 0.03140550030082103, predict: 0.0
label: -1.0, output: 0.9890894369807973, predict: 1.0
label: -1.0, output: 0.008788075269352111, predict: 0.0
label: -1.0, output: 0.00706993374510686, predict: 0.0
label: -1.0, output: 0.009442085946690302, predict: 0.0
label: -1.0, output: 0.05072604283711662, predict: 0.0
label: -1.0, output: 0.0066721101858768, predict: 0.0
label: -1.0, output: 0.9921143972239316, predict: 1.0
label: -1.0, output: 0.6443690178612428, predict: 1.0
label: -1.0, output: 0.00724593335748373, predict: 0.0
label: -1.0, output: 0.03267261849327441, predict: 0.0
label: -1.0, output: 0.009191962873654466, predict: 0.0
label: -1.0, output: 0.9988690308672601, predict: 1.0
label: -1.0, output: 0.009208797325919693, predict: 0.0
label: -1.0, output: 0.9956807717417752, predict: 1.0
label: -1.0, output: 0.10841045883039946, predict: 0.0
label: -1.0, output: 0.0026889221733188105, predict: 0.0
label: -1.0, output: 0.9897599088546369, predict: 1.0
label: -1.0, output: 0.9822099693295908, predict: 1.0
label: -1.0, output: 0.007660074930214602, predict: 0.0
label: -1.0, output: 0.9239430907810999, predict: 1.0
label: -1.0, output: 0.04851133654301712, predict: 0.0
label: -1.0, output: 0.008188361428788331, predict: 0.0
label: -1.0, output: 0.005559588343126536, predict: 0.0
label: -1.0, output: 0.005436460756324828, predict: 0.0
label: -1.0, output: 0.17851787624060464, predict: 0.0
label: -1.0, output: 0.009245701581494223, predict: 0.0
label: -1.0, output: 0.8085452453826882, predict: 1.0
label: -1.0, output: 0.00990728415969206, predict: 0.0
label: -1.0, output: 0.7626829835625218, predict: 1.0
label: -1.0, output: 0.002081603417466728, predict: 0.0
label: -1.0, output: 0.05072012740316011, predict: 0.0
label: -1.0, output: 0.1533197866181879, predict: 0.0
total: 0.0, correct: 0.0, wrong: 0.0, accurate: 0.0
predictTime: 146
==========Finish Predict==========
==========Start WritePredict==========
writePredictTime: 6958
==========Finish WritePredict==========
==========train 0==========
train: 0, loss: -1.0
layerType: input, nodeSize: 5
double[][] w: null
double[] b: null
double[][] partialDerivative: null
double[] z: null
double[] h: 
[-1.9941991262763736, 0.6138804859660809, -1.2241999287516743, -0.911696129588226, 0.5760437715856739]
layerType: hidden, nodeSize: 8
double[][] w: 
[-2.261772643279246, 2.0274259666461076, 1.9107611485718108, 0.005176841814013828, -0.06713420431323007]
[1.021036482179361, 1.6773979490225586, -1.9379160729400404, -0.6306423907038278, 0.9215282998197245]
[0.5745671321333642, -1.1182943023804637, 1.006087242996733, 0.017854932723056525, 0.49527744308761956]
[-2.359924685548056, -1.3357398539909913, -1.7347086422978675, -1.2937358487176749, 0.4474739510295182]
[0.510043687852742, 1.3211362275097016, 0.9164595612180664, -1.0625580595315247, 1.1239704336236607]
[1.3574863077835229, 1.7472085281825727, 2.0492309262999036, 0.7335940416155134, 0.7551700600244556]
[-0.5778188751588048, -0.6541528565082166, 0.5294170166424293, 0.8662498601365566, 0.9651345376738396]
[0.1488722088108032, 0.4476720866774303, -0.051770188732629126, 1.3874219124381169, 0.6842193614358532]
double[] b: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[][] partialDerivative: 
[-5.635564011681505E-12, -5.635564011681505E-12, -5.635564011681505E-12, -5.635564011681505E-12, -5.635564011681505E-12]
[3.954982350727136E-14, 3.954982350727136E-14, 3.954982350727136E-14, 3.954982350727136E-14, 3.954982350727136E-14]
[-1.2593084872138464E-8, -1.2593084872138464E-8, -1.2593084872138464E-8, -1.2593084872138464E-8, -1.2593084872138464E-8]
[1.9759959088426455E-6, 1.9759959088426455E-6, 1.9759959088426455E-6, 1.9759959088426455E-6, 1.9759959088426455E-6]
[-8.234976723588098E-7, -8.234976723588098E-7, -8.234976723588098E-7, -8.234976723588098E-7, -8.234976723588098E-7]
[-8.90330476040114E-8, -8.90330476040114E-8, -8.90330476040114E-8, -8.90330476040114E-8, -8.90330476040114E-8]
[-2.0406440215933393E-8, -2.0406440215933393E-8, -2.0406440215933393E-8, -2.0406440215933393E-8, -2.0406440215933393E-8]
[7.686262254142011E-7, 7.686262254142011E-7, 7.686262254142011E-7, 7.686262254142011E-7, 7.686262254142011E-7]
double[] z: 
[3.3724766578848393, 2.471763390076047, -2.794929040640631, 7.447063860034521, 0.2881475812184112, -4.376992981386093, -0.13119495708887075, -0.8294534953786773]
double[] h: 
[0.9668332014475884, 0.9221384689990111, 0.05759881690582303, 0.9994171885523783, 0.5715425697829261, 0.012407206572581018, 0.4672482245389546, 0.30376063810315657]
layerType: output, nodeSize: 1
double[][] w: 
[2.222586225965247, -2.193827835435187, 1.6938294258887954, -2.1415052005190938, 0.6875008203149753, 1.4187774478287558, 0.9251640054099878, -2.0897378112431593]
double[] b: 
[0.0]
double[][] partialDerivative: 
[-5.3594343654073304E-6, -5.3594343654073304E-6, -5.3594343654073304E-6, -5.3594343654073304E-6, -5.3594343654073304E-6, -5.3594343654073304E-6, -5.3594343654073304E-6, -5.3594343654073304E-6]
double[] z: 
[-1.7087972225038803]
double[] h: 
[0.1533197866181879]
==============================
programTotalTime: 6964
==========Start ReadInput==========
readInputTime: 555
==========Finish ReadInput==========
==========Start BuildNetwork==========
buildNetworkTime: 6
==========Finish BuildNetwork==========
==========train 0==========
train: 0, loss: -1.0
layerType: input, nodeSize: 5
double[][] w: null
double[] b: null
double[][] partialDerivative: null
double[] z: null
double[] h: 
[0.0, 0.0, 0.0, 0.0, 0.0]
layerType: hidden, nodeSize: 8
double[][] w: 
[0.41973887732818027, -0.5910369473262567, -0.8001822733644892, 0.1574339313454185, 0.06143843771360169]
[0.02797561794159731, -0.8522476079321155, 0.32097794716757644, -0.19570314908051722, -0.6129329207944534]
[0.8059257288956525, -0.7151199780539821, -0.5686129131886026, 0.6036386181611519, 0.4160185776290368]
[0.6591454205093759, 0.7614063735649776, -0.31709632614014427, 0.9074690606114584, -0.9364158682561561]
[-0.9019996268934862, -0.6581439787994883, -0.8528255869332391, -0.041262311188133616, -0.974902490507972]
[0.9504424155129707, -0.46037134290184123, -0.6132338879509702, 0.24509213906231353, 0.6642196731812535]
[-0.06687768409538886, -0.6305744000479838, -0.43508258500078445, -0.3158959078702499, 0.2270857688736938]
[-0.14715276825046963, 0.10575314723050355, -0.49735058470127713, 0.07760932235345575, -0.7060965042071903]
double[] b: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[][] partialDerivative: 
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
double[] z: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[] h: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
layerType: output, nodeSize: 1
double[][] w: 
[-0.008594492015888244, -0.20555734622288258, 0.7628284707249899, 0.9492674045309168, -0.564243371459558, 0.7957348106916, -0.8774802957311654, 0.34022498222608166]
double[] b: 
[0.0]
double[][] partialDerivative: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[] z: 
[0.0]
double[] h: 
[0.0]
==============================
==========Start Train==========
train: 1, loss: 0.2605976202238565
train: 2, loss: 0.21974589013113738
train: 3, loss: 0.050405238294165065
train: 4, loss: 0.10918231053078872
train: 5, loss: 0.1505793547326836
train: 6, loss: 0.03300915013165981
train: 7, loss: 0.05640254746113695
train: 8, loss: 0.009879646667178346
train: 9, loss: 0.09566070202355278
train: 10, loss: 0.10155139733696253
train: 11, loss: 0.09934260310903371
train: 12, loss: 0.17301752824133232
train: 13, loss: 0.051362799097321714
train: 14, loss: 0.10205176237604911
train: 15, loss: 0.009121311207774345
train: 16, loss: 0.17159526864439242
train: 17, loss: 0.005601627696643365
train: 18, loss: 0.0287675718656895
train: 19, loss: 0.024777290217010074
train: 20, loss: 0.1523097037413774
train: 21, loss: 0.05512393719666344
train: 22, loss: 0.04252842572620967
train: 23, loss: 0.0076517227357784465
train: 24, loss: 0.04979214484702566
train: 25, loss: 0.00535536293381308
train: 26, loss: 0.03561377158239322
train: 27, loss: 0.1292756872085434
train: 28, loss: 0.20786830192747177
train: 29, loss: 0.0014833798701876745
train: 30, loss: 6.656905639887742E-4
train: 31, loss: 0.027392561502309818
train: 32, loss: 0.03866335631825437
train: 33, loss: 6.619881219399962E-4
train: 34, loss: 0.044677130393231866
train: 35, loss: 0.09468033913396419
train: 36, loss: 0.002829511916152916
train: 37, loss: 0.044105387928387745
train: 38, loss: 0.001471883378882458
train: 39, loss: 0.006690049373527167
train: 40, loss: 0.06718018049266024
train: 41, loss: 0.29749392860494794
train: 42, loss: 0.13048782943480208
train: 43, loss: 0.0035469924259879976
train: 44, loss: 0.02514959168723503
train: 45, loss: 0.08553186863761503
train: 46, loss: 0.030903848880878736
train: 47, loss: 5.309593804099177E-4
train: 48, loss: 8.692720261054672E-4
train: 49, loss: 0.03332912661098445
train: 50, loss: 0.07038687201236654
train: 51, loss: 0.024122491657625506
train: 52, loss: 0.022766899938748718
train: 53, loss: 0.0312507429294624
train: 54, loss: 0.020085707504747488
train: 55, loss: 0.02929097522963261
train: 56, loss: 0.005582535937246665
train: 57, loss: 0.2711743031351318
train: 58, loss: 0.09777837860621207
train: 59, loss: 0.002164181843210635
train: 60, loss: 0.0022692188760728353
train: 61, loss: 1.7737382339536343E-4
train: 62, loss: 0.024535401411662258
train: 63, loss: 6.323270713000299E-4
train: 64, loss: 0.05381895825619414
train: 65, loss: 0.0023503156413244275
train: 66, loss: 0.09871169396799204
train: 67, loss: 0.046600200966855826
train: 68, loss: 0.01719245121318632
train: 69, loss: 8.925207350763581E-4
train: 70, loss: 0.015041830183154631
train: 71, loss: 0.004021955020620458
train: 72, loss: 0.4021710695199557
train: 73, loss: 2.26177765287249E-4
train: 74, loss: 0.01773117903540403
train: 75, loss: 0.01409500203301638
train: 76, loss: 0.01566487495756388
train: 77, loss: 0.010416528912981931
train: 78, loss: 0.0016568582660694573
train: 79, loss: 3.0768193325825435E-5
train: 80, loss: 0.47062364924886074
train: 81, loss: 0.050070716114589936
train: 82, loss: 0.0066503185315320784
train: 83, loss: 0.06771332474899075
train: 84, loss: 0.010150377684813784
train: 85, loss: 0.01287789156019592
train: 86, loss: 0.0022129692530433024
train: 87, loss: 0.025079653777553747
train: 88, loss: 0.029118423872817253
train: 89, loss: 0.002894617350307461
train: 90, loss: 0.002010571125836713
train: 91, loss: 0.008383246999484428
train: 92, loss: 0.003485824952369109
train: 93, loss: 1.472169517694192E-5
train: 94, loss: 0.016088491258450893
train: 95, loss: 0.290845393887261
train: 96, loss: 0.0036165149003668
train: 97, loss: 6.356010904382915E-5
train: 98, loss: 0.003557128469610143
train: 99, loss: 2.691938635686156E-4
train: 100, loss: 0.06131843155033827
train: 101, loss: 5.9014725543372934E-5
train: 102, loss: 0.039549879102321266
train: 103, loss: 9.883743399590467E-6
train: 104, loss: 0.006552125812036995
train: 105, loss: 0.0015030357397217164
train: 106, loss: 2.4513288012650988E-5
train: 107, loss: 0.11366736052952216
train: 108, loss: 0.0010222580492424689
train: 109, loss: 7.399719448900283E-6
train: 110, loss: 0.08945570909568923
train: 111, loss: 0.007027644217528022
train: 112, loss: 0.02477313437682554
train: 113, loss: 2.290355774131225E-5
train: 114, loss: 0.006647118400508914
train: 115, loss: 0.4815442359129147
train: 116, loss: 7.297810122310491E-5
train: 117, loss: 0.015176517966878923
train: 118, loss: 0.47501198152570134
train: 119, loss: 0.48352108401912064
train: 120, loss: 0.0015273691425163349
train: 121, loss: 7.101033816482282E-5
train: 122, loss: 0.011965700526759134
train: 123, loss: 0.01453042075610744
train: 124, loss: 0.012340075654295004
train: 125, loss: 5.1911387497066333E-5
train: 126, loss: 0.0037283147285880618
train: 127, loss: 0.013681351577425603
train: 128, loss: 1.868685888400729E-4
train: 129, loss: 6.160205626925057E-5
train: 130, loss: 0.07384545449443147
train: 131, loss: 0.020537251749394976
train: 132, loss: 2.5061636921329323E-5
train: 133, loss: 0.013424900782097323
train: 134, loss: 0.001312163815363752
train: 135, loss: 1.404674085091749E-5
train: 136, loss: 1.6774979053741385E-5
train: 137, loss: 0.0012364341453650748
train: 138, loss: 3.5155600920207036E-4
train: 139, loss: 0.0677720316366063
train: 140, loss: 8.228640835271592E-5
train: 141, loss: 0.06069014709422827
train: 142, loss: 1.4855731948246338E-5
train: 143, loss: 0.010838606684193222
train: 144, loss: 1.591176676950888E-6
train: 145, loss: 0.002472224157619062
train: 146, loss: 2.9682010505353523E-5
train: 147, loss: 0.043123793099138774
train: 148, loss: 0.0018546906096011846
train: 149, loss: 0.4912579638496043
train: 150, loss: 0.0016770783728686628
train: 151, loss: 0.0054292535604660635
train: 152, loss: 0.009337399526784128
train: 153, loss: 0.4591614786855059
train: 154, loss: 1.3675627753026809E-6
train: 155, loss: 0.0015817363126803129
train: 156, loss: 0.005000949343144811
train: 157, loss: 0.02664360397430731
train: 158, loss: 0.03995854030188905
train: 159, loss: 1.3764554326073195E-5
train: 160, loss: 1.1320373537517123E-6
train: 161, loss: 0.4338874912203017
train: 162, loss: 5.34884636890642E-6
train: 163, loss: 6.963165105651264E-7
train: 164, loss: 0.43382793545402365
train: 165, loss: 0.015175301537436958
train: 166, loss: 0.009323224656597887
train: 167, loss: 0.027312648303429766
train: 168, loss: 0.00171352629232077
train: 169, loss: 5.3008746599423165E-6
train: 170, loss: 0.02400114181011414
train: 171, loss: 0.004010006574535251
train: 172, loss: 0.004362640551173009
train: 173, loss: 7.619730372603815E-4
train: 174, loss: 0.1663888376385637
train: 175, loss: 0.06921826026570325
train: 176, loss: 0.0030119443734725644
train: 177, loss: 0.003509295214695051
train: 178, loss: 0.021805200972800007
train: 179, loss: 0.01523545962157
train: 180, loss: 1.0681867808713456E-5
train: 181, loss: 0.4550298430183253
train: 182, loss: 0.002684496961438409
train: 183, loss: 0.03482699330606488
train: 184, loss: 0.01653551793658867
train: 185, loss: 0.007222424742589541
train: 186, loss: 1.0282062905052509E-5
train: 187, loss: 0.001987309536845477
train: 188, loss: 0.48848227771405045
train: 189, loss: 0.007696224284859759
train: 190, loss: 0.06584298639068904
train: 191, loss: 0.0015974796206251578
train: 192, loss: 0.00959231456029775
train: 193, loss: 0.10196734681661247
train: 194, loss: 1.9063398430531158E-4
train: 195, loss: 1.1223507174850772E-6
train: 196, loss: 9.861738221683058E-5
train: 197, loss: 0.00445420976619038
train: 198, loss: 0.4591789485274375
train: 199, loss: 0.2732155516632223
train: 200, loss: 0.05212342665872176
train: 201, loss: 4.849375067744611E-6
train: 202, loss: 0.005169073912396201
train: 203, loss: 4.1251553284914986E-4
train: 204, loss: 5.251785600710678E-4
train: 205, loss: 0.009308713035546395
train: 206, loss: 7.804246376018942E-4
train: 207, loss: 0.01906473951389822
train: 208, loss: 0.0015978452006944043
train: 209, loss: 0.0010713296510314661
train: 210, loss: 0.010977924249105963
train: 211, loss: 0.006080337521254312
train: 212, loss: 0.00199141695109544
train: 213, loss: 0.007754166452773672
train: 214, loss: 0.010395274119016038
train: 215, loss: 0.006385352777055962
train: 216, loss: 0.0031310929197018037
train: 217, loss: 2.4923996247531E-6
train: 218, loss: 0.004028568027321586
train: 219, loss: 0.07599909590136741
train: 220, loss: 9.674581701320452E-7
train: 221, loss: 6.327091796362178E-7
train: 222, loss: 0.1224067670340108
train: 223, loss: 0.2087354094997402
train: 224, loss: 4.199848441804418E-8
train: 225, loss: 0.0016885349194552797
train: 226, loss: 0.0019778518827341723
train: 227, loss: 0.016411485766115338
train: 228, loss: 0.010699667920263178
train: 229, loss: 0.008507801147013218
train: 230, loss: 7.06210731922852E-4
train: 231, loss: 0.015533518362871267
train: 232, loss: 2.11664877705043E-4
train: 233, loss: 0.005666197493408673
train: 234, loss: 1.046795051003289E-6
train: 235, loss: 0.007242304079482248
train: 236, loss: 0.011318850055281387
train: 237, loss: 1.0350688801112026E-5
train: 238, loss: 3.9756645454240475E-8
train: 239, loss: 0.0018324593210981139
train: 240, loss: 0.008838800522022672
train: 241, loss: 0.022208474938771178
train: 242, loss: 0.006686627049666331
train: 243, loss: 0.0010066933782356211
train: 244, loss: 0.05219832325058584
train: 245, loss: 2.5907067460014313E-4
train: 246, loss: 1.8502408760128357E-7
train: 247, loss: 0.003461894016423469
train: 248, loss: 0.003324054887758623
train: 249, loss: 0.0013025112606208404
train: 250, loss: 0.00633874444229076
train: 251, loss: 0.007159870482541426
train: 252, loss: 1.6769810217030295E-8
train: 253, loss: 1.5240810193095424E-8
train: 254, loss: 9.506963625103524E-7
train: 255, loss: 0.026661332617927757
train: 256, loss: 8.370557852599984E-9
train: 257, loss: 0.01053045326552217
train: 258, loss: 4.0537251366367907E-8
train: 259, loss: 3.392722869793636E-7
train: 260, loss: 0.4573061923369825
train: 261, loss: 0.03209413279922341
train: 262, loss: 0.014505110900912528
train: 263, loss: 2.4903186049356004E-5
train: 264, loss: 4.3233155148068335E-6
train: 265, loss: 0.29688458122105965
train: 266, loss: 0.0357314547399437
train: 267, loss: 0.008496681655537812
train: 268, loss: 8.907263401630616E-9
train: 269, loss: 0.010589115671012648
train: 270, loss: 3.111223113667419E-7
train: 271, loss: 0.008428411232728115
train: 272, loss: 7.312216750157509E-4
train: 273, loss: 6.832217429687542E-5
train: 274, loss: 0.002742013602292006
train: 275, loss: 1.7509133639373526E-4
train: 276, loss: 6.135974721625012E-4
train: 277, loss: 5.263752067199642E-4
train: 278, loss: 5.195359053200892E-7
train: 279, loss: 0.0016253691134851056
train: 280, loss: 0.27309627324652297
train: 281, loss: 0.0012224876778876488
train: 282, loss: 0.009679508888408142
train: 283, loss: 0.0035347562243668324
train: 284, loss: 6.127878042165147E-4
train: 285, loss: 0.012304986678878306
train: 286, loss: 0.002468516628482973
train: 287, loss: 8.98081436892874E-4
train: 288, loss: 5.797587942919539E-9
train: 289, loss: 0.028907706566010192
train: 290, loss: 2.7124456659292413E-4
train: 291, loss: 3.1196028563345708E-6
train: 292, loss: 8.349532592956104E-9
train: 293, loss: 0.0018135708775265142
train: 294, loss: 0.018015994475377776
train: 295, loss: 0.010040312573053456
train: 296, loss: 9.190720429182627E-7
train: 297, loss: 0.010299945700344879
train: 298, loss: 0.018899994601245217
train: 299, loss: 0.004867333377206359
train: 300, loss: 0.008046278465297374
train: 301, loss: 6.939933272288219E-4
train: 302, loss: 0.27254131542929966
train: 303, loss: 8.672136942577038E-5
train: 304, loss: 0.017857977691581674
train: 305, loss: 7.656862953164815E-4
train: 306, loss: 0.004449648537210771
train: 307, loss: 0.008449033581140833
train: 308, loss: 0.013633736034161516
train: 309, loss: 0.009639832772753619
train: 310, loss: 0.3298416898480739
train: 311, loss: 3.6583926439624104E-6
train: 312, loss: 0.06542376955814361
train: 313, loss: 0.02354738050217099
train: 314, loss: 0.028193002782646127
train: 315, loss: 0.0012614182527568964
train: 316, loss: 0.0011402453365842776
train: 317, loss: 0.005245437076675853
train: 318, loss: 0.02712020964223811
train: 319, loss: 3.416034158645636E-4
train: 320, loss: 6.58453576083343E-6
train: 321, loss: 0.020809351475064464
train: 322, loss: 2.4397587564944324E-6
train: 323, loss: 0.002609281929425054
train: 324, loss: 2.950652859717375E-6
train: 325, loss: 0.03119638387536261
train: 326, loss: 1.2093509951437683E-5
train: 327, loss: 2.3693412170761048E-5
train: 328, loss: 2.826890551544497E-4
train: 329, loss: 1.0836023404583821E-7
train: 330, loss: 0.00554239250620204
train: 331, loss: 0.22436164663064578
train: 332, loss: 2.8056166221940796E-6
train: 333, loss: 0.014264969066226641
train: 334, loss: 5.372078962505526E-4
train: 335, loss: 0.005647497533381914
train: 336, loss: 0.01234638440988802
train: 337, loss: 0.005199392058055914
train: 338, loss: 0.0018360820476518234
train: 339, loss: 3.0983147804793775E-7
train: 340, loss: 0.0036831047294557818
train: 341, loss: 4.2474972551308855E-6
train: 342, loss: 3.922236708300804E-6
train: 343, loss: 0.006253500709887309
train: 344, loss: 0.019710834865576093
train: 345, loss: 0.01634877440795876
train: 346, loss: 0.009593170382475476
train: 347, loss: 0.0035763106221372506
train: 348, loss: 0.08491794534954486
train: 349, loss: 0.003163909559464369
train: 350, loss: 2.1476336160962693E-4
train: 351, loss: 3.592755993127754E-4
train: 352, loss: 0.0029100745744898822
train: 353, loss: 7.755114003549516E-8
train: 354, loss: 0.0019406819784707015
train: 355, loss: 0.006993371570197065
train: 356, loss: 2.2234845086698844E-4
train: 357, loss: 0.015293764697780704
train: 358, loss: 2.86626469919324E-7
train: 359, loss: 0.0039915663861852455
train: 360, loss: 2.968595435669977E-7
train: 361, loss: 1.1474863546674224E-5
train: 362, loss: 1.987077077025918E-4
train: 363, loss: 0.024854562328136633
train: 364, loss: 0.019489288637003714
train: 365, loss: 6.218433236118953E-6
train: 366, loss: 0.004075218453274615
train: 367, loss: 1.1055877128113853E-6
train: 368, loss: 4.0247671154652416E-9
train: 369, loss: 0.004623622991202324
train: 370, loss: 0.0025401179187309717
train: 371, loss: 0.3336033080715384
train: 372, loss: 0.014102105920414203
train: 373, loss: 0.07192631862401427
train: 374, loss: 3.0404386567137236E-4
train: 375, loss: 0.004891441776040902
train: 376, loss: 0.49768556365825367
train: 377, loss: 1.2854206229713471E-5
train: 378, loss: 4.3571779329145443E-7
train: 379, loss: 7.857826439349595E-6
train: 380, loss: 0.010490793527616806
train: 381, loss: 0.008615170460180373
train: 382, loss: 0.035252863308734936
train: 383, loss: 0.0025802297017476646
train: 384, loss: 4.250682222685701E-4
train: 385, loss: 0.0014130360664285563
train: 386, loss: 0.10099465258449139
train: 387, loss: 0.017258040946435172
train: 388, loss: 2.1046911056729474E-7
train: 389, loss: 2.5406750957150144E-4
train: 390, loss: 0.008619859743086941
train: 391, loss: 0.008308071685101157
train: 392, loss: 0.0023373790725019538
train: 393, loss: 2.094336783270118E-4
train: 394, loss: 0.00925422077640513
train: 395, loss: 2.573493919296958E-9
train: 396, loss: 0.004573914493470606
train: 397, loss: 0.005075428319493396
train: 398, loss: 0.12535044744174734
train: 399, loss: 0.0022050735813665084
train: 400, loss: 0.047615548287409395
train: 401, loss: 0.16926746537277218
train: 402, loss: 0.0952541077107621
train: 403, loss: 5.799394319734212E-10
train: 404, loss: 0.005186239569502541
train: 405, loss: 0.02414745005838255
train: 406, loss: 0.004788180254296965
train: 407, loss: 1.484246219490335E-4
train: 408, loss: 1.1019700961031946E-4
train: 409, loss: 0.006135483337320899
train: 410, loss: 4.4332981906768966E-4
train: 411, loss: 0.0016017471622216189
train: 412, loss: 1.0959386609865389E-4
train: 413, loss: 0.003525775550637268
train: 414, loss: 0.009972962980544415
train: 415, loss: 0.007477204409031446
train: 416, loss: 6.033211168507015E-4
train: 417, loss: 0.005973931944412517
train: 418, loss: 0.012021986511331845
train: 419, loss: 0.024717555148067744
train: 420, loss: 8.659765588409339E-8
train: 421, loss: 0.08680839660394848
train: 422, loss: 0.004107937274704614
train: 423, loss: 3.268964514543883E-6
train: 424, loss: 0.0026485491293172262
train: 425, loss: 0.005335362558312965
train: 426, loss: 4.390990011657912E-5
train: 427, loss: 0.0014798978729196899
train: 428, loss: 1.2823717709787045E-4
train: 429, loss: 0.001595313160029913
train: 430, loss: 9.549894657868589E-5
train: 431, loss: 0.019878549916671408
train: 432, loss: 0.012402227782974222
train: 433, loss: 0.004143062372154989
train: 434, loss: 3.7494398436528594E-6
train: 435, loss: 0.00273379759044923
train: 436, loss: 8.332789412429413E-5
train: 437, loss: 0.011175021601988345
train: 438, loss: 0.0021700530178543178
train: 439, loss: 0.49875864506140954
train: 440, loss: 0.006314165262448864
train: 441, loss: 0.0042429039268683305
train: 442, loss: 2.0322981228978467E-5
train: 443, loss: 0.008323380050819345
train: 444, loss: 0.06380007829176432
train: 445, loss: 0.0053396921156614215
train: 446, loss: 0.136914868938052
train: 447, loss: 0.00926108496991881
train: 448, loss: 1.6217054003972269E-4
train: 449, loss: 0.001275282730665716
train: 450, loss: 0.009275069355759814
train: 451, loss: 6.95049139692438E-5
train: 452, loss: 0.13216823861767
train: 453, loss: 1.7196848642572439E-6
train: 454, loss: 3.6786239058553762E-6
train: 455, loss: 0.018836369257789212
train: 456, loss: 0.048615615767180716
train: 457, loss: 0.005470003334554601
train: 458, loss: 0.022387247195312116
train: 459, loss: 1.3822031364222538E-5
train: 460, loss: 0.09734895182604393
train: 461, loss: 4.657865529506858E-5
train: 462, loss: 0.009274514919536979
train: 463, loss: 1.9135138439908204E-4
train: 464, loss: 0.003739350436511388
train: 465, loss: 4.164238236275055E-5
train: 466, loss: 0.0031003514699029107
train: 467, loss: 0.22669090908160527
train: 468, loss: 0.006189521575758526
train: 469, loss: 0.033012824729652446
train: 470, loss: 9.711933072496099E-11
train: 471, loss: 0.003144214771778438
train: 472, loss: 0.007034618351109967
train: 473, loss: 3.7979776229182745E-7
train: 474, loss: 0.008746945666087904
train: 475, loss: 9.52546424527337E-10
train: 476, loss: 7.275385555451939E-6
train: 477, loss: 0.009377529573323499
train: 478, loss: 3.871799713826794E-4
train: 479, loss: 0.018177675442676258
train: 480, loss: 0.047666428095125266
train: 481, loss: 2.969181712899006E-5
train: 482, loss: 4.436663542227007E-9
train: 483, loss: 0.006145047582059384
train: 484, loss: 9.717111538416392E-6
train: 485, loss: 8.670585121938752E-6
train: 486, loss: 5.811062684424255E-4
train: 487, loss: 0.010230470592205318
train: 488, loss: 0.0015086792938335594
train: 489, loss: 0.04050767574813558
train: 490, loss: 0.008141406071504799
train: 491, loss: 0.009024372838820497
train: 492, loss: 2.6187608764397573E-5
train: 493, loss: 0.005146152579755917
train: 494, loss: 0.005899760129804911
train: 495, loss: 6.270123363400149E-5
train: 496, loss: 5.7569946334836906E-5
train: 497, loss: 5.818225166835587E-8
train: 498, loss: 1.7036083012758138E-4
train: 499, loss: 0.025746332339441614
train: 500, loss: 5.849211842176106E-10
train: 501, loss: 0.003712514075347144
train: 502, loss: 1.875290574960522E-5
train: 503, loss: 0.0014868823798385247
train: 504, loss: 0.4967475361770297
train: 505, loss: 0.0062310244318861474
train: 506, loss: 0.00555152005644981
train: 507, loss: 0.00744979488492432
train: 508, loss: 0.004239519227579428
train: 509, loss: 1.5527925974002514E-5
train: 510, loss: 2.339162176238065E-5
train: 511, loss: 0.009149603390248137
train: 512, loss: 0.003770104111151774
train: 513, loss: 0.007741576084240276
train: 514, loss: 1.8518181580215172E-8
train: 515, loss: 0.006476377139914374
train: 516, loss: 0.027595688964084177
train: 517, loss: 0.49117937413211504
train: 518, loss: 0.003343218351587826
train: 519, loss: 0.3224194719603739
train: 520, loss: 0.0012679071811195052
train: 521, loss: 9.282756537003796E-4
train: 522, loss: 0.0069764095362533154
train: 523, loss: 7.545826464903608E-4
train: 524, loss: 5.778383528047956E-10
train: 525, loss: 0.4978914625419606
train: 526, loss: 7.259363524623731E-6
train: 527, loss: 0.1714654501788398
train: 528, loss: 0.3244995468254284
train: 529, loss: 0.0011437246743537157
train: 530, loss: 0.008410236423457796
train: 531, loss: 0.0054281780015620305
train: 532, loss: 0.1208607193940179
train: 533, loss: 0.0036606311164825655
train: 534, loss: 0.002362268637824527
train: 535, loss: 1.5630277963107415E-7
train: 536, loss: 8.757361029995157E-10
train: 537, loss: 5.191404547137153E-5
train: 538, loss: 0.39323461193301124
train: 539, loss: 4.125345213215592E-4
train: 540, loss: 0.0016306230763959617
train: 541, loss: 0.00672145607526485
train: 542, loss: 0.0030561791711586446
train: 543, loss: 0.006674168239004865
train: 544, loss: 0.004220447295208435
train: 545, loss: 1.8945514698617013E-6
train: 546, loss: 0.002454251817383884
train: 547, loss: 2.3601492903727523E-10
train: 548, loss: 0.048288944522686394
train: 549, loss: 1.5449940197088725E-9
train: 550, loss: 0.004904649275762568
train: 551, loss: 0.01984550638793069
train: 552, loss: 0.00385876324839871
train: 553, loss: 0.0039075583585104195
train: 554, loss: 0.0055677485830376025
train: 555, loss: 0.03296476653925137
train: 556, loss: 1.28804034822748E-6
train: 557, loss: 0.010511065292476555
train: 558, loss: 0.004227333415176426
train: 559, loss: 0.02316039364864006
train: 560, loss: 0.011521715292520386
train: 561, loss: 4.3172391456522435E-4
train: 562, loss: 3.563546012385731E-10
train: 563, loss: 3.7679831350878596E-7
train: 564, loss: 4.3585873275847427E-10
train: 565, loss: 0.005812519525613747
train: 566, loss: 1.5710547825319216E-11
train: 567, loss: 2.567573378494303E-4
train: 568, loss: 2.3735606986519223E-4
train: 569, loss: 4.5111903916518705E-10
train: 570, loss: 5.689244430400635E-6
train: 571, loss: 0.11419263125397029
train: 572, loss: 0.005894528178184351
train: 573, loss: 3.263451563825871E-6
train: 574, loss: 4.167716953836272E-9
train: 575, loss: 0.013120737466960292
train: 576, loss: 2.4830152271230945E-4
train: 577, loss: 0.002840003626504305
train: 578, loss: 0.0024659314816199147
train: 579, loss: 0.004564720304465449
train: 580, loss: 3.0088502206656954E-5
train: 581, loss: 0.4912785093821727
train: 582, loss: 3.3667808537123164E-9
train: 583, loss: 0.016983460705151797
train: 584, loss: 6.796743441807697E-6
train: 585, loss: 0.0067930394561941145
train: 586, loss: 0.4933028480318228
train: 587, loss: 1.4481234685904477E-11
train: 588, loss: 0.03696785488534316
train: 589, loss: 9.311198375452421E-4
train: 590, loss: 0.003870619731029307
train: 591, loss: 8.792473913764033E-11
train: 592, loss: 0.00472352776670532
train: 593, loss: 0.004360811658200634
train: 594, loss: 4.095755377290512E-5
train: 595, loss: 0.006796918782166438
train: 596, loss: 0.3567042538999678
train: 597, loss: 0.006822637973229865
train: 598, loss: 0.012827555854240323
train: 599, loss: 3.711877690306556E-9
train: 600, loss: 4.199778138469324E-8
train: 601, loss: 1.1888954478104071E-9
train: 602, loss: 5.8601183869123E-10
train: 603, loss: 4.890381695826707E-6
train: 604, loss: 1.041911751131736E-9
train: 605, loss: 0.3162351409524132
train: 606, loss: 0.4918673216240095
train: 607, loss: 0.0021749803924260027
train: 608, loss: 0.11510007809616676
train: 609, loss: 0.005760442962925897
train: 610, loss: 4.756460210614486E-4
train: 611, loss: 0.01982967556828332
train: 612, loss: 0.004389463154211246
train: 613, loss: 6.709653642202776E-6
train: 614, loss: 0.005247209871318808
train: 615, loss: 2.079524324251591E-10
train: 616, loss: 0.462570468069291
train: 617, loss: 4.577576164259774E-11
train: 618, loss: 7.0356083859201025E-6
train: 619, loss: 0.0070302716519270985
train: 620, loss: 0.004798630340265545
train: 621, loss: 0.004217172401616621
train: 622, loss: 0.001793638857761712
train: 623, loss: 0.005853188042665495
train: 624, loss: 0.49382328900602157
train: 625, loss: 0.01897338816347008
train: 626, loss: 1.1222144873318203E-5
train: 627, loss: 0.00858954769612076
train: 628, loss: 0.26949088509614344
train: 629, loss: 8.525619040460103E-5
train: 630, loss: 0.004788626878404591
train: 631, loss: 1.0071326852192062E-11
train: 632, loss: 1.0322814048206073E-4
train: 633, loss: 0.0030862550297083135
train: 634, loss: 0.006983944707301432
train: 635, loss: 9.634663157839995E-4
train: 636, loss: 0.0028901082289931757
train: 637, loss: 0.007037941617311955
train: 638, loss: 2.6274063669183207E-8
train: 639, loss: 0.007293094905036435
train: 640, loss: 1.1173997704423833E-10
train: 641, loss: 1.3768886311773871E-8
train: 642, loss: 0.001397470217011703
train: 643, loss: 0.00420268464102083
train: 644, loss: 0.0035022807563591944
train: 645, loss: 1.2093232943070386E-11
train: 646, loss: 5.874163573950657E-9
train: 647, loss: 0.009094143125278429
train: 648, loss: 0.1389764689755646
train: 649, loss: 0.48801848545152104
train: 650, loss: 0.005950981402786486
train: 651, loss: 1.7563188919754882E-8
train: 652, loss: 0.0038545199524362907
train: 653, loss: 4.709543702445659E-7
train: 654, loss: 8.791679746718008E-11
train: 655, loss: 0.49990383062314836
train: 656, loss: 2.959594942216783E-6
train: 657, loss: 0.006577288258073213
train: 658, loss: 5.398730074147557E-7
train: 659, loss: 0.004561614571116998
train: 660, loss: 2.1511267847757975E-4
train: 661, loss: 7.862030415615425E-4
train: 662, loss: 0.009226687826337667
train: 663, loss: 2.811686654794953E-6
train: 664, loss: 0.003986991116021725
train: 665, loss: 1.827746993398859E-10
train: 666, loss: 2.436328422558348E-5
train: 667, loss: 0.49716376330232687
train: 668, loss: 0.003336621065723352
train: 669, loss: 5.97558994339126E-11
train: 670, loss: 0.00767830259256859
train: 671, loss: 1.3644536627022963E-5
train: 672, loss: 0.012123812964132947
train: 673, loss: 1.4616193239193978E-8
train: 674, loss: 3.1157312605693027E-6
train: 675, loss: 1.8380009025593125E-4
train: 676, loss: 4.7309070927768484E-5
train: 677, loss: 1.3042129005828845E-11
train: 678, loss: 2.1612597075119215E-10
train: 679, loss: 9.126950267205348E-4
train: 680, loss: 0.0011122421071490297
train: 681, loss: 0.004126174795864838
train: 682, loss: 0.015397711212549316
train: 683, loss: 4.879492877447394E-5
train: 684, loss: 0.006780425072800146
train: 685, loss: 1.8050164785098538E-6
train: 686, loss: 9.552779021628825E-12
train: 687, loss: 0.020768585654656384
train: 688, loss: 0.011372262715130727
train: 689, loss: 1.011495355397903E-9
train: 690, loss: 3.033825289658923E-7
train: 691, loss: 0.004272703969749166
train: 692, loss: 7.276776324630455E-11
train: 693, loss: 3.1702095659113003E-6
train: 694, loss: 0.004002630527395903
train: 695, loss: 1.406073359055068E-10
train: 696, loss: 0.004965359698001516
train: 697, loss: 1.479232786002153E-9
train: 698, loss: 1.8571510337977962E-4
train: 699, loss: 0.010252183193006843
train: 700, loss: 0.002930149910920813
train: 701, loss: 3.1912184392220163E-10
train: 702, loss: 0.002000136859022661
train: 703, loss: 0.010803046897527872
train: 704, loss: 3.46076236292954E-8
train: 705, loss: 0.004941238593387805
train: 706, loss: 1.6218724355237044E-7
train: 707, loss: 1.2629462733126569E-8
train: 708, loss: 3.4113837824525293E-4
train: 709, loss: 6.1115133981918415E-6
train: 710, loss: 2.609463445545564E-6
train: 711, loss: 3.887654327161465E-6
train: 712, loss: 0.006671573182991503
train: 713, loss: 8.997418227236487E-9
train: 714, loss: 0.007493341820165169
train: 715, loss: 3.3011786196893E-12
train: 716, loss: 2.3481810124754586E-8
train: 717, loss: 0.004392769786088782
train: 718, loss: 0.0018951441604645074
train: 719, loss: 2.7966414224980557E-11
train: 720, loss: 0.31974658379322113
train: 721, loss: 0.20568433179647883
train: 722, loss: 0.07759754951380447
train: 723, loss: 0.01821039513146674
train: 724, loss: 3.0775823392175666E-5
train: 725, loss: 5.922567782830199E-6
train: 726, loss: 8.121243647533405E-9
train: 727, loss: 9.182542180777105E-8
train: 728, loss: 0.004163969395707055
train: 729, loss: 0.0022793591476773277
train: 730, loss: 3.910416112867282E-4
train: 731, loss: 0.0025069901405194565
train: 732, loss: 0.02046451580480359
train: 733, loss: 0.004313095690998299
train: 734, loss: 6.031215068219806E-6
train: 735, loss: 0.005225449233670496
train: 736, loss: 3.8172679070450215E-11
train: 737, loss: 1.8842771432996873E-10
train: 738, loss: 3.290744256310817E-9
train: 739, loss: 1.8676242309003341E-6
train: 740, loss: 6.600145377304991E-11
train: 741, loss: 3.3793195301517556E-6
train: 742, loss: 8.886463088382093E-6
train: 743, loss: 1.3151663009039298E-7
train: 744, loss: 0.002550518994802063
train: 745, loss: 5.501588484738103E-5
train: 746, loss: 2.593130499390268E-11
train: 747, loss: 1.6489465726049557E-10
train: 748, loss: 0.0031775232519944763
train: 749, loss: 0.009389225548700425
train: 750, loss: 3.008134858622323E-10
train: 751, loss: 7.356572930181587E-11
train: 752, loss: 0.019371978254471126
train: 753, loss: 2.190643850428896E-6
train: 754, loss: 0.03896738610192038
train: 755, loss: 0.012783592951832054
train: 756, loss: 3.992010086978219E-5
train: 757, loss: 1.22142973016606E-5
train: 758, loss: 4.298459322335535E-11
train: 759, loss: 0.002397642781426188
train: 760, loss: 3.093035659056055E-5
train: 761, loss: 0.0026992834716698414
train: 762, loss: 0.00398283003920538
train: 763, loss: 0.001718779026724124
train: 764, loss: 5.006660177438041E-8
train: 765, loss: 0.027525627534542636
train: 766, loss: 0.3924364385618847
train: 767, loss: 1.5228075700254591E-5
train: 768, loss: 2.9201566167636747E-5
train: 769, loss: 1.7075971193088913E-6
train: 770, loss: 0.002144561301525556
train: 771, loss: 0.003959528438877017
train: 772, loss: 0.05037394961325279
train: 773, loss: 1.0731531762082004E-7
train: 774, loss: 9.088673956013544E-7
train: 775, loss: 1.1859837840510546E-9
train: 776, loss: 5.1565327974570586E-8
train: 777, loss: 2.203229151294541E-6
train: 778, loss: 0.0051968638895983615
train: 779, loss: 0.003977707780426558
train: 780, loss: 0.008266885447737345
train: 781, loss: 3.958798384404274E-4
train: 782, loss: 1.7978413085338004E-11
train: 783, loss: 0.005313760791320195
train: 784, loss: 0.08606178668582273
train: 785, loss: 0.004987587213064093
train: 786, loss: 0.007356874348160676
train: 787, loss: 0.05472418490606926
train: 788, loss: 2.3654505810077173E-5
train: 789, loss: 0.49706944505890094
train: 790, loss: 0.0076614359009826985
train: 791, loss: 0.4785461352346065
train: 792, loss: 1.0837638649816989E-10
train: 793, loss: 0.0033556731785567263
train: 794, loss: 0.0037857144203190946
train: 795, loss: 0.008714514235959765
train: 796, loss: 6.130222998315912E-8
train: 797, loss: 0.009299942370457026
train: 798, loss: 1.4698213580788086E-5
train: 799, loss: 1.5622452750197664E-4
train: 800, loss: 4.621211601202692E-6
train: 801, loss: 0.008744042115130309
train: 802, loss: 0.0033304406964457184
train: 803, loss: 0.0055746782441449495
train: 804, loss: 0.021600480674563948
train: 805, loss: 5.42899278510744E-7
train: 806, loss: 1.9223938360317935E-6
train: 807, loss: 6.547825293821467E-5
train: 808, loss: 0.34282532806815075
train: 809, loss: 0.009538584639311176
train: 810, loss: 0.0038265226206451158
train: 811, loss: 0.003490836561536373
train: 812, loss: 0.01275080464258818
train: 813, loss: 0.001954067341619735
train: 814, loss: 0.019623964954076634
train: 815, loss: 0.4933705942791135
train: 816, loss: 1.1184938367107358E-7
train: 817, loss: 0.00644953137378246
train: 818, loss: 0.4797604219692076
train: 819, loss: 3.7710308586444316E-4
train: 820, loss: 1.8196205795888913E-12
train: 821, loss: 4.680159086847726E-4
train: 822, loss: 0.006172788488740335
train: 823, loss: 1.7853569118432988E-6
train: 824, loss: 0.003057490545951959
train: 825, loss: 0.018407408247404757
train: 826, loss: 2.564996620316787E-10
train: 827, loss: 0.0024274581242794673
train: 828, loss: 2.851871695919485E-4
train: 829, loss: 8.757817283228166E-8
train: 830, loss: 0.007526788319940609
train: 831, loss: 0.0033960793972922895
train: 832, loss: 0.24097307313548944
train: 833, loss: 0.01248143659208072
train: 834, loss: 0.4959528406917291
train: 835, loss: 0.002657332435741669
train: 836, loss: 5.840552255469669E-6
train: 837, loss: 7.752423025977643E-13
train: 838, loss: 6.909197216981281E-13
train: 839, loss: 4.079505385480999E-6
train: 840, loss: 0.003154809714603904
train: 841, loss: 0.04857467890564243
train: 842, loss: 1.325983166605707E-12
train: 843, loss: 5.5914603324122806E-8
train: 844, loss: 0.0029314754156925028
train: 845, loss: 8.333402256357608E-7
train: 846, loss: 5.538651803897889E-4
train: 847, loss: 0.0033098506090542723
train: 848, loss: 0.008363644064155087
train: 849, loss: 0.00862043866622601
train: 850, loss: 0.10627590099679109
train: 851, loss: 0.06959508756074714
train: 852, loss: 0.027686774327201805
train: 853, loss: 0.006527858339627531
train: 854, loss: 2.1457608532530024E-6
train: 855, loss: 2.6208506206273683E-8
train: 856, loss: 0.47103705152362524
train: 857, loss: 0.002540293039132124
train: 858, loss: 1.2305365538521586E-6
train: 859, loss: 0.012190966488711697
train: 860, loss: 0.024070409522012018
train: 861, loss: 2.740531174182995E-10
train: 862, loss: 6.785742669333424E-7
train: 863, loss: 3.1310646874730174E-6
train: 864, loss: 3.9670812947219545E-11
train: 865, loss: 1.9420132686872383E-12
train: 866, loss: 1.1327435094344403E-10
train: 867, loss: 2.188713930109035E-6
train: 868, loss: 0.011347635600789323
train: 869, loss: 1.4796352062936223E-6
train: 870, loss: 0.004847670530300941
train: 871, loss: 0.005444351759226117
train: 872, loss: 0.002417062773458612
train: 873, loss: 0.005986406084594288
train: 874, loss: 8.699223141269589E-8
train: 875, loss: 1.696512379730729E-10
train: 876, loss: 1.7034865035858987E-10
train: 877, loss: 0.004272683424519405
train: 878, loss: 0.010922337961705638
train: 879, loss: 4.0972665821819614E-13
train: 880, loss: 0.0028897156431669123
train: 881, loss: 1.1770376062830949E-6
train: 882, loss: 5.6522091649761424E-6
train: 883, loss: 0.00332458496859285
train: 884, loss: 3.430805051123902E-5
train: 885, loss: 2.8426344270719484E-9
train: 886, loss: 1.7285017652800857E-5
train: 887, loss: 0.018357724868965867
train: 888, loss: 0.008915443419737247
train: 889, loss: 4.178772366849069E-5
train: 890, loss: 0.004233333972298752
train: 891, loss: 0.003455757487171684
train: 892, loss: 4.681286910385089E-10
train: 893, loss: 1.9649127736964022E-4
train: 894, loss: 0.012208560422865689
train: 895, loss: 0.4909686188825246
train: 896, loss: 0.02739603830204634
train: 897, loss: 2.894012681812775E-13
train: 898, loss: 0.027037920107251142
train: 899, loss: 0.04953601329983338
train: 900, loss: 0.00224668697124162
train: 901, loss: 0.005715637774421644
train: 902, loss: 0.002205462258194282
train: 903, loss: 6.223231390189754E-5
train: 904, loss: 0.0034936704709609668
train: 905, loss: 0.4586908742050979
train: 906, loss: 9.740006595536219E-12
train: 907, loss: 0.0026613168303582903
train: 908, loss: 8.18249474872218E-10
train: 909, loss: 8.073454488150738E-5
train: 910, loss: 0.004539266059205868
train: 911, loss: 0.012774072512728354
train: 912, loss: 0.002619336234168766
train: 913, loss: 0.4997848157577231
train: 914, loss: 0.010823234187450313
train: 915, loss: 0.0026918621985037266
train: 916, loss: 6.336406139402753E-9
train: 917, loss: 0.11161113864126561
train: 918, loss: 3.605884741894041E-4
train: 919, loss: 1.1539668371003594E-6
train: 920, loss: 0.004770611294493258
train: 921, loss: 5.990670648421122E-13
train: 922, loss: 0.001777597452429421
train: 923, loss: 5.635984347727338E-4
train: 924, loss: 3.333913984192112E-6
train: 925, loss: 0.3786647340150085
train: 926, loss: 8.241427876064013E-10
train: 927, loss: 0.0065679920948468
train: 928, loss: 6.382954513119239E-5
train: 929, loss: 0.008476405801513579
train: 930, loss: 8.956408449898432E-7
train: 931, loss: 0.0020821002053041434
train: 932, loss: 2.3715980515281614E-5
train: 933, loss: 0.011214090712069887
train: 934, loss: 0.006428735871707095
train: 935, loss: 0.002478573635246314
train: 936, loss: 9.026664580570617E-12
train: 937, loss: 0.00204052576323248
train: 938, loss: 0.003214202753767309
train: 939, loss: 1.2305275339974268E-5
train: 940, loss: 0.05947434702570555
train: 941, loss: 6.8061081098542594E-6
train: 942, loss: 1.6129084768830227E-7
train: 943, loss: 0.005801634000530363
train: 944, loss: 6.643189694013862E-12
train: 945, loss: 0.015233184554466296
train: 946, loss: 0.0018221988748558218
train: 947, loss: 0.017191860488485214
train: 948, loss: 0.17219918319044364
train: 949, loss: 1.3438056419640154E-8
train: 950, loss: 8.935824099068635E-10
train: 951, loss: 0.007627997686494337
train: 952, loss: 1.4191061391171275E-10
train: 953, loss: 0.00757117771265516
train: 954, loss: 1.8772339304148435E-6
train: 955, loss: 0.16521617661421237
train: 956, loss: 4.3947374288418864E-4
train: 957, loss: 0.0949906405355005
train: 958, loss: 0.005665752153384259
train: 959, loss: 8.514774386346765E-4
train: 960, loss: 1.975724225863069E-6
train: 961, loss: 0.00631657100276158
train: 962, loss: 0.0221465229664412
train: 963, loss: 7.933589768696728E-4
train: 964, loss: 3.2093484870590177E-10
train: 965, loss: 0.0042569500433296045
train: 966, loss: 0.0028564776577761557
train: 967, loss: 0.005818962462594275
train: 968, loss: 0.07868108131504671
train: 969, loss: 1.4587056780310005E-6
train: 970, loss: 1.885208067912019E-11
train: 971, loss: 0.027785536654657106
train: 972, loss: 0.0036325426799697627
train: 973, loss: 7.633528619972296E-8
train: 974, loss: 0.002528525738290818
train: 975, loss: 0.30265794242658905
train: 976, loss: 2.6847083393535648E-11
train: 977, loss: 0.019828716991779145
train: 978, loss: 5.0060193678149656E-12
train: 979, loss: 0.0027646967789838463
train: 980, loss: 8.333407682953832E-6
train: 981, loss: 0.015773715047986112
train: 982, loss: 8.33553269778057E-4
train: 983, loss: 0.0016401421212814944
train: 984, loss: 6.161177778468572E-4
train: 985, loss: 1.0168013649742714E-11
train: 986, loss: 1.183097137416443E-13
train: 987, loss: 0.004200668537559761
train: 988, loss: 0.00482574617941336
train: 989, loss: 0.00412772775900251
train: 990, loss: 0.0037752986738331094
train: 991, loss: 0.003096570990159181
train: 992, loss: 0.006358358897270303
train: 993, loss: 0.004723603040584184
train: 994, loss: 5.3177760920554975E-9
train: 995, loss: 8.420460586337269E-14
train: 996, loss: 0.011051312854193602
train: 997, loss: 7.09943506393762E-10
train: 998, loss: 0.0081440720922014
train: 999, loss: 0.0016750644255484834
train: 1000, loss: 9.942812124157475E-5
trainTime: 6825
==========Finish Train==========
==========Start Predict==========
label: -1.0, output: 0.007470886778748584, predict: 0.0
label: -1.0, output: 0.004660099534907501, predict: 0.0
label: -1.0, output: 0.9207188854287477, predict: 1.0
label: -1.0, output: 0.9064486596625239, predict: 1.0
label: -1.0, output: 0.0228724144213126, predict: 0.0
label: -1.0, output: 0.7226971573913034, predict: 1.0
label: -1.0, output: 0.002489470229063138, predict: 0.0
label: -1.0, output: 0.9099384438362418, predict: 1.0
label: -1.0, output: 0.9800210004811442, predict: 1.0
label: -1.0, output: 0.9653595492333635, predict: 1.0
label: -1.0, output: 0.0011496042013236826, predict: 0.0
label: -1.0, output: 0.9314028142813956, predict: 1.0
label: -1.0, output: 0.9164459613079485, predict: 1.0
label: -1.0, output: 0.9191318753617178, predict: 1.0
label: -1.0, output: 1.180051073763207E-6, predict: 0.0
label: -1.0, output: 0.9109251228373402, predict: 1.0
label: -1.0, output: 0.8075212219911552, predict: 1.0
label: -1.0, output: 4.604407913875235E-5, predict: 0.0
label: -1.0, output: 0.8804288752176999, predict: 1.0
label: -1.0, output: 0.010301451460237367, predict: 0.0
label: -1.0, output: 0.012980961529920669, predict: 0.0
label: -1.0, output: 0.9107750979957415, predict: 1.0
label: -1.0, output: 0.7573187576175645, predict: 1.0
label: -1.0, output: 0.8874164029297168, predict: 1.0
label: -1.0, output: 0.8859346011195522, predict: 1.0
label: -1.0, output: 1.1945117282025425E-5, predict: 0.0
label: -1.0, output: 0.9705915504235272, predict: 1.0
label: -1.0, output: 0.9667408498799656, predict: 1.0
label: -1.0, output: 2.9006504852684405E-4, predict: 0.0
label: -1.0, output: 0.03040512289593173, predict: 0.0
label: -1.0, output: 0.8831341149443734, predict: 1.0
label: -1.0, output: 0.9461773249803916, predict: 1.0
label: -1.0, output: 0.004020939886506472, predict: 0.0
label: -1.0, output: 0.6468380150405147, predict: 1.0
label: -1.0, output: 0.9129486538085014, predict: 1.0
label: -1.0, output: 0.8342969703550434, predict: 1.0
label: -1.0, output: 0.9028083382890483, predict: 1.0
label: -1.0, output: 0.9699616070407091, predict: 1.0
label: -1.0, output: 0.0021231515596531495, predict: 0.0
label: -1.0, output: 0.9224602221092036, predict: 1.0
label: -1.0, output: 0.9178930929381772, predict: 1.0
label: -1.0, output: 0.001572460746557321, predict: 0.0
label: -1.0, output: 0.916270484358627, predict: 1.0
label: -1.0, output: 0.8198714918284964, predict: 1.0
label: -1.0, output: 0.8872665582172242, predict: 1.0
label: -1.0, output: 6.692887173632959E-4, predict: 0.0
label: -1.0, output: 0.9437852702994514, predict: 1.0
label: -1.0, output: 0.004036135137403205, predict: 0.0
label: -1.0, output: 0.035878455450232126, predict: 0.0
label: -1.0, output: 0.11962975571917217, predict: 0.0
label: -1.0, output: 1.4455294065401244E-4, predict: 0.0
label: -1.0, output: 9.627490312721503E-4, predict: 0.0
label: -1.0, output: 0.9434973164534259, predict: 1.0
label: -1.0, output: 0.001139421001366912, predict: 0.0
label: -1.0, output: 7.565714889160824E-5, predict: 0.0
label: -1.0, output: 0.9533530008150689, predict: 1.0
label: -1.0, output: 0.9353131432754861, predict: 1.0
label: -1.0, output: 0.9094075370334782, predict: 1.0
label: -1.0, output: 0.00799283253005592, predict: 0.0
label: -1.0, output: 0.879906491544006, predict: 1.0
label: -1.0, output: 5.426835523528785E-4, predict: 0.0
label: -1.0, output: 0.9222925009651667, predict: 1.0
label: -1.0, output: 0.9719539310592853, predict: 1.0
label: -1.0, output: 0.9981480230302181, predict: 1.0
label: -1.0, output: 0.8075600227350903, predict: 1.0
label: -1.0, output: 0.976685026139908, predict: 1.0
label: -1.0, output: 0.00997069235000275, predict: 0.0
label: -1.0, output: 0.014370913660130633, predict: 0.0
label: -1.0, output: 0.8693895576556004, predict: 1.0
label: -1.0, output: 0.9408281837329455, predict: 1.0
label: -1.0, output: 0.899252228231132, predict: 1.0
label: -1.0, output: 0.8953726237900539, predict: 1.0
label: -1.0, output: 0.8901318448559609, predict: 1.0
label: -1.0, output: 0.016479905767116873, predict: 0.0
label: -1.0, output: 4.396098424433847E-7, predict: 0.0
label: -1.0, output: 0.027770424187439992, predict: 0.0
label: -1.0, output: 0.954053434527197, predict: 1.0
label: -1.0, output: 0.9112591471543026, predict: 1.0
label: -1.0, output: 0.9172749234847062, predict: 1.0
label: -1.0, output: 2.776471795005466E-5, predict: 0.0
label: -1.0, output: 0.9523074355192195, predict: 1.0
label: -1.0, output: 1.420590670842967E-4, predict: 0.0
label: -1.0, output: 0.946305959033628, predict: 1.0
label: -1.0, output: 0.21721245551534601, predict: 0.0
label: -1.0, output: 0.7807254937062451, predict: 1.0
label: -1.0, output: 0.025732471884176236, predict: 0.0
label: -1.0, output: 0.9156499684690238, predict: 1.0
label: -1.0, output: 0.005569059247508318, predict: 0.0
label: -1.0, output: 0.8814381864231193, predict: 1.0
label: -1.0, output: 0.21828254023463084, predict: 0.0
label: -1.0, output: 0.9060002395865594, predict: 1.0
label: -1.0, output: 0.9103968959310962, predict: 1.0
label: -1.0, output: 8.9872206747012E-5, predict: 0.0
label: -1.0, output: 0.9565803051430732, predict: 1.0
label: -1.0, output: 0.006963885868610136, predict: 0.0
label: -1.0, output: 0.9334537301118024, predict: 1.0
label: -1.0, output: 0.9396327142793771, predict: 1.0
label: -1.0, output: 2.595625852509933E-4, predict: 0.0
label: -1.0, output: 0.6149933513832113, predict: 1.0
label: -1.0, output: 1.736472895671217E-5, predict: 0.0
label: -1.0, output: 0.8209285619489105, predict: 1.0
label: -1.0, output: 0.9247478216631313, predict: 1.0
label: -1.0, output: 0.8761089979746898, predict: 1.0
label: -1.0, output: 0.0018107571715499187, predict: 0.0
label: -1.0, output: 0.5408346474551695, predict: 1.0
label: -1.0, output: 0.5384820931235095, predict: 1.0
label: -1.0, output: 0.8595630484601227, predict: 1.0
label: -1.0, output: 0.8137709360093227, predict: 1.0
label: -1.0, output: 0.9150508312593076, predict: 1.0
label: -1.0, output: 0.7587648961071131, predict: 1.0
label: -1.0, output: 0.46282621073011154, predict: 0.0
label: -1.0, output: 0.001428737020573104, predict: 0.0
label: -1.0, output: 0.8598988054737359, predict: 1.0
label: -1.0, output: 0.043020627626318864, predict: 0.0
label: -1.0, output: 0.9418024389085574, predict: 1.0
label: -1.0, output: 0.793369564686717, predict: 1.0
label: -1.0, output: 0.7139244954512299, predict: 1.0
label: -1.0, output: 0.004118801984650034, predict: 0.0
label: -1.0, output: 0.4201783814380471, predict: 0.0
label: -1.0, output: 0.7846462345769228, predict: 1.0
label: -1.0, output: 0.887154539589481, predict: 1.0
label: -1.0, output: 0.0011858804952051233, predict: 0.0
label: -1.0, output: 0.0019498015683371292, predict: 0.0
label: -1.0, output: 4.3864618925443246E-7, predict: 0.0
label: -1.0, output: 4.0204110981659965E-7, predict: 0.0
label: -1.0, output: 0.8970507555581843, predict: 1.0
label: -1.0, output: 0.0012513791071474595, predict: 0.0
label: -1.0, output: 3.683802289323166E-6, predict: 0.0
label: -1.0, output: 0.3936818630025781, predict: 0.0
label: -1.0, output: 0.9443090098516669, predict: 1.0
label: -1.0, output: 1.9790334339254664E-4, predict: 0.0
label: -1.0, output: 0.9487099432262813, predict: 1.0
label: -1.0, output: 0.9165279308325571, predict: 1.0
label: -1.0, output: 6.511628878569244E-7, predict: 0.0
label: -1.0, output: 0.5989386951707999, predict: 1.0
label: -1.0, output: 0.8349131944793818, predict: 1.0
label: -1.0, output: 0.8994780598452434, predict: 1.0
label: -1.0, output: 0.9379124270502178, predict: 1.0
label: -1.0, output: 0.5597349938279659, predict: 1.0
label: -1.0, output: 1.9002831734102983E-5, predict: 0.0
label: -1.0, output: 0.9498675812841408, predict: 1.0
label: -1.0, output: 0.9303044713679455, predict: 1.0
label: -1.0, output: 0.931433960681447, predict: 1.0
label: -1.0, output: 0.9338637521864578, predict: 1.0
label: -1.0, output: 1.1946176012189963E-4, predict: 0.0
label: -1.0, output: 0.004788611492505874, predict: 0.0
label: -1.0, output: 0.9274210735962196, predict: 1.0
label: -1.0, output: 0.9980527767550088, predict: 1.0
label: -1.0, output: 1.982487441076672E-5, predict: 0.0
label: -1.0, output: 0.14849591411072904, predict: 0.0
label: -1.0, output: 2.913611897213764E-4, predict: 0.0
label: -1.0, output: 0.0011414556593020173, predict: 0.0
label: -1.0, output: 6.521795371567329E-6, predict: 0.0
label: -1.0, output: 0.8694106499305133, predict: 1.0
label: -1.0, output: 4.5466580958725345E-4, predict: 0.0
label: -1.0, output: 7.715885897398778E-7, predict: 0.0
label: -1.0, output: 0.9994770862911357, predict: 1.0
label: -1.0, output: 0.9315263681178451, predict: 1.0
label: -1.0, output: 0.02440368823105856, predict: 0.0
label: -1.0, output: 0.013567171625462511, predict: 0.0
label: -1.0, output: 0.9290596925373815, predict: 1.0
label: -1.0, output: 0.9486895882170338, predict: 1.0
label: -1.0, output: 0.9278066098194555, predict: 1.0
label: -1.0, output: 1.1186812492396367E-5, predict: 0.0
label: -1.0, output: 0.014953209255885266, predict: 0.0
label: -1.0, output: 0.003082701180119549, predict: 0.0
label: -1.0, output: 0.9245142840172066, predict: 1.0
label: -1.0, output: 0.929451780332303, predict: 1.0
label: -1.0, output: 2.7699611167973984E-5, predict: 0.0
label: -1.0, output: 0.9414093615111034, predict: 1.0
label: -1.0, output: 0.0408909582877909, predict: 0.0
label: -1.0, output: 1.0141735201786602E-4, predict: 0.0
label: -1.0, output: 0.04513802332342857, predict: 0.0
label: -1.0, output: 0.9810656934862946, predict: 1.0
label: -1.0, output: 0.9952196759212015, predict: 1.0
label: -1.0, output: 9.992161311838509E-6, predict: 0.0
label: -1.0, output: 0.3114591057386805, predict: 0.0
label: -1.0, output: 0.9458451296684178, predict: 1.0
label: -1.0, output: 0.0013519728350136686, predict: 0.0
label: -1.0, output: 0.9470412748006958, predict: 1.0
label: -1.0, output: 2.8779887028064996E-6, predict: 0.0
label: -1.0, output: 0.48451853754325613, predict: 0.0
label: -1.0, output: 0.9253490777893154, predict: 1.0
label: -1.0, output: 0.9041425452391627, predict: 1.0
label: -1.0, output: 0.8210636779856283, predict: 1.0
label: -1.0, output: 1.6159053386604635E-5, predict: 0.0
label: -1.0, output: 0.08980569929877115, predict: 0.0
label: -1.0, output: 0.008849698567916453, predict: 0.0
label: -1.0, output: 0.9100178302942584, predict: 1.0
label: -1.0, output: 0.36910899118041635, predict: 0.0
label: -1.0, output: 0.9699554545443373, predict: 1.0
label: -1.0, output: 0.7819422401816919, predict: 1.0
label: -1.0, output: 0.8980525181567731, predict: 1.0
label: -1.0, output: 4.8794637260622794E-4, predict: 0.0
label: -1.0, output: 0.916231474068479, predict: 1.0
label: -1.0, output: 0.002865382398312472, predict: 0.0
label: -1.0, output: 0.9335946311465215, predict: 1.0
label: -1.0, output: 0.21078897574695402, predict: 0.0
label: -1.0, output: 1.5573570619788137E-5, predict: 0.0
label: -1.0, output: 0.9973711729902286, predict: 1.0
label: -1.0, output: 0.0015420160295210536, predict: 0.0
label: -1.0, output: 1.1915488945191435E-5, predict: 0.0
label: -1.0, output: 5.062136800876724E-6, predict: 0.0
label: -1.0, output: 3.2745074582109564E-5, predict: 0.0
label: -1.0, output: 7.57563488990092E-5, predict: 0.0
label: -1.0, output: 0.8803097006130469, predict: 1.0
label: -1.0, output: 0.05649061912742987, predict: 0.0
label: -1.0, output: 0.0014516863241641493, predict: 0.0
label: -1.0, output: 0.9535239264134396, predict: 1.0
label: -1.0, output: 0.7058394273062631, predict: 1.0
label: -1.0, output: 0.9204194089415587, predict: 1.0
label: -1.0, output: 0.9100995022433762, predict: 1.0
label: -1.0, output: 0.13545366728992747, predict: 0.0
label: -1.0, output: 0.001912244286908095, predict: 0.0
label: -1.0, output: 0.9325079288728664, predict: 1.0
label: -1.0, output: 0.0037330907830313447, predict: 0.0
label: -1.0, output: 0.988406980739291, predict: 1.0
label: -1.0, output: 0.9983658263108011, predict: 1.0
label: -1.0, output: 0.034917824042825896, predict: 0.0
label: -1.0, output: 0.8949846617580921, predict: 1.0
label: -1.0, output: 0.004131151048912332, predict: 0.0
label: -1.0, output: 4.003980246923454E-6, predict: 0.0
label: -1.0, output: 0.8019062956996967, predict: 1.0
label: -1.0, output: 0.9119918310038574, predict: 1.0
label: -1.0, output: 5.2678022126828554E-5, predict: 0.0
label: -1.0, output: 0.9144013029453705, predict: 1.0
label: -1.0, output: 0.007112451768267052, predict: 0.0
label: -1.0, output: 0.02646445598542932, predict: 0.0
label: -1.0, output: 0.8973394520411822, predict: 1.0
label: -1.0, output: 0.8211465828411866, predict: 1.0
label: -1.0, output: 0.8960304113501389, predict: 1.0
label: -1.0, output: 4.952047754700939E-7, predict: 0.0
label: -1.0, output: 0.0011353039749812325, predict: 0.0
label: -1.0, output: 7.331843014267338E-6, predict: 0.0
label: -1.0, output: 0.8805140651046836, predict: 1.0
label: -1.0, output: 4.822264468463254E-5, predict: 0.0
label: -1.0, output: 2.7081222064787444E-5, predict: 0.0
label: -1.0, output: 0.2514863643297173, predict: 0.0
label: -1.0, output: 0.004316659616194144, predict: 0.0
label: -1.0, output: 0.9993156705555373, predict: 1.0
label: -1.0, output: 0.9144775498363522, predict: 1.0
label: -1.0, output: 2.806287066968359E-5, predict: 0.0
label: -1.0, output: 0.34416839676840183, predict: 0.0
label: -1.0, output: 0.8794690326369714, predict: 1.0
label: -1.0, output: 0.42645550558165357, predict: 0.0
label: -1.0, output: 0.9501533909046751, predict: 1.0
label: -1.0, output: 2.920444875404549E-6, predict: 0.0
label: -1.0, output: 0.8724042024941439, predict: 1.0
label: -1.0, output: 0.0016947334279738002, predict: 0.0
label: -1.0, output: 0.9351915843216612, predict: 1.0
label: -1.0, output: 0.9996421185671731, predict: 1.0
label: -1.0, output: 0.9382540830240957, predict: 1.0
label: -1.0, output: 0.0359900068154551, predict: 0.0
label: -1.0, output: 0.8357361125117654, predict: 1.0
label: -1.0, output: 0.9203794438115894, predict: 1.0
label: -1.0, output: 9.832126340766983E-7, predict: 0.0
label: -1.0, output: 4.93661287721891E-4, predict: 0.0
label: -1.0, output: 0.00106185227646159, predict: 0.0
label: -1.0, output: 0.01582540405497371, predict: 0.0
label: -1.0, output: 3.0196708738657982E-5, predict: 0.0
label: -1.0, output: 0.6911482736888243, predict: 1.0
label: -1.0, output: 0.924501867143, predict: 1.0
label: -1.0, output: 0.8784442257325099, predict: 1.0
label: -1.0, output: 0.9268983910923537, predict: 1.0
label: -1.0, output: 0.03217882380703117, predict: 0.0
label: -1.0, output: 0.92261449383512, predict: 1.0
label: -1.0, output: 3.5768140360615464E-7, predict: 0.0
label: -1.0, output: 1.8255265348539772E-5, predict: 0.0
label: -1.0, output: 0.007452879628879706, predict: 0.0
label: -1.0, output: 0.9396950327001156, predict: 1.0
label: -1.0, output: 0.002409906571784441, predict: 0.0
label: -1.0, output: 6.762488336903153E-7, predict: 0.0
label: -1.0, output: 1.4790318974491776E-5, predict: 0.0
label: -1.0, output: 0.440593254972852, predict: 0.0
label: -1.0, output: 0.9750008765361146, predict: 1.0
label: -1.0, output: 0.054700462223318244, predict: 0.0
label: -1.0, output: 4.437551224911755E-5, predict: 0.0
label: -1.0, output: 0.0026066346852401255, predict: 0.0
label: -1.0, output: 3.1743666363072724E-6, predict: 0.0
label: -1.0, output: 7.589601232068264E-4, predict: 0.0
label: -1.0, output: 2.11475584290147E-4, predict: 0.0
label: -1.0, output: 0.8044413694900807, predict: 1.0
label: -1.0, output: 0.9088392356948959, predict: 1.0
label: -1.0, output: 0.8701628470247246, predict: 1.0
label: -1.0, output: 0.15662149538694428, predict: 0.0
label: -1.0, output: 0.0017753520699396905, predict: 0.0
label: -1.0, output: 0.1783452097504117, predict: 0.0
label: -1.0, output: 9.423615306275068E-5, predict: 0.0
label: -1.0, output: 0.0011415278027395778, predict: 0.0
label: -1.0, output: 0.8134934496793361, predict: 1.0
label: -1.0, output: 0.9953482806194897, predict: 1.0
label: -1.0, output: 0.8130369160266923, predict: 1.0
label: -1.0, output: 0.1113280310099732, predict: 0.0
label: -1.0, output: 6.607150307781187E-4, predict: 0.0
label: -1.0, output: 0.891516650285557, predict: 1.0
label: -1.0, output: 5.429960949962691E-5, predict: 0.0
label: -1.0, output: 0.8981114084979956, predict: 1.0
label: -1.0, output: 6.482889675955987E-6, predict: 0.0
label: -1.0, output: 0.9612545290831075, predict: 1.0
label: -1.0, output: 0.6970706746495012, predict: 1.0
label: -1.0, output: 0.9363414034090104, predict: 1.0
label: -1.0, output: 0.8899429139317229, predict: 1.0
label: -1.0, output: 0.0025805510768014384, predict: 0.0
label: -1.0, output: 0.27244157262456203, predict: 0.0
label: -1.0, output: 0.9356465357381698, predict: 1.0
label: -1.0, output: 1.0302836956038492E-5, predict: 0.0
label: -1.0, output: 0.9388916464895828, predict: 1.0
label: -1.0, output: 0.8825009577326718, predict: 1.0
label: -1.0, output: 3.028913684268931E-4, predict: 0.0
label: -1.0, output: 5.571800166986784E-5, predict: 0.0
label: -1.0, output: 0.0016110324994538397, predict: 0.0
label: -1.0, output: 0.9467822972584766, predict: 1.0
label: -1.0, output: 0.0906233070666832, predict: 0.0
label: -1.0, output: 0.949924819891868, predict: 1.0
label: -1.0, output: 4.5340454992959364E-4, predict: 0.0
label: -1.0, output: 0.946057224677088, predict: 1.0
label: -1.0, output: 0.8880955677820274, predict: 1.0
label: -1.0, output: 0.0020796420183278215, predict: 0.0
label: -1.0, output: 0.8780242736905909, predict: 1.0
label: -1.0, output: 3.917779231646005E-7, predict: 0.0
label: -1.0, output: 3.1209854301437285E-6, predict: 0.0
label: -1.0, output: 0.002125992898106915, predict: 0.0
label: -1.0, output: 0.9267220869327291, predict: 1.0
label: -1.0, output: 0.9320428696223916, predict: 1.0
label: -1.0, output: 0.9429872565223686, predict: 1.0
label: -1.0, output: 0.9324679733841381, predict: 1.0
label: -1.0, output: 7.439109552782012E-5, predict: 0.0
label: -1.0, output: 0.7606523383131649, predict: 1.0
label: -1.0, output: 0.8021526846367806, predict: 1.0
label: -1.0, output: 4.332025965763822E-7, predict: 0.0
label: -1.0, output: 0.8977005222186429, predict: 1.0
label: -1.0, output: 0.9734352850103097, predict: 1.0
label: -1.0, output: 0.8124290112264966, predict: 1.0
label: -1.0, output: 0.9188374948114263, predict: 1.0
label: -1.0, output: 1.2324413038703813E-5, predict: 0.0
label: -1.0, output: 0.8891286104548225, predict: 1.0
label: -1.0, output: 0.9066606479050835, predict: 1.0
label: -1.0, output: 4.328132337170484E-6, predict: 0.0
label: -1.0, output: 0.01073339707887318, predict: 0.0
label: -1.0, output: 0.4555523912505331, predict: 0.0
label: -1.0, output: 6.690926533584006E-5, predict: 0.0
label: -1.0, output: 5.126662572736463E-4, predict: 0.0
label: -1.0, output: 0.870059973310976, predict: 1.0
label: -1.0, output: 0.9124737983073219, predict: 1.0
label: -1.0, output: 0.4800273138590178, predict: 0.0
label: -1.0, output: 0.9193244812465641, predict: 1.0
label: -1.0, output: 0.14609163169315562, predict: 0.0
label: -1.0, output: 0.9143813022009063, predict: 1.0
label: -1.0, output: 0.020564021186261265, predict: 0.0
label: -1.0, output: 0.0011531917360732568, predict: 0.0
label: -1.0, output: 2.668401309268782E-5, predict: 0.0
label: -1.0, output: 0.8803692629340029, predict: 1.0
label: -1.0, output: 0.8945813385992603, predict: 1.0
label: -1.0, output: 4.478884066485242E-5, predict: 0.0
label: -1.0, output: 5.778020393616518E-4, predict: 0.0
label: -1.0, output: 0.5236106158071592, predict: 1.0
label: -1.0, output: 0.05699978713768843, predict: 0.0
label: -1.0, output: 0.9225456491681991, predict: 1.0
label: -1.0, output: 0.722171635135266, predict: 1.0
label: -1.0, output: 0.025697426531135825, predict: 0.0
label: -1.0, output: 0.8909534697668989, predict: 1.0
label: -1.0, output: 0.0010559150677697641, predict: 0.0
label: -1.0, output: 0.9447434088550051, predict: 1.0
label: -1.0, output: 0.8241016507799871, predict: 1.0
label: -1.0, output: 4.4478091672100535E-4, predict: 0.0
label: -1.0, output: 0.891910083475463, predict: 1.0
label: -1.0, output: 0.005093028692695223, predict: 0.0
label: -1.0, output: 0.0014660652850438714, predict: 0.0
label: -1.0, output: 0.0011838210633270017, predict: 0.0
label: -1.0, output: 0.8833742082338661, predict: 1.0
label: -1.0, output: 1.0183284892788148E-4, predict: 0.0
label: -1.0, output: 3.322158891868092E-6, predict: 0.0
label: -1.0, output: 5.4997474873480106E-5, predict: 0.0
label: -1.0, output: 7.294368006742164E-4, predict: 0.0
label: -1.0, output: 0.18891410749066573, predict: 0.0
label: -1.0, output: 0.9301967442779595, predict: 1.0
label: -1.0, output: 0.8524520850413865, predict: 1.0
label: -1.0, output: 0.7591836687123329, predict: 1.0
label: -1.0, output: 0.9295964713697003, predict: 1.0
label: -1.0, output: 5.407339235716241E-4, predict: 0.0
label: -1.0, output: 0.7823062067043544, predict: 1.0
label: -1.0, output: 0.9667921609594358, predict: 1.0
label: -1.0, output: 0.006517362971607967, predict: 0.0
label: -1.0, output: 0.9772197852063351, predict: 1.0
label: -1.0, output: 0.15643005610258479, predict: 0.0
label: -1.0, output: 0.8883281611066058, predict: 1.0
label: -1.0, output: 0.010226084980943309, predict: 0.0
label: -1.0, output: 0.7546440548964163, predict: 1.0
label: -1.0, output: 0.896709675843604, predict: 1.0
label: -1.0, output: 0.9384051523546876, predict: 1.0
label: -1.0, output: 1.6736851056792413E-6, predict: 0.0
label: -1.0, output: 0.9439035215095996, predict: 1.0
label: -1.0, output: 0.012959323689879792, predict: 0.0
label: -1.0, output: 0.8896018520343375, predict: 1.0
label: -1.0, output: 0.8958106478208931, predict: 1.0
label: -1.0, output: 0.005037075090831086, predict: 0.0
label: -1.0, output: 8.942984984939237E-5, predict: 0.0
label: -1.0, output: 0.9239950585778747, predict: 1.0
label: -1.0, output: 0.04206106449987473, predict: 0.0
label: -1.0, output: 0.0013776633858167955, predict: 0.0
label: -1.0, output: 0.914897768762751, predict: 1.0
label: -1.0, output: 6.24889422372475E-7, predict: 0.0
label: -1.0, output: 0.8977973751561014, predict: 1.0
label: -1.0, output: 0.8901632643160072, predict: 1.0
label: -1.0, output: 2.223040727710922E-5, predict: 0.0
label: -1.0, output: 0.38580680713307697, predict: 0.0
label: -1.0, output: 1.9503013001640726E-4, predict: 0.0
label: -1.0, output: 0.7675403061922801, predict: 1.0
label: -1.0, output: 0.001138411996568746, predict: 0.0
label: -1.0, output: 3.385098964209844E-6, predict: 0.0
label: -1.0, output: 0.036229554174581326, predict: 0.0
label: -1.0, output: 0.9210609549968501, predict: 1.0
label: -1.0, output: 0.001715629082713805, predict: 0.0
label: -1.0, output: 0.998957562608925, predict: 1.0
label: -1.0, output: 3.095282232724414E-5, predict: 0.0
label: -1.0, output: 0.0010731307917273057, predict: 0.0
label: -1.0, output: 0.9158041359332865, predict: 1.0
label: -1.0, output: 0.9053587475671183, predict: 1.0
label: -1.0, output: 0.0034275313140481184, predict: 0.0
label: -1.0, output: 0.934886273258817, predict: 1.0
label: -1.0, output: 0.907583145420492, predict: 1.0
label: -1.0, output: 0.6167235664449912, predict: 1.0
label: -1.0, output: 0.8742215731778087, predict: 1.0
label: -1.0, output: 0.9149321321361285, predict: 1.0
label: -1.0, output: 0.8583833455711729, predict: 1.0
label: -1.0, output: 1.3729876562264355E-5, predict: 0.0
label: -1.0, output: 0.0013601260046064091, predict: 0.0
label: -1.0, output: 1.2014101450078005E-4, predict: 0.0
label: -1.0, output: 0.006364205635612008, predict: 0.0
label: -1.0, output: 0.9144145375788136, predict: 1.0
label: -1.0, output: 2.2989217759445973E-5, predict: 0.0
label: -1.0, output: 0.001214521741206761, predict: 0.0
label: -1.0, output: 3.256231879213681E-6, predict: 0.0
label: -1.0, output: 0.5096127844828053, predict: 1.0
label: -1.0, output: 0.24327544710413784, predict: 0.0
label: -1.0, output: 0.9495909102771782, predict: 1.0
label: -1.0, output: 0.28197324831901727, predict: 0.0
label: -1.0, output: 9.821400461764447E-4, predict: 0.0
label: -1.0, output: 0.9228632192774937, predict: 1.0
label: -1.0, output: 0.9996568003284361, predict: 1.0
label: -1.0, output: 0.918290525261542, predict: 1.0
label: -1.0, output: 0.0014515946176309196, predict: 0.0
label: -1.0, output: 3.5084990652649214E-4, predict: 0.0
label: -1.0, output: 0.7516572069950441, predict: 1.0
label: -1.0, output: 7.1354714066637515E-6, predict: 0.0
label: -1.0, output: 0.9978232254852698, predict: 1.0
label: -1.0, output: 1.8020094005103017E-4, predict: 0.0
label: -1.0, output: 0.006322138593553192, predict: 0.0
label: -1.0, output: 0.4347680325825797, predict: 0.0
label: -1.0, output: 0.011475351320226515, predict: 0.0
label: -1.0, output: 7.299306505276208E-5, predict: 0.0
label: -1.0, output: 0.8161157728130035, predict: 1.0
label: -1.0, output: 0.9190118121107621, predict: 1.0
label: -1.0, output: 0.0012117609557357471, predict: 0.0
label: -1.0, output: 1.812637542644514E-5, predict: 0.0
label: -1.0, output: 0.006850844742037873, predict: 0.0
label: -1.0, output: 0.0013026488894407012, predict: 0.0
label: -1.0, output: 3.1635168380605135E-6, predict: 0.0
label: -1.0, output: 0.9909703839505354, predict: 1.0
label: -1.0, output: 0.0011892475472620365, predict: 0.0
label: -1.0, output: 0.0026950156126724443, predict: 0.0
label: -1.0, output: 0.9094354522688949, predict: 1.0
label: -1.0, output: 3.711129887025234E-5, predict: 0.0
label: -1.0, output: 3.7688804100314617E-7, predict: 0.0
label: -1.0, output: 0.6184048581527701, predict: 1.0
label: -1.0, output: 0.033893743521800884, predict: 0.0
label: -1.0, output: 0.001160277330487509, predict: 0.0
label: -1.0, output: 0.002774381953240022, predict: 0.0
label: -1.0, output: 0.020347035698807657, predict: 0.0
label: -1.0, output: 0.5359118011092007, predict: 1.0
label: -1.0, output: 0.6299096163044311, predict: 1.0
label: -1.0, output: 0.830801152444855, predict: 1.0
label: -1.0, output: 0.009587888780829031, predict: 0.0
label: -1.0, output: 0.006646737795650468, predict: 0.0
label: -1.0, output: 4.2841022624349415E-4, predict: 0.0
label: -1.0, output: 0.7128376848151188, predict: 1.0
label: -1.0, output: 5.732327958357628E-5, predict: 0.0
label: -1.0, output: 0.12354015660492626, predict: 0.0
label: -1.0, output: 0.9212913614041535, predict: 1.0
label: -1.0, output: 0.9160466280002071, predict: 1.0
label: -1.0, output: 1.995039274111823E-6, predict: 0.0
label: -1.0, output: 0.8910299067398627, predict: 1.0
label: -1.0, output: 0.02170247729298121, predict: 0.0
label: -1.0, output: 0.8741041152945729, predict: 1.0
label: -1.0, output: 0.05710993129873801, predict: 0.0
label: -1.0, output: 0.0020917946864268044, predict: 0.0
label: -1.0, output: 0.7370759337168203, predict: 1.0
label: -1.0, output: 0.8884868836681747, predict: 1.0
label: -1.0, output: 0.9298793795465089, predict: 1.0
label: -1.0, output: 0.7697787022130738, predict: 1.0
label: -1.0, output: 2.7493885850874615E-4, predict: 0.0
label: -1.0, output: 0.8353428586837682, predict: 1.0
label: -1.0, output: 0.01127172554315256, predict: 0.0
label: -1.0, output: 0.002703366627444006, predict: 0.0
label: -1.0, output: 5.143618910801646E-7, predict: 0.0
label: -1.0, output: 0.27825571772005503, predict: 0.0
label: -1.0, output: 7.912404438122629E-4, predict: 0.0
label: -1.0, output: 0.8975325310869777, predict: 1.0
label: -1.0, output: 0.8082376842960054, predict: 1.0
label: -1.0, output: 0.029842291824302813, predict: 0.0
total: 0.0, correct: 0.0, wrong: 0.0, accurate: 0.0
predictTime: 148
==========Finish Predict==========
==========Start WritePredict==========
writePredictTime: 7692
==========Finish WritePredict==========
==========train 0==========
train: 0, loss: -1.0
layerType: input, nodeSize: 5
double[][] w: null
double[] b: null
double[][] partialDerivative: null
double[] z: null
double[] h: 
[-1.3268246890372462, 3.605889708042523, -4.784383490576127, -0.9703862477206422, -0.44779438281765926]
layerType: hidden, nodeSize: 8
double[][] w: 
[-1.2094063884838453, -0.7901885735956293, -1.9653806955130162, 3.047724116847093, -3.182794944504799]
[2.7998332868967015, -1.7564247029045692, 0.6297811380642052, -0.6617620689498456, -7.971858965166544]
[1.821515241399656, -2.222678449739877, -0.6035074518374703, 0.4859715317588756, -7.056768635731951]
[1.4110616030602545, 1.4146499340349683, -0.4067981809409435, 4.738170307556723, 1.924712809299109]
[-1.77140849117444, -0.8278719063678966, -1.1002605982790146, -4.747968312275428, -2.01819342614915]
[1.798660544806906, -1.708774459012038, -0.6806685740324447, 4.793640744560229, -5.022320283234619]
[-0.7275477886040475, 0.9552542786369584, -2.6182453814344853, -8.966019272633407, 8.07236953897941]
[-1.7281124768800522, 1.9865006413548125, -0.8663260862368639, -6.0069808513935135, 5.763954715570546]
double[] b: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[][] partialDerivative: 
[-2.294788576559179E-7, -2.294788576559179E-7, -2.294788576559179E-7, -2.294788576559179E-7, -2.294788576559179E-7]
[-6.743580777726128E-5, -6.743580777726128E-5, -6.743580777726128E-5, -6.743580777726128E-5, -6.743580777726128E-5]
[5.334300690760907E-4, 5.334300690760907E-4, 5.334300690760907E-4, 5.334300690760907E-4, 5.334300690760907E-4]
[-5.4163169519850346E-9, -5.4163169519850346E-9, -5.4163169519850346E-9, -5.4163169519850346E-9, -5.4163169519850346E-9]
[-1.713616466020593E-5, -1.713616466020593E-5, -1.713616466020593E-5, -1.713616466020593E-5, -1.713616466020593E-5]
[8.115563677624158E-6, 8.115563677624158E-6, 8.115563677624158E-6, 8.115563677624158E-6, 8.115563677624158E-6]
[-1.3164562620326034E-4, -1.3164562620326034E-4, -1.3164562620326034E-4, -1.3164562620326034E-4, -1.3164562620326034E-4]
[7.24640412609856E-4, 7.24640412609856E-4, 7.24640412609856E-4, 7.24640412609856E-4, 7.24640412609856E-4]
double[] z: 
[6.626240490662692, -8.849557692840444, -4.85575238621872, -0.284612124535511, 10.140301209963031, -7.694296208500527, 22.022299980865576, 16.848865805951682]
double[] h: 
[0.9986766180677961, 1.434245955068019E-4, 0.0077233603881346185, 0.4293234168578164, 0.9999605446405676, 4.5521008412449356E-4, 0.9999999997272049, 0.9999999518462036]
layerType: output, nodeSize: 1
double[][] w: 
[-4.414891132941936, -9.228255735668164, 10.885888010466031, -5.689065400864871, -5.401649849324492, 9.947886636582211, -8.983980024280433, 17.668102616718443]
double[] b: 
[0.0]
double[][] partialDerivative: 
[1.960520433977616E-4, 1.960520433977616E-4, 1.960520433977616E-4, 1.960520433977616E-4, 1.960520433977616E-4, 1.960520433977616E-4, 1.960520433977616E-4, 1.960520433977616E-4]
double[] z: 
[-3.4815320678801385]
double[] h: 
[0.029842291824302813]
==============================
programTotalTime: 7694
==========Start ReadInput==========
readInputTime: 584
==========Finish ReadInput==========
==========Start BuildNetwork==========
buildNetworkTime: 10
==========Finish BuildNetwork==========
==========train 0==========
train: 0, loss: -1.0
layerType: input, nodeSize: 5
double[][] w: null
double[] b: null
double[][] partialDerivative: null
double[] z: null
double[] h: 
[0.0, 0.0, 0.0, 0.0, 0.0]
layerType: hidden, nodeSize: 8
double[][] w: 
[-0.38952923914918336, -0.9328469877819165, -0.18626340303635147, 0.2837612522771664, -0.20620755502454702]
[0.6303699625055694, 0.12077279866268742, 0.41233599344449323, -0.8752742765821597, -0.05460615255159307]
[-0.07223067938417582, 0.5142063290317924, 0.46188062361405735, -0.7711072429036845, 0.2845426361927137]
[0.3613414370038206, -0.29300213900733807, 0.9649834668439841, -0.965468053311554, -0.757825777866505]
[-0.13175716835310514, -0.04127600684347543, -0.2524064915186133, -0.7669789548285004, -0.23160100481962287]
[0.046422727913939754, -0.9312794691181792, 0.9122367967023293, 0.6436269224735363, -0.8360031168638025]
[0.26839514963707223, 0.42244042574328944, -0.6508406783418699, -0.1015586259891037, -0.7177699843820553]
[0.4912230085707596, 0.921503791080782, -0.8662026663136273, -0.40798751182917714, -0.21167051284672622]
double[] b: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[][] partialDerivative: 
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
[0.0, 0.0, 0.0, 0.0, 0.0]
double[] z: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[] h: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
layerType: output, nodeSize: 1
double[][] w: 
[-0.871635297447606, 0.6850416616939279, -0.11736405346775003, -0.6124003342375393, -0.4389131206844985, 0.6348186526625488, 0.19689707993815753, -0.3445083764510515]
double[] b: 
[0.0]
double[][] partialDerivative: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[] z: 
[0.0]
double[] h: 
[0.0]
==============================
==========Start Train==========
train: 1, loss: 0.1369248630489926
train: 2, loss: 0.12382570499053877
train: 3, loss: 0.12529279618557432
train: 4, loss: 0.07782731577490974
train: 5, loss: 0.09457056334020593
train: 6, loss: 0.1465430898594974
train: 7, loss: 0.11370998082148782
train: 8, loss: 0.08486191637953387
train: 9, loss: 0.09365275531062947
train: 10, loss: 0.03970464123411537
train: 11, loss: 0.07864099948868707
train: 12, loss: 0.15368601007969956
train: 13, loss: 0.15238618503666346
train: 14, loss: 0.13951630718293823
train: 15, loss: 0.2194753268842132
train: 16, loss: 0.1626308401100087
train: 17, loss: 0.09907008359945133
train: 18, loss: 0.10217652359628056
train: 19, loss: 0.06828385521118142
train: 20, loss: 0.04685479477427487
train: 21, loss: 0.04134619035724196
train: 22, loss: 0.06757515557961723
train: 23, loss: 0.1083385469637374
train: 24, loss: 0.07531053888754412
train: 25, loss: 0.018722245710277603
train: 26, loss: 0.08870231974912661
train: 27, loss: 0.0398385094374421
train: 28, loss: 0.02055389131396039
train: 29, loss: 0.10182959793935846
train: 30, loss: 0.20103206706833737
train: 31, loss: 0.017315940232314364
train: 32, loss: 0.08491525699862518
train: 33, loss: 0.1327106017738262
train: 34, loss: 0.17968253315221175
train: 35, loss: 0.003934090585563247
train: 36, loss: 0.026028784245052044
train: 37, loss: 0.015632723298732068
train: 38, loss: 0.014207546905996593
train: 39, loss: 0.026773417458225483
train: 40, loss: 0.06277852925477172
train: 41, loss: 0.04115400604761955
train: 42, loss: 0.010129711504158027
train: 43, loss: 0.017199058960945496
train: 44, loss: 0.009633183053987695
train: 45, loss: 0.01258443227522471
train: 46, loss: 0.06663553433304303
train: 47, loss: 0.026249123941958295
train: 48, loss: 0.009145286369073465
train: 49, loss: 0.006376124170727108
train: 50, loss: 9.755754463888875E-4
train: 51, loss: 0.0039689355195939155
train: 52, loss: 0.09634579200648248
train: 53, loss: 0.0172053918651571
train: 54, loss: 0.004194675787508751
train: 55, loss: 0.04825858714452211
train: 56, loss: 0.043365987789408415
train: 57, loss: 0.005989536501992302
train: 58, loss: 0.006687046656778433
train: 59, loss: 0.06031668321117096
train: 60, loss: 0.012041621219884065
train: 61, loss: 0.010305358095488165
train: 62, loss: 0.03261086485540312
train: 63, loss: 0.002757404077426077
train: 64, loss: 0.04086320916914474
train: 65, loss: 0.016382112983059246
train: 66, loss: 0.005721761153562235
train: 67, loss: 0.05984404051465726
train: 68, loss: 0.005614450520452528
train: 69, loss: 0.04536799619595125
train: 70, loss: 0.010660030969775747
train: 71, loss: 0.013695933121382257
train: 72, loss: 0.010891839646863822
train: 73, loss: 0.02522017214046424
train: 74, loss: 2.621241093195147E-4
train: 75, loss: 0.015276708042434512
train: 76, loss: 0.05205289929912989
train: 77, loss: 0.13872175368603698
train: 78, loss: 0.002692735504358662
train: 79, loss: 0.020051509299517656
train: 80, loss: 0.020870044186002298
train: 81, loss: 0.05694160417194618
train: 82, loss: 0.0033447394684596754
train: 83, loss: 0.0032807104061477123
train: 84, loss: 1.2311316779253414E-5
train: 85, loss: 0.002147925863501986
train: 86, loss: 0.001928702707660841
train: 87, loss: 0.034670497229420554
train: 88, loss: 0.02808799609014838
train: 89, loss: 0.012915259326224006
train: 90, loss: 0.03417026383931465
train: 91, loss: 2.0118594621253325E-5
train: 92, loss: 0.0011820544854022315
train: 93, loss: 0.07295984386215222
train: 94, loss: 0.009032246648972404
train: 95, loss: 0.002123996888570921
train: 96, loss: 0.0015022144972443542
train: 97, loss: 0.0012148092890593236
train: 98, loss: 0.008167574617483122
train: 99, loss: 0.009155201090752116
train: 100, loss: 0.005738752773727893
train: 101, loss: 0.0010303013691665692
train: 102, loss: 0.0018885147943848882
train: 103, loss: 0.04552863023331285
train: 104, loss: 0.0016508749276610052
train: 105, loss: 0.004974860824410044
train: 106, loss: 0.009883741736711625
train: 107, loss: 0.0049856520761537085
train: 108, loss: 3.0138032173207085E-4
train: 109, loss: 1.539136293303646E-6
train: 110, loss: 0.001568582676482705
train: 111, loss: 3.736839528585923E-6
train: 112, loss: 0.004821551028855955
train: 113, loss: 1.5216242123548284E-6
train: 114, loss: 0.006938364447902834
train: 115, loss: 0.01256821573135201
train: 116, loss: 6.451163449916784E-4
train: 117, loss: 0.05048568725439563
train: 118, loss: 8.280100249226686E-4
train: 119, loss: 5.224622742446014E-4
train: 120, loss: 0.04633803792479469
train: 121, loss: 0.42958193768371616
train: 122, loss: 0.004203188294859558
train: 123, loss: 0.0022945585992031094
train: 124, loss: 0.0048717078707549225
train: 125, loss: 0.003294692454935226
train: 126, loss: 0.026994504498414514
train: 127, loss: 0.011360075390873709
train: 128, loss: 0.024512668202217553
train: 129, loss: 1.3149513970709743E-4
train: 130, loss: 5.53962153355985E-4
train: 131, loss: 7.31904376360391E-4
train: 132, loss: 1.533239092817815E-4
train: 133, loss: 0.1150136879117387
train: 134, loss: 6.148125965045406E-4
train: 135, loss: 7.918543619267794E-4
train: 136, loss: 9.457797271360078E-4
train: 137, loss: 0.004223789939936744
train: 138, loss: 9.060378587520562E-4
train: 139, loss: 0.01934381333406821
train: 140, loss: 3.2548888778240175E-6
train: 141, loss: 0.007164017798452467
train: 142, loss: 0.007423230079521334
train: 143, loss: 0.0036028805674437896
train: 144, loss: 0.004100509348002396
train: 145, loss: 0.012926029201996338
train: 146, loss: 0.006689781312192067
train: 147, loss: 3.975710092284399E-6
train: 148, loss: 0.00228961213827177
train: 149, loss: 0.0020974476907741615
train: 150, loss: 0.0025418996536575927
train: 151, loss: 8.544674249508836E-4
train: 152, loss: 0.004752511146778618
train: 153, loss: 0.004277599282501441
train: 154, loss: 5.93026919128986E-4
train: 155, loss: 0.0054378487549586576
train: 156, loss: 1.2456001550458382E-4
train: 157, loss: 0.0012570273108883506
train: 158, loss: 7.3676621096515E-7
train: 159, loss: 0.013332386119011275
train: 160, loss: 0.0189733720999014
train: 161, loss: 0.012966234851427028
train: 162, loss: 0.18787662838901528
train: 163, loss: 1.1713606434524204E-4
train: 164, loss: 0.020108433088437395
train: 165, loss: 0.009395343086892915
train: 166, loss: 7.306897346123499E-4
train: 167, loss: 0.021985640663222644
train: 168, loss: 2.678254834334856E-4
train: 169, loss: 4.303206128517272E-4
train: 170, loss: 0.004216276916862844
train: 171, loss: 0.26095827262972776
train: 172, loss: 7.801520016176384E-5
train: 173, loss: 0.0012724761606954093
train: 174, loss: 0.038585446152776405
train: 175, loss: 9.881297388027833E-5
train: 176, loss: 3.981511026060325E-4
train: 177, loss: 0.009872248232844568
train: 178, loss: 3.765049358541446E-4
train: 179, loss: 9.595731293478963E-4
train: 180, loss: 0.0028833841286965882
train: 181, loss: 0.0012322284594805308
train: 182, loss: 9.587853517659069E-4
train: 183, loss: 0.004290807341621433
train: 184, loss: 0.0032115442797330347
train: 185, loss: 0.009023284489021682
train: 186, loss: 0.019770811513829888
train: 187, loss: 7.726275768372605E-6
train: 188, loss: 0.37016918094803886
train: 189, loss: 7.314837232487506E-5
train: 190, loss: 0.2779875516067656
train: 191, loss: 0.002178315814144925
train: 192, loss: 8.804970888223303E-4
train: 193, loss: 3.407949502787337E-4
train: 194, loss: 2.1457522900534699E-7
train: 195, loss: 7.549732584901022E-4
train: 196, loss: 0.013835857920369999
train: 197, loss: 0.012720499453078389
train: 198, loss: 0.030548340822616146
train: 199, loss: 0.05530470354456194
train: 200, loss: 1.1961193018748637E-4
train: 201, loss: 1.6671930556338475E-4
train: 202, loss: 1.1768090936545802E-6
train: 203, loss: 1.3992524995177683E-4
train: 204, loss: 0.00820563840637923
train: 205, loss: 1.3412428865805932E-4
train: 206, loss: 2.4463875726407733E-8
train: 207, loss: 0.00925103776939222
train: 208, loss: 0.01862756892786507
train: 209, loss: 0.0012022402733092427
train: 210, loss: 3.9207385616586195E-5
train: 211, loss: 1.0667738642018449E-4
train: 212, loss: 5.2149063665300426E-5
train: 213, loss: 0.00496175166245552
train: 214, loss: 3.141272816515661E-5
train: 215, loss: 0.0012344207602471814
train: 216, loss: 0.0024245329816031964
train: 217, loss: 1.3878868342127594E-7
train: 218, loss: 4.951229081389389E-4
train: 219, loss: 0.0014243737912857898
train: 220, loss: 3.14011244098308E-5
train: 221, loss: 0.015009323556806313
train: 222, loss: 0.0018171443836726717
train: 223, loss: 2.8164309413335567E-5
train: 224, loss: 0.005909909960054807
train: 225, loss: 3.901164208518617E-9
train: 226, loss: 0.017538928221746103
train: 227, loss: 7.939248923851954E-5
train: 228, loss: 2.1892733322354006E-8
train: 229, loss: 1.7237760597324862E-7
train: 230, loss: 0.013841864799283347
train: 231, loss: 2.228460765807408E-5
train: 232, loss: 4.762133571382967E-4
train: 233, loss: 0.0017835885321163395
train: 234, loss: 3.432530583042291E-5
train: 235, loss: 0.01143504217122198
train: 236, loss: 9.040931374111659E-4
train: 237, loss: 2.3570852834475966E-7
train: 238, loss: 0.005082272921369612
train: 239, loss: 0.004351051917505859
train: 240, loss: 0.018926665768149123
train: 241, loss: 1.614444643418882E-4
train: 242, loss: 5.692046664691747E-4
train: 243, loss: 0.007334380986608518
train: 244, loss: 1.4435083727125012E-4
train: 245, loss: 3.109876006121376E-5
train: 246, loss: 1.8052504725564505E-4
train: 247, loss: 0.0017713987017972538
train: 248, loss: 1.323829115374008E-8
train: 249, loss: 0.0019711192414769347
train: 250, loss: 0.0027465856981405347
train: 251, loss: 0.0012992988837189502
train: 252, loss: 2.9553820772644946E-4
train: 253, loss: 0.0017154214489601566
train: 254, loss: 2.6709864036183854E-4
train: 255, loss: 0.0020695219338873884
train: 256, loss: 8.469559695527642E-4
train: 257, loss: 0.03847904297641169
train: 258, loss: 5.246437214126524E-4
train: 259, loss: 3.033503040657015E-5
train: 260, loss: 2.964028871788391E-4
train: 261, loss: 0.005673192873987057
train: 262, loss: 0.002127602246937631
train: 263, loss: 1.2879235734220977E-4
train: 264, loss: 0.0018002284464471161
train: 265, loss: 2.2574690634219297E-7
train: 266, loss: 4.112787052451588E-6
train: 267, loss: 6.3579224320826E-4
train: 268, loss: 0.0022940736385854863
train: 269, loss: 6.009045277405524E-4
train: 270, loss: 0.006081988179835294
train: 271, loss: 0.0015633727233071528
train: 272, loss: 5.340658188931352E-10
train: 273, loss: 0.029511769132321876
train: 274, loss: 6.066355403233917E-4
train: 275, loss: 0.005545678848260355
train: 276, loss: 8.514424837721125E-4
train: 277, loss: 0.0063130618395876555
train: 278, loss: 2.1816631161875575E-4
train: 279, loss: 0.005231614073818027
train: 280, loss: 4.0560703149157903E-4
train: 281, loss: 0.006157104012069467
train: 282, loss: 1.3658792845347374E-4
train: 283, loss: 2.9186672173203167E-4
train: 284, loss: 9.218759302906956E-4
train: 285, loss: 3.334628642702059E-5
train: 286, loss: 8.255947785312443E-4
train: 287, loss: 1.7671350350050564E-4
train: 288, loss: 0.0013822860288088116
train: 289, loss: 0.0066878283701398665
train: 290, loss: 2.686712211355034E-4
train: 291, loss: 7.25990780872966E-4
train: 292, loss: 1.9485999007989654E-6
train: 293, loss: 2.928149069968854E-6
train: 294, loss: 1.615941379656686E-4
train: 295, loss: 3.4465728758726503E-6
train: 296, loss: 0.007751873956527651
train: 297, loss: 3.250669627099311E-5
train: 298, loss: 4.3966626653084743E-4
train: 299, loss: 2.7151959108283574E-4
train: 300, loss: 7.170426544635925E-4
train: 301, loss: 5.971962272865734E-7
train: 302, loss: 0.006956225434171893
train: 303, loss: 4.5459827651573036E-6
train: 304, loss: 0.0017720961220219684
train: 305, loss: 0.0652534656517795
train: 306, loss: 0.001836592628816034
train: 307, loss: 0.0012343117921659185
train: 308, loss: 0.0013936504867214222
train: 309, loss: 8.04480624016784E-6
train: 310, loss: 0.001018815524600239
train: 311, loss: 9.608642357045673E-4
train: 312, loss: 0.001677038614890865
train: 313, loss: 4.4326349554313815E-4
train: 314, loss: 3.6649059448787556E-4
train: 315, loss: 0.004224924233983531
train: 316, loss: 0.0013815827951859513
train: 317, loss: 0.0018736228679014082
train: 318, loss: 0.016556837142812086
train: 319, loss: 9.177221979685534E-6
train: 320, loss: 7.306449470732157E-4
train: 321, loss: 0.006725146556450451
train: 322, loss: 0.005280606039735245
train: 323, loss: 0.0059545315503358376
train: 324, loss: 1.1760178239787525E-6
train: 325, loss: 1.535956844775611E-5
train: 326, loss: 0.0025432579086831536
train: 327, loss: 0.0033764279241539747
train: 328, loss: 4.015552662147781E-9
train: 329, loss: 9.91695251277777E-4
train: 330, loss: 0.0011915455896798174
train: 331, loss: 0.0015510275211147052
train: 332, loss: 0.004774715114651776
train: 333, loss: 0.00404772363382736
train: 334, loss: 0.0018666162232389124
train: 335, loss: 1.1799575884956192E-5
train: 336, loss: 8.681276133520941E-10
train: 337, loss: 0.0028743411046454302
train: 338, loss: 2.0544110472051732E-5
train: 339, loss: 7.284818849184307E-4
train: 340, loss: 5.747935363471211E-13
train: 341, loss: 1.4367412519962962E-5
train: 342, loss: 7.799296129835961E-4
train: 343, loss: 0.010833995943944364
train: 344, loss: 1.9465082748988671E-7
train: 345, loss: 5.065558004091962E-6
train: 346, loss: 0.0181440480174919
train: 347, loss: 0.002670550959382188
train: 348, loss: 3.696231092925074E-7
train: 349, loss: 0.04972016215929492
train: 350, loss: 5.076337530342978E-6
train: 351, loss: 0.00817205742378165
train: 352, loss: 1.0109733256418389E-7
train: 353, loss: 9.170572922947918E-6
train: 354, loss: 0.004341515749475414
train: 355, loss: 0.005472372506757113
train: 356, loss: 0.005644507882309641
train: 357, loss: 3.828823479043341E-6
train: 358, loss: 2.930616835143783E-4
train: 359, loss: 1.0764735801841896E-5
train: 360, loss: 7.091638172948806E-5
train: 361, loss: 0.006265334189683939
train: 362, loss: 6.14213915838224E-5
train: 363, loss: 0.0023719768568785504
train: 364, loss: 0.008742456758979262
train: 365, loss: 4.559706302635133E-8
train: 366, loss: 2.9389112467766523E-4
train: 367, loss: 2.4594919573049824E-4
train: 368, loss: 1.5990892056764157E-11
train: 369, loss: 0.001204336774731705
train: 370, loss: 0.001241308610554495
train: 371, loss: 9.381255970749988E-4
train: 372, loss: 0.001243876529527928
train: 373, loss: 6.406980938997981E-4
train: 374, loss: 3.4337074386284823E-6
train: 375, loss: 2.0877922737886173E-4
train: 376, loss: 7.792851210959574E-9
train: 377, loss: 0.004566675249430674
train: 378, loss: 0.0026731245060419846
train: 379, loss: 2.038585873463177E-11
train: 380, loss: 3.865112807934062E-5
train: 381, loss: 3.6884965486998016E-5
train: 382, loss: 0.012666800930459196
train: 383, loss: 0.002185249753296543
train: 384, loss: 0.0073970223185670465
train: 385, loss: 0.013085968343147032
train: 386, loss: 0.004384377947035622
train: 387, loss: 2.1433339518241097E-4
train: 388, loss: 0.003899778408078216
train: 389, loss: 0.0010729424824508981
train: 390, loss: 6.657566766117359E-5
train: 391, loss: 0.0022742776265951437
train: 392, loss: 0.002270501550463465
train: 393, loss: 4.3712815733604887E-7
train: 394, loss: 6.235226340656496E-4
train: 395, loss: 0.0017270683248497528
train: 396, loss: 3.6714000762730516E-6
train: 397, loss: 3.107729016012907E-4
train: 398, loss: 5.989289452239841E-6
train: 399, loss: 4.2140710305174265E-4
train: 400, loss: 2.5014616934358665E-11
train: 401, loss: 3.9431975892847396E-4
train: 402, loss: 2.4308072891045826E-4
train: 403, loss: 0.0015876214276543436
train: 404, loss: 0.001989026348948758
train: 405, loss: 3.1944046768590193E-4
train: 406, loss: 8.63882061664778E-4
train: 407, loss: 5.607492924631254E-4
train: 408, loss: 1.8815710137769535E-4
train: 409, loss: 5.9234890939428816E-5
train: 410, loss: 0.0034203960100628937
train: 411, loss: 0.0015618176168002912
train: 412, loss: 5.127634214597808E-6
train: 413, loss: 1.0447068986483461E-7
train: 414, loss: 8.633006440120803E-6
train: 415, loss: 0.004819579451358006
train: 416, loss: 4.1237499394438644E-8
train: 417, loss: 0.007202976791203807
train: 418, loss: 0.0017688495216485987
train: 419, loss: 4.326032257699725E-6
train: 420, loss: 1.5316341402855639E-6
train: 421, loss: 0.0030283735217470215
train: 422, loss: 8.827466177739446E-5
train: 423, loss: 8.621797293711525E-13
train: 424, loss: 0.42882856828723265
train: 425, loss: 6.240550570266264E-4
train: 426, loss: 0.005693730309715671
train: 427, loss: 8.222912869714252E-5
train: 428, loss: 3.026642379691825E-4
train: 429, loss: 1.5065568680548968E-4
train: 430, loss: 4.8179872324283256E-4
train: 431, loss: 6.018357211580923E-4
train: 432, loss: 5.567828848133304E-4
train: 433, loss: 6.999891229937983E-8
train: 434, loss: 9.164812694896203E-6
train: 435, loss: 0.10031062657609666
train: 436, loss: 5.985557380859585E-7
train: 437, loss: 8.308331891237001E-6
train: 438, loss: 6.185930273796257E-4
train: 439, loss: 0.0010978261258707705
train: 440, loss: 0.0012875733185246671
train: 441, loss: 0.009902809499358777
train: 442, loss: 1.4636053706692682E-4
train: 443, loss: 1.4241654156461461E-10
train: 444, loss: 0.001657102248080041
train: 445, loss: 2.1967647874527666E-7
train: 446, loss: 2.580538264406589E-9
train: 447, loss: 3.3958787976285552E-6
train: 448, loss: 3.81528609593314E-4
train: 449, loss: 5.431448545908162E-4
train: 450, loss: 3.183448124772117E-10
train: 451, loss: 0.0026790993729663612
train: 452, loss: 0.03135945438331257
train: 453, loss: 5.364448233995793E-4
train: 454, loss: 6.061593569282893E-5
train: 455, loss: 0.0011028905564316543
train: 456, loss: 2.800949656965489E-7
train: 457, loss: 0.003183523244746068
train: 458, loss: 4.689912050561382E-6
train: 459, loss: 0.00864783948038782
train: 460, loss: 0.0018941984028182947
train: 461, loss: 2.44806799286783E-4
train: 462, loss: 4.5107009236703565E-4
train: 463, loss: 4.70821432375197E-6
train: 464, loss: 4.930312146072155E-4
train: 465, loss: 1.388094100995689E-4
train: 466, loss: 1.3908412590851805E-4
train: 467, loss: 2.019507311667163E-6
train: 468, loss: 6.860026108613284E-4
train: 469, loss: 5.09390631061591E-4
train: 470, loss: 2.9619569983924034E-10
train: 471, loss: 0.002937389136636519
train: 472, loss: 0.002843790573184872
train: 473, loss: 1.6966906161241664E-4
train: 474, loss: 0.0024339428618848887
train: 475, loss: 2.2776204683879672E-7
train: 476, loss: 0.001800488981735969
train: 477, loss: 0.010190855945445663
train: 478, loss: 3.1889993241839514E-11
train: 479, loss: 7.375960496561229E-10
train: 480, loss: 8.26374623171604E-4
train: 481, loss: 1.3025760191348268E-4
train: 482, loss: 8.898843966877282E-4
train: 483, loss: 7.812118815181026E-4
train: 484, loss: 3.390697261253348E-5
train: 485, loss: 3.282481618415717E-4
train: 486, loss: 3.0962747120100784E-5
train: 487, loss: 3.1014546199115774E-5
train: 488, loss: 8.808543068005918E-5
train: 489, loss: 2.424724232494134E-7
train: 490, loss: 1.1582019223897228E-4
train: 491, loss: 0.004262637945901503
train: 492, loss: 1.047475361315285E-4
train: 493, loss: 8.912363877406236E-8
train: 494, loss: 1.911892941332761E-6
train: 495, loss: 3.087079575541834E-5
train: 496, loss: 9.682281566854679E-5
train: 497, loss: 1.1562509547469452E-4
train: 498, loss: 0.0012784968683520945
train: 499, loss: 3.4973821751369796E-6
train: 500, loss: 0.011692923157912075
train: 501, loss: 4.947288870335178E-4
train: 502, loss: 8.681276506966903E-4
train: 503, loss: 3.7181530119529737E-11
train: 504, loss: 8.084548836400317E-5
train: 505, loss: 0.0018873701345602525
train: 506, loss: 5.429908729482836E-4
train: 507, loss: 1.0450311284887988E-4
train: 508, loss: 4.4516921375962494E-4
train: 509, loss: 3.4705068808443348E-6
train: 510, loss: 5.990730714929264E-4
train: 511, loss: 2.776861669846581E-6
train: 512, loss: 0.0011202974109605654
train: 513, loss: 0.0033666428446255826
train: 514, loss: 0.0025800406437749422
train: 515, loss: 2.1997558168201377E-4
train: 516, loss: 0.0010373284496667371
train: 517, loss: 3.415193291034325E-4
train: 518, loss: 1.1222964374440612E-6
train: 519, loss: 0.009913712326214713
train: 520, loss: 6.107182046356139E-5
train: 521, loss: 2.6252692991990455E-4
train: 522, loss: 0.002470647311458359
train: 523, loss: 1.371098636594713E-4
train: 524, loss: 1.2185315245100264E-4
train: 525, loss: 1.6425773230292516E-12
train: 526, loss: 1.4254392510370585E-7
train: 527, loss: 1.990624369332704E-8
train: 528, loss: 0.0033392387241404095
train: 529, loss: 0.0021136495000431167
train: 530, loss: 3.3823477157249833E-6
train: 531, loss: 0.002513269299739203
train: 532, loss: 0.0020308043725107226
train: 533, loss: 7.078106972938728E-4
train: 534, loss: 2.0091720429997435E-4
train: 535, loss: 3.0204017324846535E-5
train: 536, loss: 6.898533575397122E-4
train: 537, loss: 6.87242574404606E-5
train: 538, loss: 0.3088870762374324
train: 539, loss: 2.736394470760186E-8
train: 540, loss: 2.6745736235252873E-5
train: 541, loss: 3.5895995064262948E-6
train: 542, loss: 0.12027018621974828
train: 543, loss: 0.020873402769100602
train: 544, loss: 0.0014145623292815775
train: 545, loss: 4.006548719166671E-5
train: 546, loss: 5.330794493098893E-4
train: 547, loss: 3.183877274412922E-5
train: 548, loss: 2.52090234467404E-7
train: 549, loss: 2.170846740268028E-6
train: 550, loss: 4.987069701961037E-5
train: 551, loss: 0.0025979823560300613
train: 552, loss: 2.4898248764760366E-5
train: 553, loss: 1.3474795880333979E-8
train: 554, loss: 0.07055036442211302
train: 555, loss: 0.12532473613195177
train: 556, loss: 1.880623388370942E-5
train: 557, loss: 8.489465121367261E-8
train: 558, loss: 0.001604984433908691
train: 559, loss: 0.001468118010808983
train: 560, loss: 0.3437991333761218
train: 561, loss: 9.011255566382296E-14
train: 562, loss: 7.147686411928827E-4
train: 563, loss: 9.818519836008918E-14
train: 564, loss: 4.2877556885376323E-5
train: 565, loss: 0.10003793317830699
train: 566, loss: 4.124158516286337E-4
train: 567, loss: 2.4067384636282046E-4
train: 568, loss: 1.2370932591044894E-4
train: 569, loss: 0.005014266080186274
train: 570, loss: 1.5322801417543713E-4
train: 571, loss: 1.8298373876727735E-7
train: 572, loss: 2.285768722275696E-4
train: 573, loss: 9.386353422580323E-4
train: 574, loss: 0.0023397547214036433
train: 575, loss: 0.003828029290860863
train: 576, loss: 6.725370343015182E-4
train: 577, loss: 5.078991869206103E-5
train: 578, loss: 0.0022930340477010855
train: 579, loss: 4.302988528940676E-7
train: 580, loss: 0.0050318642406816366
train: 581, loss: 4.333103778882508E-5
train: 582, loss: 0.0036524953013349144
train: 583, loss: 0.0017273470072073642
train: 584, loss: 1.0265368467094588E-6
train: 585, loss: 1.854025652030338E-7
train: 586, loss: 1.8427773325403732E-4
train: 587, loss: 1.4477994023334041E-5
train: 588, loss: 0.0022189518633684885
train: 589, loss: 4.408159977739213E-4
train: 590, loss: 8.51952789828704E-5
train: 591, loss: 0.01615748239716325
train: 592, loss: 9.185166868371204E-4
train: 593, loss: 2.4106210956034517E-4
train: 594, loss: 0.0022994324728023412
train: 595, loss: 2.2569425618975678E-10
train: 596, loss: 0.012349040169752135
train: 597, loss: 0.0014982538953944471
train: 598, loss: 1.2029871306614489E-4
train: 599, loss: 6.710225857060434E-4
train: 600, loss: 0.0010213066416427847
train: 601, loss: 2.3841186416062244E-6
train: 602, loss: 2.954027721065245E-4
train: 603, loss: 1.5115019100442307E-4
train: 604, loss: 7.773530012918145E-4
train: 605, loss: 7.42056592347836E-4
train: 606, loss: 2.325668530310985E-5
train: 607, loss: 1.6562652176497857E-6
train: 608, loss: 1.8527551265444127E-11
train: 609, loss: 3.005121528351507E-4
train: 610, loss: 1.9949984685741498E-6
train: 611, loss: 0.0019685921700197512
train: 612, loss: 3.5683434262987866E-7
train: 613, loss: 0.0033874757284648002
train: 614, loss: 5.483762309565956E-4
train: 615, loss: 9.805140818541964E-5
train: 616, loss: 0.0010938746771746806
train: 617, loss: 1.9269697773941876E-5
train: 618, loss: 0.0012430241357545696
train: 619, loss: 6.353311766903343E-14
train: 620, loss: 3.350125494804822E-6
train: 621, loss: 0.0037102596549507074
train: 622, loss: 3.159926211345809E-5
train: 623, loss: 0.0034710429482578357
train: 624, loss: 5.878964941272068E-4
train: 625, loss: 0.0026811240707150835
train: 626, loss: 5.985712942261132E-4
train: 627, loss: 0.004287470187841719
train: 628, loss: 5.20297569939848E-4
train: 629, loss: 8.043568157052272E-4
train: 630, loss: 1.2940569198525375E-12
train: 631, loss: 1.584862011712602E-4
train: 632, loss: 3.629071652311202E-8
train: 633, loss: 2.699189748729646E-4
train: 634, loss: 0.003488759762240398
train: 635, loss: 7.457069877437493E-4
train: 636, loss: 0.0014003909426290628
train: 637, loss: 8.590359433999799E-12
train: 638, loss: 2.8284158320787E-5
train: 639, loss: 6.024959398102797E-4
train: 640, loss: 3.8590631695598884E-4
train: 641, loss: 2.0959398096006108E-5
train: 642, loss: 1.4609590615092005E-6
train: 643, loss: 0.0076727810664245005
train: 644, loss: 6.03678385811679E-4
train: 645, loss: 6.828694502055461E-7
train: 646, loss: 0.0013165068973250394
train: 647, loss: 1.708254592561441E-4
train: 648, loss: 2.5938744693173637E-5
train: 649, loss: 1.561867703496293E-15
train: 650, loss: 4.465763130991165E-5
train: 651, loss: 3.5486047751208166E-4
train: 652, loss: 0.001776796539008968
train: 653, loss: 5.585367582212217E-5
train: 654, loss: 9.407907933861494E-11
train: 655, loss: 2.126293954237677E-6
train: 656, loss: 4.237184993089204E-8
train: 657, loss: 3.6768498498181256E-5
train: 658, loss: 1.2132142140854795E-4
train: 659, loss: 1.2860308072555095E-4
train: 660, loss: 0.0014006805383356118
train: 661, loss: 9.312500467759775E-6
train: 662, loss: 0.0011222004569587168
train: 663, loss: 9.229629455454941E-5
train: 664, loss: 1.243169688264128E-5
train: 665, loss: 1.205319989643407E-6
train: 666, loss: 4.87931565177099E-5
train: 667, loss: 0.0011694367312820853
train: 668, loss: 1.2406639566714023E-6
train: 669, loss: 1.2643970703821676E-6
train: 670, loss: 1.7167688906344294E-5
train: 671, loss: 0.0030775724749512916
train: 672, loss: 4.386455432650577E-5
train: 673, loss: 1.4666332755823868E-5
train: 674, loss: 5.165637041510785E-6
train: 675, loss: 0.013600642200697071
train: 676, loss: 3.064884046887886E-4
train: 677, loss: 4.018175385051544E-5
train: 678, loss: 0.002548633053959568
train: 679, loss: 0.0016014708924606395
train: 680, loss: 4.750469543604587E-8
train: 681, loss: 4.225064289803984E-8
train: 682, loss: 5.278088941820082E-5
train: 683, loss: 9.200432771180714E-7
train: 684, loss: 1.852296171269557E-4
train: 685, loss: 0.001038928331707164
train: 686, loss: 6.370383202498634E-15
train: 687, loss: 0.0018336071031431914
train: 688, loss: 2.926743727240267E-4
train: 689, loss: 4.000117011932379E-4
train: 690, loss: 0.0011094993927173345
train: 691, loss: 3.4910977207375796E-4
train: 692, loss: 4.9481401933471154E-8
train: 693, loss: 1.49460540242972E-5
train: 694, loss: 2.0586739613561313E-5
train: 695, loss: 3.5934666336349346E-7
train: 696, loss: 0.004271566352882864
train: 697, loss: 5.585970012842509E-4
train: 698, loss: 0.0017891603532793848
train: 699, loss: 9.016303724854915E-5
train: 700, loss: 1.3389642957049762E-5
train: 701, loss: 4.7109233003357994E-4
train: 702, loss: 2.5740266940795685E-4
train: 703, loss: 1.2478577425710776E-6
train: 704, loss: 4.6425845341363604E-5
train: 705, loss: 7.116167130771129E-5
train: 706, loss: 0.002271454232466564
train: 707, loss: 8.21169805874011E-4
train: 708, loss: 1.2550201991993983E-6
train: 709, loss: 0.0012495856225904903
train: 710, loss: 3.413761231703522E-4
train: 711, loss: 0.0019581434389535036
train: 712, loss: 3.918143548082964E-5
train: 713, loss: 3.388744625328741E-4
train: 714, loss: 1.0790665895041856E-6
train: 715, loss: 2.098379374237667E-7
train: 716, loss: 2.4857144304485184E-4
train: 717, loss: 3.7446414265411255E-5
train: 718, loss: 1.8703719164947182E-9
train: 719, loss: 1.5167033039905428E-15
train: 720, loss: 3.8025915933307306E-7
train: 721, loss: 1.0098954702419705E-4
train: 722, loss: 5.172115924372934E-8
train: 723, loss: 3.5718422801336657E-4
train: 724, loss: 0.0010623822667873099
train: 725, loss: 8.419999258763964E-5
train: 726, loss: 6.594393565712896E-4
train: 727, loss: 1.4915680735720324E-8
train: 728, loss: 0.004786029391746565
train: 729, loss: 7.294318008340561E-7
train: 730, loss: 4.2861115485801006E-4
train: 731, loss: 4.2540233791503457E-4
train: 732, loss: 0.0016298953499312618
train: 733, loss: 1.7265882327898547E-5
train: 734, loss: 1.7881649446831062E-9
train: 735, loss: 8.774277892429672E-6
train: 736, loss: 3.6848323912352173E-9
train: 737, loss: 0.0018023573142601794
train: 738, loss: 1.0097850595281718E-8
train: 739, loss: 3.599401970941211E-5
train: 740, loss: 3.29241188537207E-4
train: 741, loss: 2.28677034039819E-9
train: 742, loss: 8.008447897412577E-4
train: 743, loss: 6.257317924611857E-5
train: 744, loss: 2.586664506761554E-5
train: 745, loss: 9.33943463941018E-7
train: 746, loss: 1.1559220986966344E-4
train: 747, loss: 3.391239684670684E-4
train: 748, loss: 9.596451352311293E-7
train: 749, loss: 9.424909920529318E-7
train: 750, loss: 2.521622339247244E-5
train: 751, loss: 7.386463682002391E-7
train: 752, loss: 0.001169700604364768
train: 753, loss: 5.643031410840779E-7
train: 754, loss: 3.053107014964895E-5
train: 755, loss: 6.37449316397997E-5
train: 756, loss: 2.9764805954977532E-8
train: 757, loss: 1.3604641149526735E-4
train: 758, loss: 3.0967198180152276E-4
train: 759, loss: 5.5397458962743284E-9
train: 760, loss: 0.20511876076232755
train: 761, loss: 0.001284645563487182
train: 762, loss: 4.764489940871224E-13
train: 763, loss: 4.3110289731553165E-7
train: 764, loss: 6.890048793393545E-5
train: 765, loss: 3.150739180243613E-4
train: 766, loss: 0.0012000582374838268
train: 767, loss: 8.558548590542404E-4
train: 768, loss: 6.862457484338685E-5
train: 769, loss: 1.6368702945521073E-7
train: 770, loss: 8.097233811760951E-7
train: 771, loss: 2.7925380450143116E-4
train: 772, loss: 4.235216912385081E-4
train: 773, loss: 1.484026585073918E-6
train: 774, loss: 6.448580248321945E-4
train: 775, loss: 0.005311243558763555
train: 776, loss: 0.0012154827532063581
train: 777, loss: 8.979045065207622E-7
train: 778, loss: 1.4333030305434634E-7
train: 779, loss: 0.005883996828955378
train: 780, loss: 5.665991371885146E-4
train: 781, loss: 7.285949399761079E-6
train: 782, loss: 8.676173190357874E-4
train: 783, loss: 0.002642613314286124
train: 784, loss: 0.0010640829502210618
train: 785, loss: 7.321860694574474E-10
train: 786, loss: 1.0830747567603998E-8
train: 787, loss: 7.103077221712809E-4
train: 788, loss: 2.242264651632521E-8
train: 789, loss: 0.0010796873261044715
train: 790, loss: 2.6522134295637485E-5
train: 791, loss: 0.4316033228780368
train: 792, loss: 1.437052324253507E-5
train: 793, loss: 1.6860645798425357E-8
train: 794, loss: 0.001480112166017226
train: 795, loss: 0.001440290512397799
train: 796, loss: 5.995809060501478E-5
train: 797, loss: 0.0023530033755194864
train: 798, loss: 1.420308136071219E-5
train: 799, loss: 0.0017343824945649654
train: 800, loss: 1.1385601735765817E-4
train: 801, loss: 0.0035809557671220103
train: 802, loss: 1.5631441754073098E-4
train: 803, loss: 0.0024118954282988385
train: 804, loss: 2.5269608675931893E-5
train: 805, loss: 1.0811538753016901E-4
train: 806, loss: 3.341457384617616E-4
train: 807, loss: 8.911266251046506E-6
train: 808, loss: 9.197461116054914E-5
train: 809, loss: 0.002635513441538821
train: 810, loss: 0.11622268476122152
train: 811, loss: 6.487488690321037E-7
train: 812, loss: 2.6518139790445532E-8
train: 813, loss: 0.002013729390306529
train: 814, loss: 0.0012922419079450201
train: 815, loss: 4.541186638609025E-7
train: 816, loss: 3.0529683238270626E-4
train: 817, loss: 6.278659389366255E-4
train: 818, loss: 2.7122584711931067E-4
train: 819, loss: 1.8454713125811112E-5
train: 820, loss: 0.0015315619669964884
train: 821, loss: 9.339250714590153E-16
train: 822, loss: 1.6292353807889852E-5
train: 823, loss: 1.298556182684446E-4
train: 824, loss: 9.115741827736088E-14
train: 825, loss: 2.467492422217806E-8
train: 826, loss: 2.2895841690215264E-7
train: 827, loss: 2.2878593101516467E-5
train: 828, loss: 3.3210656608434164E-5
train: 829, loss: 8.529851739262513E-4
train: 830, loss: 2.6916975429007685E-6
train: 831, loss: 1.1050371007921E-9
train: 832, loss: 3.6369865193092196E-13
train: 833, loss: 0.0032867562042931796
train: 834, loss: 2.3984653243559715E-6
train: 835, loss: 7.573566770162133E-6
train: 836, loss: 1.626823456631053E-5
train: 837, loss: 7.294927619778834E-7
train: 838, loss: 0.016543766291615036
train: 839, loss: 1.0093006435937673E-14
train: 840, loss: 1.9109332360574084E-15
train: 841, loss: 0.036329688790261126
train: 842, loss: 2.0592539604136632E-4
train: 843, loss: 6.537639258049107E-6
train: 844, loss: 0.001092160288844483
train: 845, loss: 3.2206242044017375E-4
train: 846, loss: 4.753094875958509E-5
train: 847, loss: 3.5289457994861295E-5
train: 848, loss: 6.516224842493188E-8
train: 849, loss: 4.792506791522758E-7
train: 850, loss: 2.914589850093187E-4
train: 851, loss: 0.002063216752180254
train: 852, loss: 1.1309435291275315E-4
train: 853, loss: 2.989603171990144E-6
train: 854, loss: 6.883382833124846E-5
train: 855, loss: 6.831833943636014E-4
train: 856, loss: 0.004930531118699491
train: 857, loss: 3.6368873174335423E-8
train: 858, loss: 5.636637346903748E-4
train: 859, loss: 0.0011964188279445854
train: 860, loss: 0.016292631794340297
train: 861, loss: 1.994867347709949E-4
train: 862, loss: 0.08630262242537887
train: 863, loss: 0.3634472617994541
train: 864, loss: 1.3476004882778172E-8
train: 865, loss: 0.0022058568207620134
train: 866, loss: 2.0611367753153674E-5
train: 867, loss: 1.3898674154362836E-4
train: 868, loss: 4.990553540728595E-10
train: 869, loss: 0.002502474844417208
train: 870, loss: 1.874894073557494E-4
train: 871, loss: 0.0010823374874163493
train: 872, loss: 1.3731928823762754E-4
train: 873, loss: 1.8383514212239826E-8
train: 874, loss: 1.177721223860746E-5
train: 875, loss: 9.721080135476817E-7
train: 876, loss: 2.813155385972309E-9
train: 877, loss: 3.258253179504327E-15
train: 878, loss: 2.705642391905141E-4
train: 879, loss: 3.108048989864021E-6
train: 880, loss: 0.001393430901701079
train: 881, loss: 4.950403041826756E-4
train: 882, loss: 0.0019914352491948897
train: 883, loss: 3.034807552234216E-4
train: 884, loss: 4.2264640854936544E-7
train: 885, loss: 6.137191481285267E-8
train: 886, loss: 2.3152313875004146E-4
train: 887, loss: 6.255359150698384E-12
train: 888, loss: 0.06650529398669938
train: 889, loss: 6.319518728443931E-4
train: 890, loss: 2.3065744559754733E-4
train: 891, loss: 0.36430193906375047
train: 892, loss: 1.46398736637071E-4
train: 893, loss: 7.768624572006062E-7
train: 894, loss: 3.1312448221827304E-5
train: 895, loss: 5.207110453244666E-4
train: 896, loss: 2.3669369603078073E-4
train: 897, loss: 7.970646179094315E-4
train: 898, loss: 0.0013572338974810905
train: 899, loss: 6.633255936460794E-5
train: 900, loss: 5.242173197198749E-7
train: 901, loss: 0.0020658975682404574
train: 902, loss: 1.6423156234648056E-8
train: 903, loss: 7.116088658244527E-4
train: 904, loss: 0.052202335717128345
train: 905, loss: 5.0871991242908065E-9
train: 906, loss: 2.2824281760847114E-6
train: 907, loss: 0.0014367526622369841
train: 908, loss: 7.301112510082824E-4
train: 909, loss: 0.004147639375264338
train: 910, loss: 9.104395481086695E-4
train: 911, loss: 5.470466249760161E-6
train: 912, loss: 2.4215275992670564E-7
train: 913, loss: 0.008606862181790968
train: 914, loss: 1.6532138992178874E-5
train: 915, loss: 4.6983175270748443E-4
train: 916, loss: 0.003420002104312484
train: 917, loss: 0.0016757496899588574
train: 918, loss: 0.004113629161527083
train: 919, loss: 4.096687818869486E-4
train: 920, loss: 3.7693704793033845E-4
train: 921, loss: 8.643727403804706E-7
train: 922, loss: 1.1433544194574173E-5
train: 923, loss: 0.002067579341719091
train: 924, loss: 0.0024753105629646216
train: 925, loss: 4.3531271754352755E-7
train: 926, loss: 2.0105272501429638E-7
train: 927, loss: 0.001841288917995119
train: 928, loss: 1.6019857284076385E-4
train: 929, loss: 3.472397062867441E-4
train: 930, loss: 3.47410603461129E-7
train: 931, loss: 9.550271305438181E-6
train: 932, loss: 0.0015658529508693062
train: 933, loss: 1.3904811579720165E-9
train: 934, loss: 7.388327655921098E-7
train: 935, loss: 2.4916819486469288E-14
train: 936, loss: 0.0014149572596191841
train: 937, loss: 6.207083916256483E-4
train: 938, loss: 1.347820963672917E-5
train: 939, loss: 8.339101356033361E-10
train: 940, loss: 5.902345015997177E-5
train: 941, loss: 2.8643484424489166E-5
train: 942, loss: 3.1966312537902755E-7
train: 943, loss: 0.0035760216869653305
train: 944, loss: 1.425496335106224E-5
train: 945, loss: 1.4190841908294364E-6
train: 946, loss: 2.354746698173176E-5
train: 947, loss: 1.9072191102258499E-4
train: 948, loss: 5.999200478330767E-5
train: 949, loss: 5.464590687781167E-4
train: 950, loss: 3.542228088241963E-6
train: 951, loss: 0.007116693587525493
train: 952, loss: 0.023215346079868093
train: 953, loss: 2.5605673299717373E-13
train: 954, loss: 0.001602199905139802
train: 955, loss: 2.1894527109919443E-4
train: 956, loss: 1.339088926583732E-7
train: 957, loss: 9.93636259986494E-5
train: 958, loss: 5.783806711774688E-4
train: 959, loss: 0.008499752280246643
train: 960, loss: 0.0013453857382774441
train: 961, loss: 1.781206538918937E-5
train: 962, loss: 2.1090256127804876E-4
train: 963, loss: 2.2650222745154816E-4
train: 964, loss: 1.8988075378423366E-8
train: 965, loss: 0.12619627179976725
train: 966, loss: 2.4284799576000597E-4
train: 967, loss: 3.3283551104534755E-4
train: 968, loss: 1.9330929333553543E-6
train: 969, loss: 5.460375081295745E-4
train: 970, loss: 1.9867236507936758E-5
train: 971, loss: 0.002584217772559426
train: 972, loss: 5.878613092815059E-4
train: 973, loss: 9.579607070504368E-8
train: 974, loss: 5.222129732859214E-5
train: 975, loss: 0.00138175784720706
train: 976, loss: 3.4315859429554933E-4
train: 977, loss: 2.347518235158267E-6
train: 978, loss: 5.9124360646126256E-5
train: 979, loss: 2.4793986498170093E-4
train: 980, loss: 1.1371112896335685E-4
train: 981, loss: 2.7266579089017275E-7
train: 982, loss: 0.002762249857353579
train: 983, loss: 1.162880825028923E-6
train: 984, loss: 5.401198152927242E-6
train: 985, loss: 3.3575067344929795E-8
train: 986, loss: 0.0010421101460296438
train: 987, loss: 0.002691962448370437
train: 988, loss: 1.0742242032450815E-4
train: 989, loss: 6.464172157978604E-6
train: 990, loss: 3.861847728768133E-4
train: 991, loss: 2.123117947304721E-8
train: 992, loss: 2.0997788936882784E-6
train: 993, loss: 0.003954349049114103
train: 994, loss: 5.476668043094355E-5
train: 995, loss: 6.177258389230693E-4
train: 996, loss: 0.0015424465384768912
train: 997, loss: 1.424256615225798E-4
train: 998, loss: 9.944459929526203E-9
train: 999, loss: 1.477254695246706E-4
train: 1000, loss: 1.3882049932979035E-7
trainTime: 7337
==========Finish Train==========
==========Start Predict==========
label: -1.0, output: 0.9853648409213908, predict: 1.0
label: -1.0, output: 0.00493616536880423, predict: 0.0
label: -1.0, output: 0.9906854074434446, predict: 1.0
label: -1.0, output: 0.9912334157444194, predict: 1.0
label: -1.0, output: 0.8610054894120358, predict: 1.0
label: -1.0, output: 0.973438007403723, predict: 1.0
label: -1.0, output: 0.9998595424004203, predict: 1.0
label: -1.0, output: 0.05824664834083277, predict: 0.0
label: -1.0, output: 0.9999981156785596, predict: 1.0
label: -1.0, output: 0.9909317859512327, predict: 1.0
label: -1.0, output: 0.006386168550297885, predict: 0.0
label: -1.0, output: 0.9954385680128439, predict: 1.0
label: -1.0, output: 0.9370531056344535, predict: 1.0
label: -1.0, output: 0.9896668877053217, predict: 1.0
label: -1.0, output: 0.9860414215993931, predict: 1.0
label: -1.0, output: 0.006221182691517837, predict: 0.0
label: -1.0, output: 0.9730017449224008, predict: 1.0
label: -1.0, output: 5.492032253610965E-4, predict: 0.0
label: -1.0, output: 0.9598005831491531, predict: 1.0
label: -1.0, output: 0.004153182068308849, predict: 0.0
label: -1.0, output: 0.9273821892112692, predict: 1.0
label: -1.0, output: 0.002701224255333999, predict: 0.0
label: -1.0, output: 0.9680921946935033, predict: 1.0
label: -1.0, output: 0.9856347739886611, predict: 1.0
label: -1.0, output: 0.9865805114952438, predict: 1.0
label: -1.0, output: 0.01757264634062538, predict: 0.0
label: -1.0, output: 0.03825008056013937, predict: 0.0
label: -1.0, output: 0.9634811134626711, predict: 1.0
label: -1.0, output: 0.9894667245824864, predict: 1.0
label: -1.0, output: 0.8049407037901674, predict: 1.0
label: -1.0, output: 0.03430775978175859, predict: 0.0
label: -1.0, output: 0.029086962556209923, predict: 0.0
label: -1.0, output: 7.512268401560624E-4, predict: 0.0
label: -1.0, output: 0.9999769378740269, predict: 1.0
label: -1.0, output: 0.015554438225536258, predict: 0.0
label: -1.0, output: 0.8207399244593752, predict: 1.0
label: -1.0, output: 0.017001653945980452, predict: 0.0
label: -1.0, output: 0.05537715040223537, predict: 0.0
label: -1.0, output: 0.9995788820131037, predict: 1.0
label: -1.0, output: 5.268451619541996E-7, predict: 0.0
label: -1.0, output: 0.005030337254965252, predict: 0.0
label: -1.0, output: 0.05951411537823494, predict: 0.0
label: -1.0, output: 0.02012884450160313, predict: 0.0
label: -1.0, output: 0.9769992134722536, predict: 1.0
label: -1.0, output: 2.577712532195925E-7, predict: 0.0
label: -1.0, output: 0.004459368749413179, predict: 0.0
label: -1.0, output: 0.9995365627339976, predict: 1.0
label: -1.0, output: 0.9410426093891248, predict: 1.0
label: -1.0, output: 0.02273693864686524, predict: 0.0
label: -1.0, output: 0.9302574128639496, predict: 1.0
label: -1.0, output: 0.02931030292883508, predict: 0.0
label: -1.0, output: 0.009990590803326892, predict: 0.0
label: -1.0, output: 2.8213039200680268E-5, predict: 0.0
label: -1.0, output: 0.03321080808450976, predict: 0.0
label: -1.0, output: 0.008039367934652437, predict: 0.0
label: -1.0, output: 2.006610173412885E-4, predict: 0.0
label: -1.0, output: 0.9528859287910285, predict: 1.0
label: -1.0, output: 0.9301116475169289, predict: 1.0
label: -1.0, output: 0.9416321927362883, predict: 1.0
label: -1.0, output: 0.9487166442466963, predict: 1.0
label: -1.0, output: 0.9214283543302951, predict: 1.0
label: -1.0, output: 0.9988746990444479, predict: 1.0
label: -1.0, output: 0.9734081062363749, predict: 1.0
label: -1.0, output: 2.3397951286686399E-4, predict: 0.0
label: -1.0, output: 0.952853791468082, predict: 1.0
label: -1.0, output: 0.9821147227545273, predict: 1.0
label: -1.0, output: 0.17380517805038298, predict: 0.0
label: -1.0, output: 0.9224012839052761, predict: 1.0
label: -1.0, output: 0.9622162612811308, predict: 1.0
label: -1.0, output: 0.006186788561701722, predict: 0.0
label: -1.0, output: 0.9751044579165972, predict: 1.0
label: -1.0, output: 0.029149835418804535, predict: 0.0
label: -1.0, output: 0.9936644557508177, predict: 1.0
label: -1.0, output: 0.9583640468473253, predict: 1.0
label: -1.0, output: 0.9989051541742092, predict: 1.0
label: -1.0, output: 0.9689424786191003, predict: 1.0
label: -1.0, output: 0.998711644398048, predict: 1.0
label: -1.0, output: 0.004988654764034509, predict: 0.0
label: -1.0, output: 0.9464737109954304, predict: 1.0
label: -1.0, output: 0.03985255393975722, predict: 0.0
label: -1.0, output: 2.3993835968164776E-6, predict: 0.0
label: -1.0, output: 0.010695733316263636, predict: 0.0
label: -1.0, output: 0.0011809127625688953, predict: 0.0
label: -1.0, output: 0.13703344457486746, predict: 0.0
label: -1.0, output: 0.9886668316571205, predict: 1.0
label: -1.0, output: 0.00626134260635879, predict: 0.0
label: -1.0, output: 0.9891854382566715, predict: 1.0
label: -1.0, output: 0.020246660783488853, predict: 0.0
label: -1.0, output: 0.9999999835028592, predict: 1.0
label: -1.0, output: 0.00542417677529281, predict: 0.0
label: -1.0, output: 0.8510103653394926, predict: 1.0
label: -1.0, output: 0.0012244233807823263, predict: 0.0
label: -1.0, output: 0.9798101706908995, predict: 1.0
label: -1.0, output: 0.9496658683474283, predict: 1.0
label: -1.0, output: 0.011881131720468039, predict: 0.0
label: -1.0, output: 0.9507297254604213, predict: 1.0
label: -1.0, output: 0.9993681903775881, predict: 1.0
label: -1.0, output: 0.005987388298288506, predict: 0.0
label: -1.0, output: 2.545271190914505E-7, predict: 0.0
label: -1.0, output: 0.010699863345748389, predict: 0.0
label: -1.0, output: 0.992706159914318, predict: 1.0
label: -1.0, output: 0.04341459897834789, predict: 0.0
label: -1.0, output: 0.02162241497380315, predict: 0.0
label: -1.0, output: 0.9277636898551841, predict: 1.0
label: -1.0, output: 0.006337183414420159, predict: 0.0
label: -1.0, output: 0.9734336958379595, predict: 1.0
label: -1.0, output: 0.02052005569541363, predict: 0.0
label: -1.0, output: 0.9999883089198968, predict: 1.0
label: -1.0, output: 0.9881463870824468, predict: 1.0
label: -1.0, output: 0.9576883903292371, predict: 1.0
label: -1.0, output: 0.9619075839728641, predict: 1.0
label: -1.0, output: 3.014616964829331E-7, predict: 0.0
label: -1.0, output: 0.967262613093741, predict: 1.0
label: -1.0, output: 0.007202256276861925, predict: 0.0
label: -1.0, output: 0.016903685331820933, predict: 0.0
label: -1.0, output: 0.9855501145852925, predict: 1.0
label: -1.0, output: 0.9773487164779596, predict: 1.0
label: -1.0, output: 0.791079414334156, predict: 1.0
label: -1.0, output: 0.1060773418792201, predict: 0.0
label: -1.0, output: 0.021010868076842635, predict: 0.0
label: -1.0, output: 0.047429709738939056, predict: 0.0
label: -1.0, output: 0.009678722995296998, predict: 0.0
label: -1.0, output: 0.9838548067700045, predict: 1.0
label: -1.0, output: 0.9908951283532937, predict: 1.0
label: -1.0, output: 0.9995372332135708, predict: 1.0
label: -1.0, output: 0.9627975931109886, predict: 1.0
label: -1.0, output: 0.9207412381059684, predict: 1.0
label: -1.0, output: 0.9792336963741994, predict: 1.0
label: -1.0, output: 0.8757508157279954, predict: 1.0
label: -1.0, output: 0.9175174435893786, predict: 1.0
label: -1.0, output: 0.9929139117256459, predict: 1.0
label: -1.0, output: 0.9935919298804214, predict: 1.0
label: -1.0, output: 0.9744121157081284, predict: 1.0
label: -1.0, output: 0.9854860453556125, predict: 1.0
label: -1.0, output: 0.9851023828406771, predict: 1.0
label: -1.0, output: 4.542698489868381E-4, predict: 0.0
label: -1.0, output: 0.029431770033115298, predict: 0.0
label: -1.0, output: 0.07981086209094063, predict: 0.0
label: -1.0, output: 0.0012057845478742333, predict: 0.0
label: -1.0, output: 0.9832245395834279, predict: 1.0
label: -1.0, output: 0.015413835355359173, predict: 0.0
label: -1.0, output: 5.898649973659156E-7, predict: 0.0
label: -1.0, output: 0.9865352246942543, predict: 1.0
label: -1.0, output: 0.04415206149969183, predict: 0.0
label: -1.0, output: 0.007730545871280702, predict: 0.0
label: -1.0, output: 0.05321233363460295, predict: 0.0
label: -1.0, output: 0.02801641543769733, predict: 0.0
label: -1.0, output: 0.02041943622079246, predict: 0.0
label: -1.0, output: 0.9628446869071614, predict: 1.0
label: -1.0, output: 0.965170328405227, predict: 1.0
label: -1.0, output: 0.9391640705563041, predict: 1.0
label: -1.0, output: 0.07393022390432714, predict: 0.0
label: -1.0, output: 0.9999999772698002, predict: 1.0
label: -1.0, output: 0.9999113014696911, predict: 1.0
label: -1.0, output: 0.9785682810187051, predict: 1.0
label: -1.0, output: 0.9851488519272221, predict: 1.0
label: -1.0, output: 0.0037763055377947156, predict: 0.0
label: -1.0, output: 0.9801740195272426, predict: 1.0
label: -1.0, output: 0.9843422417723175, predict: 1.0
label: -1.0, output: 0.06341787507636976, predict: 0.0
label: -1.0, output: 0.02407983246381488, predict: 0.0
label: -1.0, output: 0.006509440432630842, predict: 0.0
label: -1.0, output: 0.9448209270505079, predict: 1.0
label: -1.0, output: 4.4162269320903674E-6, predict: 0.0
label: -1.0, output: 0.04634038690549881, predict: 0.0
label: -1.0, output: 0.8879702198025732, predict: 1.0
label: -1.0, output: 0.9896965143345923, predict: 1.0
label: -1.0, output: 0.0764996796761519, predict: 0.0
label: -1.0, output: 0.039187054040130696, predict: 0.0
label: -1.0, output: 0.9404471085507017, predict: 1.0
label: -1.0, output: 0.011929451108807509, predict: 0.0
label: -1.0, output: 0.9995431112774937, predict: 1.0
label: -1.0, output: 0.9965759250770508, predict: 1.0
label: -1.0, output: 0.047737774025780964, predict: 0.0
label: -1.0, output: 6.956646974097515E-7, predict: 0.0
label: -1.0, output: 0.9701694548412663, predict: 1.0
label: -1.0, output: 0.9933434054807583, predict: 1.0
label: -1.0, output: 0.9997434922684035, predict: 1.0
label: -1.0, output: 0.010360249852410876, predict: 0.0
label: -1.0, output: 0.007067590715907053, predict: 0.0
label: -1.0, output: 0.01745126670843875, predict: 0.0
label: -1.0, output: 2.6506096403798335E-4, predict: 0.0
label: -1.0, output: 0.9738841521083885, predict: 1.0
label: -1.0, output: 0.9996721017007438, predict: 1.0
label: -1.0, output: 0.021152210031370324, predict: 0.0
label: -1.0, output: 0.038361784346899616, predict: 0.0
label: -1.0, output: 0.9528915021501788, predict: 1.0
label: -1.0, output: 1.6102915734230133E-7, predict: 0.0
label: -1.0, output: 0.9895880626395347, predict: 1.0
label: -1.0, output: 0.9941269340266468, predict: 1.0
label: -1.0, output: 0.9928368345409772, predict: 1.0
label: -1.0, output: 0.9734256968264658, predict: 1.0
label: -1.0, output: 7.665816222518886E-5, predict: 0.0
label: -1.0, output: 0.9984367418553917, predict: 1.0
label: -1.0, output: 0.9995829782959789, predict: 1.0
label: -1.0, output: 0.017757371993759243, predict: 0.0
label: -1.0, output: 4.3718739492686464E-5, predict: 0.0
label: -1.0, output: 2.308777064199953E-7, predict: 0.0
label: -1.0, output: 0.02851515276099224, predict: 0.0
label: -1.0, output: 1.2185806897893615E-4, predict: 0.0
label: -1.0, output: 0.9309605661737976, predict: 1.0
label: -1.0, output: 9.857699142174663E-7, predict: 0.0
label: -1.0, output: 0.02614130918027831, predict: 0.0
label: -1.0, output: 1.6644877804525672E-4, predict: 0.0
label: -1.0, output: 0.15030454907493662, predict: 0.0
label: -1.0, output: 4.451898192160296E-4, predict: 0.0
label: -1.0, output: 0.9971603612075051, predict: 1.0
label: -1.0, output: 0.9420455946792872, predict: 1.0
label: -1.0, output: 0.040322433084363674, predict: 0.0
label: -1.0, output: 0.9973577153511659, predict: 1.0
label: -1.0, output: 0.9994630942252535, predict: 1.0
label: -1.0, output: 0.9843589173973958, predict: 1.0
label: -1.0, output: 0.12127271605277978, predict: 0.0
label: -1.0, output: 0.933341871305876, predict: 1.0
label: -1.0, output: 2.797306251260229E-7, predict: 0.0
label: -1.0, output: 0.037705778963763883, predict: 0.0
label: -1.0, output: 0.9942096399938551, predict: 1.0
label: -1.0, output: 0.9245091692980526, predict: 1.0
label: -1.0, output: 0.0036655234074188013, predict: 0.0
label: -1.0, output: 0.004972761786766141, predict: 0.0
label: -1.0, output: 0.9253326489221201, predict: 1.0
label: -1.0, output: 0.9130046871057547, predict: 1.0
label: -1.0, output: 0.0015528662260081792, predict: 0.0
label: -1.0, output: 0.27160661929393715, predict: 0.0
label: -1.0, output: 0.9428124736378998, predict: 1.0
label: -1.0, output: 0.025993589913553185, predict: 0.0
label: -1.0, output: 0.009079119808140444, predict: 0.0
label: -1.0, output: 0.0037585402704335127, predict: 0.0
label: -1.0, output: 0.9997689809473493, predict: 1.0
label: -1.0, output: 0.9436713627240892, predict: 1.0
label: -1.0, output: 0.008155172364709286, predict: 0.0
label: -1.0, output: 0.9991830030674971, predict: 1.0
label: -1.0, output: 0.9903153855379945, predict: 1.0
label: -1.0, output: 0.957991162288718, predict: 1.0
label: -1.0, output: 0.7174385522464031, predict: 1.0
label: -1.0, output: 0.9594714289308541, predict: 1.0
label: -1.0, output: 0.024099388143125428, predict: 0.0
label: -1.0, output: 0.9926639562595365, predict: 1.0
label: -1.0, output: 0.9294748945951541, predict: 1.0
label: -1.0, output: 0.999786238570917, predict: 1.0
label: -1.0, output: 0.26995256842074034, predict: 0.0
label: -1.0, output: 0.9916326519747048, predict: 1.0
label: -1.0, output: 0.9345275121001622, predict: 1.0
label: -1.0, output: 0.05011866977817379, predict: 0.0
label: -1.0, output: 0.05778476250921814, predict: 0.0
label: -1.0, output: 0.9992219183699774, predict: 1.0
label: -1.0, output: 0.07286976133816367, predict: 0.0
label: -1.0, output: 0.9772243401878042, predict: 1.0
label: -1.0, output: 0.03662608199630102, predict: 0.0
label: -1.0, output: 6.936123361571632E-6, predict: 0.0
label: -1.0, output: 0.9957824387095442, predict: 1.0
label: -1.0, output: 0.01393740226179561, predict: 0.0
label: -1.0, output: 0.99979583862027, predict: 1.0
label: -1.0, output: 0.02306248453368331, predict: 0.0
label: -1.0, output: 0.008393016882614329, predict: 0.0
label: -1.0, output: 0.0010874282066380902, predict: 0.0
label: -1.0, output: 0.05193692483275805, predict: 0.0
label: -1.0, output: 0.9039625531692049, predict: 1.0
label: -1.0, output: 1.8231045311410616E-7, predict: 0.0
label: -1.0, output: 0.9935519016506332, predict: 1.0
label: -1.0, output: 0.9981826530171823, predict: 1.0
label: -1.0, output: 0.997326962265487, predict: 1.0
label: -1.0, output: 0.9994234083360976, predict: 1.0
label: -1.0, output: 0.9689645895060012, predict: 1.0
label: -1.0, output: 0.9734448842846039, predict: 1.0
label: -1.0, output: 0.028221197685742175, predict: 0.0
label: -1.0, output: 0.03256666710439806, predict: 0.0
label: -1.0, output: 0.01882473712796848, predict: 0.0
label: -1.0, output: 0.9821314797040234, predict: 1.0
label: -1.0, output: 0.04581674572527479, predict: 0.0
label: -1.0, output: 0.9859523087162, predict: 1.0
label: -1.0, output: 0.9873319138592769, predict: 1.0
label: -1.0, output: 0.9903468982677551, predict: 1.0
label: -1.0, output: 0.9994880175628689, predict: 1.0
label: -1.0, output: 3.0620868441350827E-4, predict: 0.0
label: -1.0, output: 0.9993996269541341, predict: 1.0
label: -1.0, output: 0.948793637062679, predict: 1.0
label: -1.0, output: 0.6131824485068335, predict: 1.0
label: -1.0, output: 0.22701723312251793, predict: 0.0
label: -1.0, output: 0.04613256764233117, predict: 0.0
label: -1.0, output: 0.03409099941230715, predict: 0.0
label: -1.0, output: 0.9994976788843652, predict: 1.0
label: -1.0, output: 0.9372581546100075, predict: 1.0
label: -1.0, output: 0.04788478606380833, predict: 0.0
label: -1.0, output: 2.0662988310826484E-5, predict: 0.0
label: -1.0, output: 0.02190159573750731, predict: 0.0
label: -1.0, output: 0.9999938885961772, predict: 1.0
label: -1.0, output: 1.2429710813584686E-6, predict: 0.0
label: -1.0, output: 1.3881651983215629E-5, predict: 0.0
label: -1.0, output: 1.4798993617259114E-7, predict: 0.0
label: -1.0, output: 0.9400997091327171, predict: 1.0
label: -1.0, output: 0.969067601632853, predict: 1.0
label: -1.0, output: 0.9856751497309706, predict: 1.0
label: -1.0, output: 0.0026593273587249048, predict: 0.0
label: -1.0, output: 3.1628825902229536E-7, predict: 0.0
label: -1.0, output: 0.9434038876438218, predict: 1.0
label: -1.0, output: 0.999925931378941, predict: 1.0
label: -1.0, output: 0.021219451396043327, predict: 0.0
label: -1.0, output: 0.03581175716957016, predict: 0.0
label: -1.0, output: 0.9609593825749071, predict: 1.0
label: -1.0, output: 0.0033409940798017046, predict: 0.0
label: -1.0, output: 0.08707928589518739, predict: 0.0
label: -1.0, output: 8.228701110888553E-5, predict: 0.0
label: -1.0, output: 0.9914490356069173, predict: 1.0
label: -1.0, output: 0.9979951500015917, predict: 1.0
label: -1.0, output: 0.01727379866714502, predict: 0.0
label: -1.0, output: 0.00479270532775464, predict: 0.0
label: -1.0, output: 0.010105425316670262, predict: 0.0
label: -1.0, output: 0.0013444140558961303, predict: 0.0
label: -1.0, output: 0.9999574710113728, predict: 1.0
label: -1.0, output: 0.0056526781704619375, predict: 0.0
label: -1.0, output: 0.0038300744327838584, predict: 0.0
label: -1.0, output: 0.9972438197070478, predict: 1.0
label: -1.0, output: 0.04037119392822993, predict: 0.0
label: -1.0, output: 0.9982060721557022, predict: 1.0
label: -1.0, output: 0.21391985797868543, predict: 0.0
label: -1.0, output: 0.9890136470426378, predict: 1.0
label: -1.0, output: 0.007808552504257777, predict: 0.0
label: -1.0, output: 0.01946981568767245, predict: 0.0
label: -1.0, output: 0.04078150532601226, predict: 0.0
label: -1.0, output: 0.9952018790182164, predict: 1.0
label: -1.0, output: 0.7565181336720364, predict: 1.0
label: -1.0, output: 0.9703410835543234, predict: 1.0
label: -1.0, output: 0.9887241279622578, predict: 1.0
label: -1.0, output: 0.9991705820009009, predict: 1.0
label: -1.0, output: 0.028380953044550024, predict: 0.0
label: -1.0, output: 0.9997553127682166, predict: 1.0
label: -1.0, output: 0.022303124645046298, predict: 0.0
label: -1.0, output: 0.9991034878483158, predict: 1.0
label: -1.0, output: 0.004940542878734257, predict: 0.0
label: -1.0, output: 0.9998716439344264, predict: 1.0
label: -1.0, output: 0.908866387007287, predict: 1.0
label: -1.0, output: 0.9768717401861822, predict: 1.0
label: -1.0, output: 0.9999993598017887, predict: 1.0
label: -1.0, output: 0.9894170037857166, predict: 1.0
label: -1.0, output: 4.8082028438777295E-4, predict: 0.0
label: -1.0, output: 0.024315327417342512, predict: 0.0
label: -1.0, output: 0.9314694392640397, predict: 1.0
label: -1.0, output: 0.9599444970729225, predict: 1.0
label: -1.0, output: 0.9994768565659312, predict: 1.0
label: -1.0, output: 0.9995027832093046, predict: 1.0
label: -1.0, output: 0.0170643304378967, predict: 0.0
label: -1.0, output: 6.994067224619149E-4, predict: 0.0
label: -1.0, output: 0.9722525396098954, predict: 1.0
label: -1.0, output: 0.01808850212543027, predict: 0.0
label: -1.0, output: 0.006573563708342266, predict: 0.0
label: -1.0, output: 0.9958966190906748, predict: 1.0
label: -1.0, output: 0.38959924741603114, predict: 0.0
label: -1.0, output: 0.9999980139299097, predict: 1.0
label: -1.0, output: 5.679841620850891E-4, predict: 0.0
label: -1.0, output: 0.021790687252808083, predict: 0.0
label: -1.0, output: 0.0038718053225058877, predict: 0.0
label: -1.0, output: 4.1195423381514336E-7, predict: 0.0
label: -1.0, output: 0.9875029790295418, predict: 1.0
label: -1.0, output: 1.7416776383501272E-6, predict: 0.0
label: -1.0, output: 0.024226138275963777, predict: 0.0
label: -1.0, output: 0.9992392736736092, predict: 1.0
label: -1.0, output: 0.9995933816646346, predict: 1.0
label: -1.0, output: 0.98486895552459, predict: 1.0
label: -1.0, output: 0.01241973539840164, predict: 0.0
label: -1.0, output: 0.9965548581794634, predict: 1.0
label: -1.0, output: 0.9629625142872823, predict: 1.0
label: -1.0, output: 0.023396683249708297, predict: 0.0
label: -1.0, output: 0.9499754820562537, predict: 1.0
label: -1.0, output: 0.9921368761063998, predict: 1.0
label: -1.0, output: 1.2977499736926854E-4, predict: 0.0
label: -1.0, output: 0.007234810447891492, predict: 0.0
label: -1.0, output: 0.9999718450646132, predict: 1.0
label: -1.0, output: 0.9999581462648592, predict: 1.0
label: -1.0, output: 0.9768512704366453, predict: 1.0
label: -1.0, output: 0.05473215118561453, predict: 0.0
label: -1.0, output: 5.839075820114527E-4, predict: 0.0
label: -1.0, output: 0.9978231020938481, predict: 1.0
label: -1.0, output: 0.04129846428993886, predict: 0.0
label: -1.0, output: 0.9882383791412008, predict: 1.0
label: -1.0, output: 0.006608569271750158, predict: 0.0
label: -1.0, output: 0.01736769043903288, predict: 0.0
label: -1.0, output: 0.6790892613360279, predict: 1.0
label: -1.0, output: 0.025743357253158617, predict: 0.0
label: -1.0, output: 0.9717813438804843, predict: 1.0
label: -1.0, output: 0.009489689997757806, predict: 0.0
label: -1.0, output: 0.015683038313285092, predict: 0.0
label: -1.0, output: 0.021821580568356515, predict: 0.0
label: -1.0, output: 0.02940152156874607, predict: 0.0
label: -1.0, output: 0.9890803475956327, predict: 1.0
label: -1.0, output: 0.99920466281475, predict: 1.0
label: -1.0, output: 0.6457254851891209, predict: 1.0
label: -1.0, output: 0.001347164616738929, predict: 0.0
label: -1.0, output: 0.9690817828558866, predict: 1.0
label: -1.0, output: 0.9648093099851703, predict: 1.0
label: -1.0, output: 0.9991259653163833, predict: 1.0
label: -1.0, output: 0.039389834826441344, predict: 0.0
label: -1.0, output: 0.018247711641829912, predict: 0.0
label: -1.0, output: 0.04173162499277127, predict: 0.0
label: -1.0, output: 0.00862230826588693, predict: 0.0
label: -1.0, output: 0.9573383310474509, predict: 1.0
label: -1.0, output: 0.9916709470940666, predict: 1.0
label: -1.0, output: 4.2076634930432763E-7, predict: 0.0
label: -1.0, output: 0.9695675169609834, predict: 1.0
label: -1.0, output: 8.842582995139583E-6, predict: 0.0
label: -1.0, output: 0.03707263180494772, predict: 0.0
label: -1.0, output: 0.0031286851870039236, predict: 0.0
label: -1.0, output: 0.036794150462563145, predict: 0.0
label: -1.0, output: 0.9851027590724358, predict: 1.0
label: -1.0, output: 0.015182464171778416, predict: 0.0
label: -1.0, output: 0.9997859298509374, predict: 1.0
label: -1.0, output: 0.04063912194475328, predict: 0.0
label: -1.0, output: 0.030177207966686187, predict: 0.0
label: -1.0, output: 9.100224385485611E-4, predict: 0.0
label: -1.0, output: 0.014445087249543522, predict: 0.0
label: -1.0, output: 0.0026356243024511475, predict: 0.0
label: -1.0, output: 0.0021019739986874557, predict: 0.0
label: -1.0, output: 0.04254725541815358, predict: 0.0
label: -1.0, output: 5.176982813911312E-7, predict: 0.0
label: -1.0, output: 0.004106214158297204, predict: 0.0
label: -1.0, output: 0.025095959218406537, predict: 0.0
label: -1.0, output: 0.9369009084145664, predict: 1.0
label: -1.0, output: 0.9149395687463987, predict: 1.0
label: -1.0, output: 0.02822823915124087, predict: 0.0
label: -1.0, output: 0.917489679409723, predict: 1.0
label: -1.0, output: 0.951167780676269, predict: 1.0
label: -1.0, output: 0.015551985129070524, predict: 0.0
label: -1.0, output: 0.962968406840237, predict: 1.0
label: -1.0, output: 2.4063653211386053E-5, predict: 0.0
label: -1.0, output: 0.9900461134477472, predict: 1.0
label: -1.0, output: 0.03288410881241371, predict: 0.0
label: -1.0, output: 0.9680072769144362, predict: 1.0
label: -1.0, output: 0.7492646516670234, predict: 1.0
label: -1.0, output: 1.586340488338457E-6, predict: 0.0
label: -1.0, output: 6.22464706113381E-5, predict: 0.0
label: -1.0, output: 0.028268173695554656, predict: 0.0
label: -1.0, output: 0.007575158372972879, predict: 0.0
label: -1.0, output: 0.9766245165495898, predict: 1.0
label: -1.0, output: 0.0033506694077255882, predict: 0.0
label: -1.0, output: 0.8708701091830382, predict: 1.0
label: -1.0, output: 3.0841527113401508E-6, predict: 0.0
label: -1.0, output: 0.009858717166273815, predict: 0.0
label: -1.0, output: 0.07518055254744686, predict: 0.0
label: -1.0, output: 0.021279736151659037, predict: 0.0
label: -1.0, output: 3.05618987814334E-4, predict: 0.0
label: -1.0, output: 4.072710513997838E-6, predict: 0.0
label: -1.0, output: 0.9896068264789911, predict: 1.0
label: -1.0, output: 0.981973583144053, predict: 1.0
label: -1.0, output: 0.9281082326072351, predict: 1.0
label: -1.0, output: 0.9957666670445241, predict: 1.0
label: -1.0, output: 0.9995641023879067, predict: 1.0
label: -1.0, output: 0.9972957180843636, predict: 1.0
label: -1.0, output: 0.9647585453978603, predict: 1.0
label: -1.0, output: 0.9856378837126023, predict: 1.0
label: -1.0, output: 0.026409576939519994, predict: 0.0
label: -1.0, output: 0.06071788363164857, predict: 0.0
label: -1.0, output: 0.9487806029372429, predict: 1.0
label: -1.0, output: 0.9997702577319617, predict: 1.0
label: -1.0, output: 0.007839871928258219, predict: 0.0
label: -1.0, output: 0.035584489415433504, predict: 0.0
label: -1.0, output: 0.9930433216176291, predict: 1.0
label: -1.0, output: 0.008185901990184699, predict: 0.0
label: -1.0, output: 0.9991764066217504, predict: 1.0
label: -1.0, output: 0.9973639152543914, predict: 1.0
label: -1.0, output: 0.9751056856827329, predict: 1.0
label: -1.0, output: 0.04041545768921481, predict: 0.0
label: -1.0, output: 0.024175614637956033, predict: 0.0
label: -1.0, output: 0.9993126690405655, predict: 1.0
label: -1.0, output: 0.999999984532222, predict: 1.0
label: -1.0, output: 0.01276934134103659, predict: 0.0
label: -1.0, output: 0.02134797845627562, predict: 0.0
label: -1.0, output: 0.999974596006814, predict: 1.0
label: -1.0, output: 7.225063466610211E-4, predict: 0.0
label: -1.0, output: 0.9754558594561494, predict: 1.0
label: -1.0, output: 1.6464297161674438E-6, predict: 0.0
label: -1.0, output: 8.027488020306744E-7, predict: 0.0
label: -1.0, output: 0.9844815874948805, predict: 1.0
label: -1.0, output: 0.7658291468893775, predict: 1.0
label: -1.0, output: 0.10804219781274692, predict: 0.0
label: -1.0, output: 0.025649205673102148, predict: 0.0
label: -1.0, output: 0.9998288180548798, predict: 1.0
label: -1.0, output: 0.004065104398807102, predict: 0.0
label: -1.0, output: 0.7544847899414481, predict: 1.0
label: -1.0, output: 0.9903058033410056, predict: 1.0
label: -1.0, output: 0.07649399599526827, predict: 0.0
label: -1.0, output: 0.9734749767060434, predict: 1.0
label: -1.0, output: 0.9140454790189926, predict: 1.0
label: -1.0, output: 0.9782093465582696, predict: 1.0
label: -1.0, output: 0.927501728924141, predict: 1.0
label: -1.0, output: 0.0048382150579265, predict: 0.0
label: -1.0, output: 0.9990435418253882, predict: 1.0
label: -1.0, output: 0.002004150726705897, predict: 0.0
label: -1.0, output: 0.0687868088209749, predict: 0.0
label: -1.0, output: 0.9975597026563587, predict: 1.0
label: -1.0, output: 0.9592602881985407, predict: 1.0
label: -1.0, output: 0.023933072946575575, predict: 0.0
label: -1.0, output: 6.460161282241335E-4, predict: 0.0
label: -1.0, output: 0.9872045582548595, predict: 1.0
label: -1.0, output: 0.015718991995148084, predict: 0.0
label: -1.0, output: 0.09468422839494188, predict: 0.0
label: -1.0, output: 0.9805563450327719, predict: 1.0
label: -1.0, output: 0.8917686556715914, predict: 1.0
label: -1.0, output: 9.914926287730007E-4, predict: 0.0
label: -1.0, output: 0.9991345398135257, predict: 1.0
label: -1.0, output: 0.9338286842097745, predict: 1.0
total: 0.0, correct: 0.0, wrong: 0.0, accurate: 0.0
predictTime: 128
==========Finish Predict==========
==========Start WritePredict==========
writePredictTime: 8219
==========Finish WritePredict==========
==========train 0==========
train: 0, loss: -1.0
layerType: input, nodeSize: 5
double[][] w: null
double[] b: null
double[][] partialDerivative: null
double[] z: null
double[] h: 
[-0.014960021512321564, -1.3652645049762566, 0.020424386364453852, -0.014959463504169192, -0.9789525846113298]
layerType: hidden, nodeSize: 8
double[][] w: 
[0.2541131620300339, -1.2153477390950855, -0.7562785018282442, -0.0714058830769099, 3.4980504716671597]
[1.89998458694133, -2.0776589757821893, 0.06214987046246464, -10.053836768724956, 5.038612701376699]
[-1.1464908053322644, 2.216016256396431, 0.010854604675868617, -6.161597017029887, -1.4357167446874317]
[3.5140527219945423, -1.33563446822961, 0.7883553258354257, -4.987558908398485, 3.9357511254536965]
[1.3945660872943488, -0.21446501101216905, -0.11555687397509332, -2.8744394569978726, -9.211847333070741]
[-3.1046369822845397, 0.8786475280301853, 1.318429639973087, 4.7721998039123825, -2.9706715308469867]
[0.8163892521332461, 1.0713718194039648, -0.617859944779149, -1.4930554157613358, -3.475329393607595]
[2.6502397785333636, 1.5038383594417148, -4.396180108141968, 0.8540810634786525, -1.0642530030441004]
double[] b: 
[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
double[][] partialDerivative: 
[-2.5265202078950686E-7, -2.5265202078950686E-7, -2.5265202078950686E-7, -2.5265202078950686E-7, -2.5265202078950686E-7]
[8.995676144351738E-11, 8.995676144351738E-11, 8.995676144351738E-11, 8.995676144351738E-11, 8.995676144351738E-11]
[-2.7051704584444456E-7, -2.7051704584444456E-7, -2.7051704584444456E-7, -2.7051704584444456E-7, -2.7051704584444456E-7]
[-1.5488613692781994E-8, -1.5488613692781994E-8, -1.5488613692781994E-8, -1.5488613692781994E-8, -1.5488613692781994E-8]
[-2.273319462985449E-8, -2.273319462985449E-8, -2.273319462985449E-8, -2.273319462985449E-8, -2.273319462985449E-8]
[4.75999778200899E-8, 4.75999778200899E-8, 4.75999778200899E-8, 4.75999778200899E-8, 4.75999778200899E-8]
[7.724068543014941E-8, 7.724068543014941E-8, 7.724068543014941E-8, 7.724068543014941E-8, 7.724068543014941E-8]
[8.074995102058248E-18, 8.074995102058248E-18, 8.074995102058248E-18, 8.074995102058248E-18, 8.074995102058248E-18]
double[] z: 
[-1.7833342899389144, -1.972763306895727, -1.5104023078720452, -1.991276830637174, 9.330540378093334, 1.7105442943520643, 1.9369794725799683, -1.1534974250119059]
double[] h: 
[0.14389190493448842, 0.1220923900214516, 0.18087917880006704, 0.12012184546333224, 0.9999113335559412, 0.8469068683850514, 0.874019931521503, 0.23985084401810586]
layerType: output, nodeSize: 1
double[][] w: 
[-6.314106925175921, 16.243238620775376, -14.25870962057811, -6.687884376783248, -12.201971294074262, 9.157672096350119, 9.703051901611905, 3.833286111677083]
double[] b: 
[0.0]
double[][] partialDerivative: 
[2.7749470503612945E-7, 2.7749470503612945E-7, 2.7749470503612945E-7, 2.7749470503612945E-7, 2.7749470503612945E-7, 2.7749470503612945E-7, 2.7749470503612945E-7, 2.7749470503612945E-7]
double[] z: 
[2.6470459269652253]
double[] h: 
[0.9338286842097745]
==============================
programTotalTime: 8222
gauss raw acc: 97.8
gauss acc mapped to grading score: 100
xor raw acc: 95.6
xor acc mapped to grading score: 100
circle raw acc: 92.2
circle acc mapped to grading score: 100
spiral raw acc: 98.2
spiral acc mapped to grading score: 100





[Executed at: Wed Dec 7 13:31:03 PST 2022]

HW3 Report
Programming language: JAVA
==> Begin grading

Wed 07 Dec 2022 01:30:34 PM PST
(gaussian hidden test) accuracy: 97.20
(xor hidden test) accuracy: 94.40
(circle hidden test) accuracy: 90.80
(spiral hidden test) accuracy: 100.00
==> Stop grading
Wed 07 Dec 2022 01:31:01 PM PST
==> Score report
Raw avg accuracy: 95.60000000000001
Total score (according to hidden data grading scale): 100.0
==> End score report

gauss raw acc: 97.2
gauss acc mapped to grading score: 100
xor raw acc: 94.4
xor acc mapped to grading score: 100
circle raw acc: 90.8
circle acc mapped to grading score: 100
spiral raw acc: 100.0
spiral acc mapped to grading score: 100





